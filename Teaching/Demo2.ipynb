{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Demo2\n",
    "#Pong & Tetris Game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Bare bone skeleton of pong game using pygame.\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "\n",
    "score = 0\n",
    "\n",
    "def run(screen_width=400., screen_height=400.):\n",
    "    global score\n",
    "    pygame.init()\n",
    "\n",
    "    bar_width, bar_height = screen_width / 32., screen_height / 6.\n",
    "    bar_dist_from_edge = screen_width / 64.\n",
    "    circle_diameter = screen_height / 16.\n",
    "    circle_radius = circle_diameter / 2.\n",
    "    bar_1_start_x = bar_dist_from_edge\n",
    "    bar_start_y = (screen_height - bar_height) / 2.\n",
    "    bar_max_y = screen_height - bar_height - bar_dist_from_edge\n",
    "    circle_start_x, circle_start_y = (screen_width - circle_diameter), (screen_width - circle_diameter) / 2.\n",
    "\n",
    "    screen = pygame.display.set_mode((int(screen_width), int(screen_height)), 0, 32)\n",
    "\n",
    "    # Creating 2 bars, a ball and background.\n",
    "    back = pygame.Surface((int(screen_width), int(screen_height)))\n",
    "    background = back.convert()\n",
    "    background.fill((0, 0, 0))\n",
    "    bar = pygame.Surface((int(bar_width), int(bar_height)))\n",
    "    bar1 = bar.convert()\n",
    "    bar1.fill((255, 255, 255))\n",
    "    circle_surface = pygame.Surface((int(circle_diameter), int(circle_diameter)))\n",
    "    pygame.draw.circle(circle_surface, (255, 255, 255), (int(circle_radius), int(circle_radius)), int(circle_radius))\n",
    "    circle = circle_surface.convert()\n",
    "    circle.set_colorkey((0, 0, 0))\n",
    "\n",
    "    # some definitions\n",
    "    bar1_x = bar_1_start_x\n",
    "    bar1_y = bar_start_y\n",
    "    circle_x, circle_y = circle_start_x, circle_start_y\n",
    "    bar1_move, bar2_move = 0., 0.\n",
    "    speed_x, speed_y, speed_bar = -screen_width / 1.28, screen_height / 1.92, screen_height * 1.2\n",
    "\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        for event in pygame.event.get():  # User did something\n",
    "            if event.type == pygame.QUIT:  # If user clicked close\n",
    "                done = True  # Flag that we are done so we exit this loop\n",
    "            if event.type == KEYDOWN:\n",
    "                if event.key == K_UP:\n",
    "                    bar1_move = -ai_speed\n",
    "                elif event.key == K_DOWN:\n",
    "                    bar1_move = ai_speed\n",
    "            elif event.type == KEYUP:\n",
    "                if event.key == K_UP:\n",
    "                    bar1_move = 0.\n",
    "                elif event.key == K_DOWN:\n",
    "                    bar1_move = 0.\n",
    "\n",
    "        screen.blit(background, (0, 0))\n",
    "        screen.blit(bar1, (bar1_x, bar1_y))\n",
    "        screen.blit(circle, (circle_x, circle_y))\n",
    "\n",
    "        bar1_y += bar1_move\n",
    "\n",
    "        # movement of circle\n",
    "        time_passed = clock.tick(30)\n",
    "        time_sec = time_passed / 1000.0\n",
    "\n",
    "        circle_x += speed_x * time_sec\n",
    "        circle_y += speed_y * time_sec\n",
    "        ai_speed = speed_bar * time_sec\n",
    "\n",
    "        if bar1_y >= bar_max_y:\n",
    "            bar1_y = bar_max_y\n",
    "        elif bar1_y <= bar_dist_from_edge:\n",
    "            bar1_y = bar_dist_from_edge\n",
    "\n",
    "        if circle_x < bar_dist_from_edge + bar_width:\n",
    "            if circle_y >= bar1_y - circle_radius and circle_y <= bar1_y + bar_height + circle_radius:\n",
    "                circle_x = bar_dist_from_edge + bar_width\n",
    "                speed_x = -speed_x\n",
    "        if circle_x < -circle_radius:\n",
    "            score -= 1\n",
    "            circle_x, circle_y = circle_start_x, circle_start_y\n",
    "            bar1_y, bar_2_y = bar_start_y, bar_start_y\n",
    "        elif circle_x > screen_width - circle_diameter:\n",
    "            score += 1\n",
    "            speed_x = -speed_x\n",
    "        if circle_y <= bar_dist_from_edge:\n",
    "            speed_y = -speed_y\n",
    "            circle_y = bar_dist_from_edge\n",
    "        elif circle_y >= screen_height - circle_diameter - circle_radius:\n",
    "            speed_y = -speed_y\n",
    "            circle_y = screen_height - circle_diameter - circle_radius\n",
    "\n",
    "        pygame.display.update()\n",
    "\n",
    "    pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run pong game\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function intercepter to our game code...\n",
    "\n",
    "import pygame\n",
    "import numpy  # import is unused but required or we fail later\n",
    "from pygame.constants import K_DOWN, KEYDOWN, KEYUP, QUIT\n",
    "import pygame.surfarray\n",
    "import pygame.key\n",
    "\n",
    "def function_intercept(intercepted_func, intercepting_func):\n",
    "    \"\"\"\n",
    "    Intercepts a method call and calls the supplied intercepting_func with the result of it's call and it's arguments\n",
    "    Example:\n",
    "        def get_event(result_of_real_event_get, *args, **kwargs):\n",
    "            # do work\n",
    "            return result_of_real_event_get\n",
    "        pygame.event.get = function_intercept(pygame.event.get, get_event)\n",
    "    :param intercepted_func: The function we are going to intercept\n",
    "    :param intercepting_func:   The function that will get called after the intercepted func. It is supplied the return\n",
    "    value of the intercepted_func as the first argument and it's args and kwargs.\n",
    "    :return: a function that combines the intercepting and intercepted function, should normally be set to the\n",
    "             intercepted_functions location\n",
    "    \"\"\"\n",
    "    def wrap(*args, **kwargs):\n",
    "        real_results = intercepted_func(*args, **kwargs) # call the function we are intercepting and get it's result\n",
    "        intercepted_results = intercepting_func(real_results, *args, **kwargs)  # call our own function a\n",
    "        return intercepted_results\n",
    "    \n",
    "    return wrap\n",
    "\n",
    "class PyGamePlayer(object):\n",
    "    def __init__(self, force_game_fps=10, run_real_time=False, pass_quit_event=True):\n",
    "        \"\"\"\n",
    "        Abstract class for learning agents, such as running reinforcement learning neural nets against PyGame games.\n",
    "        The get_keys_pressed and get_feedback methods must be overriden by a subclass to use\n",
    "        Call start method to start playing intercepting PyGame and training our machine\n",
    "        :param force_game_fps: Fixes the pygame timer functions so the ai will get input as if it were running at this\n",
    "                               fps\n",
    "        :type force_game_fps: int\n",
    "        :param run_real_time: If True the game will actually run at the force_game_fps speed\n",
    "        :type run_real_time: bool\n",
    "        :param pass_quit_event: If True the ai will be asked for the quit event\n",
    "        :type pass_quit_event: bool\n",
    "        \"\"\"\n",
    "        self.force_game_fps = force_game_fps\n",
    "        \"\"\"Fixes the pygame timer functions so the ai will get input as if it were running at this fps\"\"\"\n",
    "        self.run_real_time = run_real_time\n",
    "        \"\"\"If True the game will actually run at the force_game_fps speed\"\"\"\n",
    "        self.pass_quit_event = pass_quit_event\n",
    "        \"\"\"Decides whether the quit event should be passed on to the game\"\"\"\n",
    "        self._keys_pressed = []\n",
    "        self._last_keys_pressed = []\n",
    "        self._playing = False\n",
    "        self._default_flip = pygame.display.flip\n",
    "        self._default_update = pygame.display.update\n",
    "        self._default_event_get = pygame.event.get\n",
    "        self._default_time_clock = pygame.time.Clock\n",
    "        self._default_get_ticks = pygame.time.get_ticks\n",
    "        self._game_time = 0.0\n",
    "        \n",
    "    def get_keys_pressed(self, screen_array, feedback, terminal):\n",
    "        \"\"\"\n",
    "        Called whenever the screen buffer is refreshed. returns the keys we want pressed in the next until the next\n",
    "        screen refresh\n",
    "        :param screen_array: 3d numpy.array of float. screen_width * screen_height * rgb\n",
    "        :param feedback: result of call to get_feedback\n",
    "        :param terminal: boolean, True if we have reached a terminal state, meaning the next frame will be a restart\n",
    "        :return: a list of the integer values of the keys we want pressed. See pygame.constants for values\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Please override this method\")\n",
    "\n",
    "    def get_feedback(self):\n",
    "        \"\"\"\n",
    "        Overriden method should hook into game events to give feeback to the learning agent\n",
    "        :return: First = value we want to give as reward/punishment to our learning agent\n",
    "                 Second = Boolean true if we have reached a terminal state\n",
    "        :rtype: tuple (float, boolean)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Please override this method\")\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start playing the game. We will now start listening for screen updates calling our play and reward functions\n",
    "        and returning our intercepted key presses\n",
    "        \"\"\"\n",
    "        if self._playing:\n",
    "            raise Exception(\"Already playing\")\n",
    "\n",
    "        pygame.display.flip = function_intercept(pygame.display.flip, self._on_screen_update)\n",
    "        pygame.display.update = function_intercept(pygame.display.update, self._on_screen_update)\n",
    "        pygame.event.get = function_intercept(pygame.event.get, self._on_event_get)\n",
    "        pygame.time.Clock = function_intercept(pygame.time.Clock, self._on_time_clock)\n",
    "        pygame.time.get_ticks = function_intercept(pygame.time.get_ticks, self.get_game_time_ms)\n",
    "        # TODO: handle pygame.time.set_timer...\n",
    "\n",
    "        self._playing = True\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stop playing the game. Will try and return PyGame to the state it was in before we started\n",
    "        \"\"\"\n",
    "        if not self._playing:\n",
    "            raise Exception(\"Already stopped\")\n",
    "\n",
    "        pygame.display.flip = self._default_flip\n",
    "        pygame.display.update = self._default_update\n",
    "        pygame.event.get = self._default_event_get\n",
    "        pygame.time.Clock = self._default_time_clock\n",
    "        pygame.time.get_ticks = self._default_get_ticks\n",
    "\n",
    "        self._playing = False\n",
    "        \n",
    "    @property\n",
    "    def playing(self):\n",
    "        \"\"\"\n",
    "        Returns if we are in a state where we are playing/intercepting PyGame calls\n",
    "        :return: boolean\n",
    "        \"\"\"\n",
    "        return self._playing\n",
    "\n",
    "    @playing.setter\n",
    "    def playing(self, value):\n",
    "        if self._playing == value:\n",
    "            return\n",
    "        if self._playing:\n",
    "            self.stop()\n",
    "        else:\n",
    "            self.start()\n",
    "\n",
    "    def get_ms_per_frame(self):\n",
    "        return 1000.0 / self.force_game_fps\n",
    "\n",
    "    def get_game_time_ms(self):\n",
    "        return self._game_time\n",
    "\n",
    "    def _on_time_clock(self, real_clock, *args, **kwargs):\n",
    "        return self._FixedFPSClock(self, real_clock)\n",
    "    \n",
    "    def _on_screen_update(self, _, *args, **kwargs):\n",
    "        surface_array = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        reward, terminal = self.get_feedback()\n",
    "        keys = self.get_keys_pressed(surface_array, reward, terminal)\n",
    "        self._last_keys_pressed = self._keys_pressed\n",
    "        self._keys_pressed = keys\n",
    "\n",
    "        # now we have processed a frame increment the game timer\n",
    "        self._game_time += self.get_ms_per_frame()\n",
    "        \n",
    "    def _on_event_get(self, _, *args, **kwargs):\n",
    "        key_down_events = [pygame.event.Event(KEYDOWN, {\"key\": x})\n",
    "                           for x in self._keys_pressed if x not in self._last_keys_pressed]\n",
    "        key_up_events = [pygame.event.Event(KEYUP, {\"key\": x})\n",
    "                         for x in self._last_keys_pressed if x not in self._keys_pressed]\n",
    "\n",
    "        result = []\n",
    "\n",
    "        # have to deal with arg type filters\n",
    "        if args:\n",
    "            if hasattr(args[0], \"__iter__\"):\n",
    "                args = args[0]\n",
    "\n",
    "            for type_filter in args:\n",
    "                if type_filter == QUIT:\n",
    "                    if type_filter == QUIT:\n",
    "                        if self.pass_quit_event:\n",
    "                            for e in _:\n",
    "                                if e.type == QUIT:\n",
    "                                    result.append(e)\n",
    "                    else:\n",
    "                        pass  # never quit\n",
    "                elif type_filter == KEYUP:\n",
    "                    result = result + key_up_events\n",
    "                elif type_filter == KEYDOWN:\n",
    "                    result = result + key_down_events\n",
    "        else:\n",
    "            result = key_down_events + key_up_events\n",
    "            if self.pass_quit_event:\n",
    "                for e in _:\n",
    "                    if e.type == QUIT:\n",
    "                        result.append(e)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop()\n",
    "        \n",
    "    \n",
    "    class _FixedFPSClock(object):\n",
    "        def __init__(self, pygame_player, real_clock):\n",
    "            self._pygame_player = pygame_player\n",
    "            self._real_clock = real_clock\n",
    "\n",
    "        def tick(self, _=None):\n",
    "            if self._pygame_player.run_real_time:\n",
    "                return self._real_clock.tick(self._pygame_player.force_game_fps)\n",
    "            else:\n",
    "                return self._pygame_player.get_ms_per_frame()\n",
    "\n",
    "        def tick_busy_loop(self, _=None):\n",
    "            if self._pygame_player.run_real_time:\n",
    "                return self._real_clock.tick_busy_loop(self._pygame_player.force_game_fps)\n",
    "            else:\n",
    "                return self._pygame_player.get_ms_per_frame()\n",
    "\n",
    "        def get_time(self):\n",
    "            return self._pygame_player.get_game_time_ms()\n",
    "\n",
    "        def get_raw_time(self):\n",
    "            return self._pygame_player.get_game_time_ms()\n",
    "\n",
    "        def get_fps(self):\n",
    "            return int(1.0 / self._pygame_player.get_ms_per_frame())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Jun 2, 2016\n",
    "\n",
    "@author: rbhat\n",
    "'''\n",
    "# This is heavily based off https://github.com/asrivat1/DeepLearningVideoGames\n",
    "# deep q learning agent that runs against Half-Pong. Runs on a much smaller screen and with fewer layers.\n",
    "# Performs significantly above random, but still has someway to go to match google deep mind performance...\n",
    "# To see a trained version of this network start it with the kwargs checkpoint_path=\"deep_q_half_pong_networks_40x40_8\"\n",
    "# and playback_mode=\"True\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pygame.constants import K_DOWN, K_UP\n",
    "\n",
    "\n",
    "class DeepQHalfPongPlayer(PyGamePlayer):\n",
    "    ACTIONS_COUNT = 3  # number of valid actions. In this case up, still and down\n",
    "    FUTURE_REWARD_DISCOUNT = 0.99  # decay rate of past observations\n",
    "    OBSERVATION_STEPS = 50000.  # time steps to observe before training\n",
    "    EXPLORE_STEPS = 500000.  # frames over which to anneal epsilon\n",
    "    INITIAL_RANDOM_ACTION_PROB = 1.0  # starting chance of an action being random\n",
    "    FINAL_RANDOM_ACTION_PROB = 0.05  # final chance of an action being random\n",
    "    MEMORY_SIZE = 500000  # number of observations to remember\n",
    "    MINI_BATCH_SIZE = 200  # size of mini batches\n",
    "    STATE_FRAMES = 4  # number of frames to store in the state\n",
    "    OBS_LAST_STATE_INDEX, OBS_ACTION_INDEX, OBS_REWARD_INDEX, OBS_CURRENT_STATE_INDEX, OBS_TERMINAL_INDEX = range(5)\n",
    "    SAVE_EVERY_X_STEPS = 10000\n",
    "    LEARN_RATE = 1e-6\n",
    "    STORE_SCORES_LEN = 200.\n",
    "    SCREEN_WIDTH = 40\n",
    "    SCREEN_HEIGHT = 40\n",
    "\n",
    "    def __init__(self,\n",
    "                 # to see a trained network change checkpoint_path=\"deep_q_half_pong_networks_40x40_8\" and\n",
    "                 # playback_mode=\"True\"\n",
    "                 checkpoint_path=\"deep_q_half_pong_networks\",\n",
    "                 playback_mode=True,\n",
    "                 verbose_logging=True):\n",
    "        \"\"\"\n",
    "        Example of deep q network for pong\n",
    "\n",
    "        :param checkpoint_path: directory to store checkpoints in\n",
    "        :type checkpoint_path: str\n",
    "        :param playback_mode: if true games runs in real time mode and demos itself running\n",
    "        :type playback_mode: bool\n",
    "        :param verbose_logging: If true then extra log information is printed to std out\n",
    "        :type verbose_logging: bool\n",
    "        \"\"\"\n",
    "        self._playback_mode = playback_mode\n",
    "        self.last_score = 0\n",
    "        super(DeepQHalfPongPlayer, self).__init__(force_game_fps=8, run_real_time=playback_mode)\n",
    "        self.verbose_logging = verbose_logging\n",
    "        self._checkpoint_path = checkpoint_path\n",
    "        self._session = tf.Session()\n",
    "        self._input_layer, self._output_layer = self._create_network()\n",
    "\n",
    "        self._action = tf.placeholder(\"float\", [None, self.ACTIONS_COUNT])\n",
    "        self._target = tf.placeholder(\"float\", [None])\n",
    "\n",
    "        readout_action = tf.reduce_sum(tf.mul(self._output_layer, self._action), reduction_indices=1)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.square(self._target - readout_action))\n",
    "        self._train_operation = tf.train.AdamOptimizer(self.LEARN_RATE).minimize(cost)\n",
    "\n",
    "        self._observations = deque()\n",
    "        self._last_scores = deque()\n",
    "\n",
    "        # set the first action to do nothing\n",
    "        self._last_action = np.zeros(self.ACTIONS_COUNT)\n",
    "        self._last_action[1] = 1\n",
    "\n",
    "        self._last_state = None\n",
    "        self._probability_of_random_action = self.INITIAL_RANDOM_ACTION_PROB\n",
    "        self._time = 0\n",
    "\n",
    "        self._session.run(tf.initialize_all_variables())\n",
    "\n",
    "        if not os.path.exists(self._checkpoint_path):\n",
    "            os.mkdir(self._checkpoint_path)\n",
    "        self._saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.get_checkpoint_state(self._checkpoint_path)\n",
    "\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self._saver.restore(self._session, checkpoint.model_checkpoint_path)\n",
    "            print(\"Loaded checkpoints %s\" % checkpoint.model_checkpoint_path)\n",
    "        elif playback_mode:\n",
    "            raise Exception(\"Could not load checkpoints for playback\")\n",
    "\n",
    "    def get_keys_pressed(self, screen_array, reward, terminal):\n",
    "        # images will be black or white\n",
    "        _, screen_binary = cv2.threshold(cv2.cvtColor(screen_array, cv2.COLOR_BGR2GRAY), 1, 255,\n",
    "                                         cv2.THRESH_BINARY)\n",
    "\n",
    "        if reward != 0.0:\n",
    "            self._last_scores.append(reward)\n",
    "            if len(self._last_scores) > self.STORE_SCORES_LEN:\n",
    "                self._last_scores.popleft()\n",
    "\n",
    "        # first frame must be handled differently\n",
    "        if self._last_state is None:\n",
    "            # the _last_state will contain the image data from the last self.STATE_FRAMES frames\n",
    "            self._last_state = np.stack(tuple(screen_binary for _ in range(self.STATE_FRAMES)), axis=2)\n",
    "            return DeepQHalfPongPlayer._key_presses_from_action(self._last_action)\n",
    "\n",
    "        screen_binary = np.reshape(screen_binary,\n",
    "                                   (self.SCREEN_WIDTH, self.SCREEN_HEIGHT, 1))\n",
    "        current_state = np.append(self._last_state[:, :, 1:], screen_binary, axis=2)\n",
    "\n",
    "        if not self._playback_mode:\n",
    "            # store the transition in previous_observations\n",
    "            self._observations.append((self._last_state, self._last_action, reward, current_state, terminal))\n",
    "\n",
    "            if len(self._observations) > self.MEMORY_SIZE:\n",
    "                self._observations.popleft()\n",
    "\n",
    "            # only train if done observing\n",
    "            if len(self._observations) > self.OBSERVATION_STEPS:\n",
    "                self._train()\n",
    "                self._time += 1\n",
    "\n",
    "        # update the old values\n",
    "        self._last_state = current_state\n",
    "\n",
    "        self._last_action = self._choose_next_action()\n",
    "\n",
    "        if not self._playback_mode:\n",
    "            # gradually reduce the probability of a random actionself.\n",
    "            if self._probability_of_random_action > self.FINAL_RANDOM_ACTION_PROB \\\n",
    "                    and len(self._observations) > self.OBSERVATION_STEPS:\n",
    "                self._probability_of_random_action -= \\\n",
    "                    (self.INITIAL_RANDOM_ACTION_PROB - self.FINAL_RANDOM_ACTION_PROB) / self.EXPLORE_STEPS\n",
    "\n",
    "            print(\"Time: %s random_action_prob: %s reward %s scores differential %s\" %\n",
    "                  (self._time, self._probability_of_random_action, reward,\n",
    "                   sum(self._last_scores) / self.STORE_SCORES_LEN))\n",
    "\n",
    "        return DeepQHalfPongPlayer._key_presses_from_action(self._last_action)\n",
    "\n",
    "    def _choose_next_action(self):\n",
    "        new_action = np.zeros([self.ACTIONS_COUNT])\n",
    "\n",
    "        if (not self._playback_mode) and (random.random() <= self._probability_of_random_action):\n",
    "            # choose an action randomly\n",
    "            action_index = random.randrange(self.ACTIONS_COUNT)\n",
    "        else:\n",
    "            # choose an action given our last state\n",
    "            readout_t = self._session.run(self._output_layer, feed_dict={self._input_layer: [self._last_state]})[0]\n",
    "            if self.verbose_logging:\n",
    "                print(\"Action Q-Values are %s\" % readout_t)\n",
    "            action_index = np.argmax(readout_t)\n",
    "\n",
    "        new_action[action_index] = 1\n",
    "        return new_action\n",
    "\n",
    "    def _train(self):\n",
    "        # sample a mini_batch to train on\n",
    "        mini_batch = random.sample(self._observations, self.MINI_BATCH_SIZE)\n",
    "        # get the batch variables\n",
    "        previous_states = [d[self.OBS_LAST_STATE_INDEX] for d in mini_batch]\n",
    "        actions = [d[self.OBS_ACTION_INDEX] for d in mini_batch]\n",
    "        rewards = [d[self.OBS_REWARD_INDEX] for d in mini_batch]\n",
    "        current_states = [d[self.OBS_CURRENT_STATE_INDEX] for d in mini_batch]\n",
    "        agents_expected_reward = []\n",
    "        # this gives us the agents expected reward for each action we might take\n",
    "        agents_reward_per_action = self._session.run(self._output_layer, feed_dict={self._input_layer: current_states})\n",
    "        for i in range(len(mini_batch)):\n",
    "            if mini_batch[i][self.OBS_TERMINAL_INDEX]:\n",
    "                # this was a terminal frame so there is no future reward...\n",
    "                agents_expected_reward.append(rewards[i])\n",
    "            else:\n",
    "                agents_expected_reward.append(\n",
    "                    rewards[i] + self.FUTURE_REWARD_DISCOUNT * np.max(agents_reward_per_action[i]))\n",
    "\n",
    "        # learn that these actions in these states lead to this reward\n",
    "        self._session.run(self._train_operation, feed_dict={\n",
    "            self._input_layer: previous_states,\n",
    "            self._action: actions,\n",
    "            self._target: agents_expected_reward})\n",
    "\n",
    "        # save checkpoints for later\n",
    "        if self._time % self.SAVE_EVERY_X_STEPS == 0:\n",
    "            self._saver.save(self._session, self._checkpoint_path + '/network', global_step=self._time)\n",
    "\n",
    "    def _create_network(self):\n",
    "        # network weights\n",
    "        convolution_weights_1 = tf.Variable(tf.truncated_normal([8, 8, self.STATE_FRAMES, 32], stddev=0.01))\n",
    "        convolution_bias_1 = tf.Variable(tf.constant(0.01, shape=[32]))\n",
    "\n",
    "        convolution_weights_2 = tf.Variable(tf.truncated_normal([4, 4, 32, 64], stddev=0.01))\n",
    "        convolution_bias_2 = tf.Variable(tf.constant(0.01, shape=[64]))\n",
    "\n",
    "        feed_forward_weights_1 = tf.Variable(tf.truncated_normal([256, 256], stddev=0.01))\n",
    "        feed_forward_bias_1 = tf.Variable(tf.constant(0.01, shape=[256]))\n",
    "\n",
    "        feed_forward_weights_2 = tf.Variable(tf.truncated_normal([256, self.ACTIONS_COUNT], stddev=0.01))\n",
    "        feed_forward_bias_2 = tf.Variable(tf.constant(0.01, shape=[self.ACTIONS_COUNT]))\n",
    "\n",
    "        input_layer = tf.placeholder(\"float\", [None, self.SCREEN_WIDTH, self.SCREEN_HEIGHT,\n",
    "                                               self.STATE_FRAMES])\n",
    "\n",
    "        hidden_convolutional_layer_1 = tf.nn.relu(\n",
    "            tf.nn.conv2d(input_layer, convolution_weights_1, strides=[1, 4, 4, 1], padding=\"SAME\") + convolution_bias_1)\n",
    "\n",
    "        hidden_max_pooling_layer_1 = tf.nn.max_pool(hidden_convolutional_layer_1, ksize=[1, 2, 2, 1],\n",
    "                                                    strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "        hidden_convolutional_layer_2 = tf.nn.relu(\n",
    "            tf.nn.conv2d(hidden_max_pooling_layer_1, convolution_weights_2, strides=[1, 2, 2, 1],\n",
    "                         padding=\"SAME\") + convolution_bias_2)\n",
    "\n",
    "        hidden_max_pooling_layer_2 = tf.nn.max_pool(hidden_convolutional_layer_2, ksize=[1, 2, 2, 1],\n",
    "                                                    strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "        hidden_convolutional_layer_3_flat = tf.reshape(hidden_max_pooling_layer_2, [-1, 256])\n",
    "\n",
    "        final_hidden_activations = tf.nn.relu(\n",
    "            tf.matmul(hidden_convolutional_layer_3_flat, feed_forward_weights_1) + feed_forward_bias_1)\n",
    "\n",
    "        output_layer = tf.matmul(final_hidden_activations, feed_forward_weights_2) + feed_forward_bias_2\n",
    "\n",
    "        return input_layer, output_layer\n",
    "\n",
    "    @staticmethod\n",
    "    def _key_presses_from_action(action_set):\n",
    "        if action_set[0] == 1:\n",
    "            return [K_DOWN]\n",
    "        elif action_set[1] == 1:\n",
    "            return []\n",
    "        elif action_set[2] == 1:\n",
    "            return [K_UP]\n",
    "        raise Exception(\"Unexpected action\")\n",
    "\n",
    "    def get_feedback(self):\n",
    "        from games.half_pong import score\n",
    "\n",
    "        # get the difference in score between this and the last run\n",
    "        score_change = (score - self.last_score)\n",
    "        self.last_score = score\n",
    "\n",
    "        return float(score_change), score_change == -1\n",
    "\n",
    "    def start(self):\n",
    "        super(DeepQHalfPongPlayer, self).start()\n",
    "        half_pong.run(screen_width=self.SCREEN_WIDTH, screen_height=self.SCREEN_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    player = DeepQHalfPongPlayer()\n",
    "    player.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
