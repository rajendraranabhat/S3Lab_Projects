{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AlphaGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Basic Go Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "- - - - - \n",
      "- - - - - \n",
      "- - - - - \n",
      "- - - - - \n",
      "- - - - - \n",
      "()\n",
      "place for o\n",
      "Place or pass (l/a)? l\n",
      "x: 1\n",
      "y: 2\n",
      "()\n",
      "- - - - - \n",
      "- - - - - \n",
      "- o - - - \n",
      "- - - - - \n",
      "- - - - - \n",
      "()\n",
      "place for x\n",
      "Place or pass (l/a)? l\n",
      "x: 3\n",
      "y: 4\n",
      "()\n",
      "- - - - - \n",
      "- - - - - \n",
      "- o - - - \n",
      "- - - - - \n",
      "- - - x - \n",
      "()\n",
      "place for o\n",
      "Place or pass (l/a)? l\n",
      "x: 4\n",
      "y: 4\n",
      "()\n",
      "- - - - - \n",
      "- - - - - \n",
      "- o - - - \n",
      "- - - - - \n",
      "- - - x o \n",
      "()\n",
      "place for x\n",
      "Place or pass (l/a)? a\n",
      "()\n",
      "- - - - - \n",
      "- - - - - \n",
      "- o - - - \n",
      "- - - - - \n",
      "- - - x o \n",
      "()\n",
      "place for o\n",
      "Place or pass (l/a)? a\n",
      "()\n",
      "final board:\n",
      "()\n",
      "- - - - - \n",
      "- - - - - \n",
      "- o - - - \n",
      "- - - - - \n",
      "- - - x o \n",
      "()\n",
      "('o points: ', '2')\n",
      "('x points: ', '1')\n",
      "o wins\n",
      "play again (y/n)? n\n"
     ]
    }
   ],
   "source": [
    "## The number of spots per side of the board\n",
    "## This code allows for an nxn board\n",
    "boardsize = 5\n",
    "## Value determining whether the player wants to quit or not\n",
    "gameon = 1\n",
    "## Lists of groups that have been removed from the board via capture,\n",
    "## held in these varaibles in case, when all captures have been\n",
    "## completed, the board resembles a previous game state and\n",
    "## the move is invalid.  In that case, the groups are restored\n",
    "## from these varaibles.\n",
    "restore_o = []\n",
    "restore_x = []\n",
    " \n",
    "## Generates blank game states\n",
    "def initalize():\n",
    "    gs = []\n",
    "    for i in range(0,boardsize):\n",
    "        gs.append([])\n",
    "        for j in range(0,boardsize):\n",
    "            gs[i].append('-')\n",
    "    return gs\n",
    " \n",
    "## Provides an ascii display of the Go board\n",
    "def printboard(gs):\n",
    "    global boardsize\n",
    "    for row in gs:\n",
    "        rowprint = ''\n",
    "        for element in row:\n",
    "            rowprint += element\n",
    "            rowprint += ' '\n",
    "        print(rowprint)\n",
    " \n",
    "## Returns a list of the board positions surrounding the\n",
    "## passed group.\n",
    "def gperm(group):\n",
    "    permimeter = []\n",
    "    global boardsize\n",
    "    hit = 0\n",
    "    loss = 0\n",
    "    ## Adds permimeter spots below\n",
    "    ## Works by looking from top to bottom, left to right,\n",
    "    ## at each posisition on the board.  When a posistion\n",
    "    ## is hit that is in the given group, I set hit = 1.\n",
    "    ## Then, at the next position that is not in that group,\n",
    "    ## or if the end of the column is reached, I set loss = 1.\n",
    "    ## That point is the first point below a point in that group,\n",
    "    ## so it is part of the permieter of that group.\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < boardsize:\n",
    "        j = 0\n",
    "        hit = 0\n",
    "        while j < boardsize:\n",
    "            if [i,j] in group:\n",
    "                hit = 1\n",
    "            elif (hit == 1) & ([i,j] not in group):\n",
    "                loss = 1\n",
    "            if (hit == 1) & (loss == 1):\n",
    "                permimeter.append([i,j])\n",
    "                hit = 0\n",
    "                loss = 0\n",
    "            j += 1\n",
    "        i += 1\n",
    "    ## Adds permimeter spots to the right\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < boardsize:\n",
    "        j = 0\n",
    "        hit = 0\n",
    "        while j < boardsize:\n",
    "            if [j,i] in group:\n",
    "                hit = 1\n",
    "            elif (hit == 1) & ([j,i] not in group):\n",
    "                loss = 1\n",
    "            if (hit == 1) & (loss == 1):\n",
    "                permimeter.append([j,i])\n",
    "                hit = 0\n",
    "                loss = 0\n",
    "            j += 1\n",
    "        i += 1\n",
    "    ## Adds permimeter spots above\n",
    "    i = 0\n",
    "    j = boardsize-1\n",
    "    while i < boardsize:\n",
    "        j = boardsize-1\n",
    "        hit = 0\n",
    "        while j >= 0:\n",
    "            if [i,j] in group:\n",
    "                hit = 1\n",
    "            elif (hit == 1) & ([i,j] not in group):\n",
    "                loss = 1\n",
    "            if (hit == 1) & (loss == 1):\n",
    "                permimeter.append([i,j])\n",
    "                hit = 0\n",
    "                loss = 0\n",
    "            j -= 1\n",
    "        i += 1\n",
    "    ## Adds permimeter spots to the left\n",
    "    i = 0\n",
    "    j = boardsize-1\n",
    "    while i < boardsize:\n",
    "        j = boardsize-1\n",
    "        hit = 0\n",
    "        while j >= 0:\n",
    "            if [j,i] in group:\n",
    "                hit = 1\n",
    "            elif (hit == 1) & ([j,i] not in group):\n",
    "                loss = 1\n",
    "            if (hit == 1) & (loss == 1):\n",
    "                permimeter.append([j,i])\n",
    "                hit = 0\n",
    "                loss = 0\n",
    "            j -= 1\n",
    "        i += 1\n",
    "    return permimeter\n",
    " \n",
    "## Returns a string that describes the game state\n",
    "def readable(gs):\n",
    "    readthis = ''\n",
    "    readthis += '<<'\n",
    "    for row in gs:\n",
    "        for element in row:\n",
    "            readthis += element\n",
    "    readthis += '>>'\n",
    "    return readthis\n",
    " \n",
    "## Counts the territory captured by each player\n",
    "def count():\n",
    "    global gsc\n",
    "    global non_groups\n",
    "    global o_points\n",
    "    global x_points\n",
    "    global boardsize\n",
    " \n",
    "    ## Creates a list of groups (non_groups) of empty positions.\n",
    "    for i in range(0,boardsize):\n",
    "        for j in range(0,boardsize):\n",
    "            if gsc[j][i] == '-':\n",
    "                new = 1\n",
    "                for group in non_groups:\n",
    "                    if [i,j] in gperm(group):\n",
    "                        group.append([i,j])\n",
    "                        new = 0\n",
    "                if new == 1:\n",
    "                    non_groups.append([[i,j]])\n",
    "    concat('-')\n",
    " \n",
    "    o_points = 0\n",
    "    x_points = 0\n",
    " \n",
    "    ## Gives a point to the each player for every pebble they have\n",
    "    ## on the board.\n",
    "    for group in o_groups:\n",
    "        o_points += len(group)\n",
    "    for group in x_groups:\n",
    "        x_points += len(group)\n",
    " \n",
    "    ## The permimeter of these empty positions is here considered,\n",
    "    ## and if every position in the permimeter of a non_group is\n",
    "    ## one player or the other, that player gains a number of points\n",
    "    ## equal to the length of that group (the number of positions\n",
    "    ## that their pieces enclose).\n",
    "    for group in non_groups:\n",
    "        no = 0\n",
    "        for element in gperm(group):\n",
    "            if gsc[element[1]][element[0]] != 'o':\n",
    "                no = 1\n",
    "        if no == 0:\n",
    "            o_points += len(group)\n",
    " \n",
    "    for group in non_groups:\n",
    "        no = 0\n",
    "        for element in gperm(group):\n",
    "            if gsc[element[1]][element[0]] != 'x':\n",
    "                no = 1\n",
    "        if no == 0:\n",
    "            x_points += len(group)\n",
    " \n",
    "## Checks for capture, and removes the captured pieces from the board\n",
    "def capture(xoro):\n",
    "    global o_groups\n",
    "    global x_groups\n",
    "    global gsf\n",
    "    global restore_o\n",
    "    global restore_x\n",
    "    global edited\n",
    "    if xoro == 'o':\n",
    "        groups = x_groups\n",
    "        otherplayer = 'o'\n",
    "    else:\n",
    "        groups = o_groups\n",
    "        otherplayer = 'x'\n",
    " \n",
    "    ## Checks to see, for each group of a particular player,\n",
    "    ## whether any of the board positions in the\n",
    "    ## perimeter around that group are held by the other player.\n",
    "    ## If any position is not held by the other player,\n",
    "    ## the group is not captured, and is safe.  Otherwise,\n",
    "    ## the group is removed.  But we haven't tested this yet\n",
    "    ## to see if this would return the board to a previous\n",
    "    ## state, so we save the removed groups with the restore lists.\n",
    "    for group in groups:\n",
    "        safe = 0\n",
    "        for element in gperm(group):\n",
    "            if gsf[element[1]][element[0]] != otherplayer:\n",
    "                safe = 1\n",
    "        if safe != 1:\n",
    "            edited = 1\n",
    "            if xoro == 'o':\n",
    "                restore_x.append(group)\n",
    "            else:\n",
    "                restore_o.append(group)\n",
    "            groups.remove(group)\n",
    " \n",
    "    # Sets gsf given the new captures\n",
    "    gsf = initalize()\n",
    "    for group in o_groups:\n",
    "        for point in group:\n",
    "            gsf[point[1]][point[0]] = 'o'\n",
    "    for group in x_groups:\n",
    "        for point in group:\n",
    "            gsf[point[1]][point[0]] = 'x'\n",
    " \n",
    "## Checks to see if the new game state, created by the most recent\n",
    "## move, returns the board to a previous state.  If not, then\n",
    "## gsc is set as this new state, and gsp is set as what gsc was, and\n",
    "## the new game state is stored in gscache.  The function returns 1\n",
    "## if the move is valid, 0 otherwise.\n",
    "def goodmove():\n",
    "    global gscache\n",
    "    global gsc\n",
    "    global gsp\n",
    "    global gsf\n",
    "    if readable(gsf) not in gscache:\n",
    "        gsp = []\n",
    "        gsc = []\n",
    "        for element in gsf:\n",
    "            gsp.append(element)\n",
    "            gsc.append(element)\n",
    "        gscache += readable(gsf)\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "## Checks if any groups contain the same point;\n",
    "## if so, joins them into one group\n",
    "def concat(xoro):\n",
    "    global o_groups\n",
    "    global x_groups\n",
    "    global non_groups\n",
    "    if xoro == 'o':\n",
    "        groups = o_groups\n",
    "    elif xoro == 'x':\n",
    "        groups = x_groups\n",
    "    else:\n",
    "        groups = non_groups\n",
    "    i = 0\n",
    "    ## currentgroups and previousgroups are used to compare the number\n",
    "    ## of groups before this nest of whiles to the number after.  If\n",
    "    ## The number is the same, then nothing needed to be concatinated,\n",
    "    ## and we can move on.  If the number is different, two groups\n",
    "    ## were concatinated, and we need to run through this nest again\n",
    "    ## to see if any other groups need to be joined together.\n",
    "    currentgroups = len(groups)\n",
    "    previousgroups = currentgroups + 1\n",
    "    ## Checks if the positions contained in any group are to be\n",
    "    ## found in any other group.  If so, all elements of the second are\n",
    "    ## added to the first, and the first is deleted.\n",
    "    while previousgroups != currentgroups:\n",
    "        while i < len(groups)-1:\n",
    "            reset = 0\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                k = 0\n",
    "                while k < len(groups[i]):\n",
    "                    if groups[i][k] in groups[j]:\n",
    "                        for element in groups[j]:\n",
    "                            if element not in groups[i]:\n",
    "                                groups[i].append(element)\n",
    "                        groups.remove(groups[j])\n",
    "                        reset = 1\n",
    "                    if reset == 1:\n",
    "                        break\n",
    "                    k += 1\n",
    "                j += 1\n",
    "            if reset == 1:\n",
    "                i = -1\n",
    "            i += 1\n",
    "        previousgroups = currentgroups\n",
    "        currentgroups = len(groups)\n",
    " \n",
    "## Adds point xy to a group if xy is in the\n",
    "## perimeter of an existing group, or creates\n",
    "## new group if xy is not a part of any existing group.\n",
    "def addpoint(xy,xoro):\n",
    "    global o_groups\n",
    "    global x_groups\n",
    "    if xoro == 'o':\n",
    "        groups = o_groups\n",
    "    else:\n",
    "        groups = x_groups\n",
    "    new = 1\n",
    "    for group in groups:\n",
    "        if xy in gperm(group):\n",
    "            group.append(xy)\n",
    "            new = 0\n",
    "    if new == 1:\n",
    "        groups.append([xy])\n",
    " \n",
    "## Lets the player select a move.\n",
    "def selectmove(xoro):\n",
    "    global boardsize\n",
    "    global gsf\n",
    "    hold = 1\n",
    "    while hold == 1:\n",
    " \n",
    "        minihold = 1\n",
    "        while minihold == 1:\n",
    "            pp = raw_input('Place or pass (l/a)? ')\n",
    "            if pp == 'a':\n",
    "                return 'pass'\n",
    "            elif pp == 'l':\n",
    "                minihold = 0\n",
    "                ## This try...except ensures that the user\n",
    "                ## raw_inputs only numbers\n",
    "                error = 0\n",
    "                try:\n",
    "                    x = int(raw_input('x: '))\n",
    "                except ValueError:\n",
    "                    error = 1\n",
    "                try:\n",
    "                    y = int(raw_input('y: '))\n",
    "                except ValueError:\n",
    "                    error = 1\n",
    "                if error == 1:\n",
    "                    minihold = 1\n",
    "                    print('invalid')\n",
    "            else:\n",
    "                print('invalid')\n",
    "        ## Ensures that the raw_input is on the board\n",
    "        if (x > boardsize) | (x < 0) | (y > boardsize) | (y < 0):\n",
    "            print('invalid')\n",
    "        elif gsc[y][x] != '-':\n",
    "            print('invalid')\n",
    "        else:\n",
    "            hold = 0\n",
    "    ## Places the piece on the 'future' board, the board\n",
    "    ## used to test if a move is valid\n",
    "    if xoro == 'o':\n",
    "        gsf[y][x] = 'o'\n",
    "    else:\n",
    "        gsf[y][x] = 'x'\n",
    " \n",
    "    return [x,y]\n",
    " \n",
    "## The 'turn,' in which a player makes a move,\n",
    "## the captures caused by that piece are made,\n",
    "## the validity of the move is checked, and\n",
    "## the endgame status is checked.\n",
    "def turn():\n",
    "    global xoro\n",
    "    global notxoro\n",
    "    global player1_pass\n",
    "    global player2_pass\n",
    "    global gameover\n",
    "    hold = 1\n",
    "    while hold == 1:\n",
    "        print()\n",
    "        print('place for '+xoro)\n",
    "        ## By calling selectmove(), the player\n",
    "        ## is given the option of whether to place\n",
    "        ## a piece or to pass, and where to place\n",
    "        ## that piece.\n",
    "        xy = selectmove(xoro)\n",
    "        if xy == 'pass':\n",
    "            if xoro == 'o':\n",
    "                player1_pass = 1\n",
    "            else:\n",
    "                player2_pass = 1\n",
    "            hold = 0\n",
    "        ## If the player doesn't pass...\n",
    "        else:\n",
    "            player1_pass = 0\n",
    "            player2_pass = 0\n",
    "            ## The new piece is added to its group,\n",
    "            ## or a new group is created for it.\n",
    "            addpoint(xy,xoro)\n",
    "            ## Groups that have been connected by\n",
    "            ## the this placement are joined together\n",
    "            concat(xoro)\n",
    "            minihold = 1\n",
    "            ## Edited is a value used to check\n",
    "            ## whether any capture is made.  capture()\n",
    "            ## is called as many times as until no pieces\n",
    "            ## are capture (until edited does not change\n",
    "            ## to 1)\n",
    "            edited = 0\n",
    "            while minihold == 1:\n",
    "                restore_o = []\n",
    "                restore_x = []\n",
    "                capture(xoro)\n",
    "                capture(notxoro)\n",
    "                if edited == 0:\n",
    "                    minihold = 0\n",
    "                    edited = 0\n",
    "                else:\n",
    "                    edited = 0\n",
    "            ## Checks to see if the move, given all the\n",
    "            ## captures it causes, would return the board\n",
    "            ## to a previous game state.\n",
    "            if goodmove() == 1:\n",
    "                hold = 0\n",
    "            ## If the move is invalid, the captured groups need\n",
    "            ## to be returned to the board, so we use\n",
    "            ## the groups stored in the restore lists to\n",
    "            ## restore the o_ and x_groups lists.\n",
    "            else:\n",
    "                print('invalid move - that returns to board to a previous state')\n",
    "                for group in restore_o:\n",
    "                    o_groups.append(group)\n",
    "                for group in restore_x:\n",
    "                    x_groups.append(group)\n",
    "    if (player1_pass == 1) & (player2_pass == 1):\n",
    "        gameover = 1\n",
    " \n",
    "## Called to start a game\n",
    "def main():\n",
    "    ## Either 'o' or 'x', determines who's turn it is\n",
    "    global xoro\n",
    "    ## The opposite of xoro, determines who's turn it is not\n",
    "    global notxoro\n",
    "    ## Game State Current, the current layout of the board\n",
    "    ## This value is two-dimensional list, the higher dimension being\n",
    "    ## lists representing the rows and the lower dimension being\n",
    "    ## strings representing individual positions on the board.\n",
    "    ## These strings are either '-', 'o', or 'x'\n",
    "    global gsc\n",
    "    ## 0 or 1, determins whether the current game is ongoing or ended\n",
    "    global gameover\n",
    "    ## Game State Future, same setup as gsc, used for testing the\n",
    "    ## waters of a new move, to see if that move is valid, before\n",
    "    ## gsc is edited to reflect that move\n",
    "    global gsf\n",
    "    ## Two-dimensional lists, the higher dimension being groups, the\n",
    "    ## lower dimension being lists of board positions in a particular\n",
    "    ## group\n",
    "    global o_groups\n",
    "    global x_groups\n",
    "    ## Groups of empty positions\n",
    "    global non_groups\n",
    "    ## String containing all the game states encountered in a particular\n",
    "    ## game, used to check validity of moves\n",
    "    global gscache\n",
    "    ## 0 or 1, for whether the player has passed their turn or not\n",
    "    global player1_pass\n",
    "    global player2_pass\n",
    "    ## Integer value reflecting the score of a player\n",
    "    global o_points\n",
    "    global x_points\n",
    " \n",
    "    ## Creates a blank game state - a blank board\n",
    "    gsc = initalize()\n",
    "    gsf = initalize()\n",
    "    ## Sets initial values\n",
    "    o_groups = []\n",
    "    x_groups = []\n",
    "    non_groups = []\n",
    "    gscache = ''\n",
    "    player1_pass = 0\n",
    "    player2_pass = 0\n",
    "    gameover = 0\n",
    "    o_points = 0\n",
    "    x_points = 0\n",
    " \n",
    "    ## Gives players turns until the end of the game\n",
    "    ## (that is, until both players pass, one after\n",
    "    ## the other)\n",
    "    while gameover != 1:\n",
    " \n",
    "        ## Set it as o-player's turn\n",
    "        xoro = 'o'\n",
    "        notxoro = 'x'\n",
    "        print()\n",
    "        printboard(gsc)\n",
    " \n",
    "        turn()\n",
    "        if gameover == 1:\n",
    "            break\n",
    " \n",
    "        ## Sets it as x-player's turn\n",
    "        xoro = 'x'\n",
    "        notxoro = 'o'\n",
    "        print()\n",
    "        printboard(gsc)\n",
    " \n",
    "        turn()\n",
    " \n",
    "    ## Counts the score of both players\n",
    "    count()\n",
    "    print()\n",
    "    print('final board:')\n",
    "    print()\n",
    "    printboard(gsc)\n",
    "    print()\n",
    "    print('o points: ',str(o_points))\n",
    "    print('x points: ',str(x_points))\n",
    "    ## Determines the winner\n",
    "    if o_points > x_points:\n",
    "        print('o wins')\n",
    "    elif x_points > o_points:\n",
    "        print('x wins')\n",
    "    else:\n",
    "        print('tie')\n",
    " \n",
    "## Finally something that is not a function!\n",
    "## This while loop will start new games for as\n",
    "## long as the user choses to.\n",
    "while gameon == 1:\n",
    "    main()\n",
    "    hold = 1\n",
    "    while hold == 1:\n",
    "        yn = raw_input('play again (y/n)? ')\n",
    "        if yn == 'n':\n",
    "            gameon = 0\n",
    "            hold = 0\n",
    "        elif yn == 'y':\n",
    "            hold = 0\n",
    "        else:\n",
    "            print('invalid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/Rochester-NRT/RocAlphaGo/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-b96253035da6>, line 240)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-b96253035da6>\"\u001b[1;36m, line \u001b[1;32m240\u001b[0m\n\u001b[1;33m    print \"Due to skipping, ended with {} layers instead of {}\".format(layer, params['layers'])\u001b[0m\n\u001b[1;37m                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Policy network\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import convolutional, merge, Input, BatchNormalization\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from AlphaGo.util import flatten_idx\n",
    "from AlphaGo.models.nn_util import Bias, NeuralNetBase, neuralnet\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@neuralnet\n",
    "class CNNPolicy(NeuralNetBase):\n",
    "\t\"\"\"uses a convolutional neural network to evaluate the state of the game\n",
    "\tand compute a probability distribution over the next action\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef _select_moves_and_normalize(self, nn_output, moves, size):\n",
    "\t\t\"\"\"helper function to normalize a distribution over the given list of moves\n",
    "\t\tand return a list of (move, prob) tuples\n",
    "\t\t\"\"\"\n",
    "\t\tif len(moves) == 0:\n",
    "\t\t\treturn []\n",
    "\t\tmove_indices = [flatten_idx(m, size) for m in moves]\n",
    "\t\t# get network activations at legal move locations\n",
    "\t\tdistribution = nn_output[move_indices]\n",
    "\t\tdistribution = distribution / distribution.sum()\n",
    "\t\treturn zip(moves, distribution)\n",
    "\n",
    "\tdef batch_eval_state(self, states, moves_lists=None):\n",
    "\t\t\"\"\"Given a list of states, evaluates them all at once to make best use of GPU\n",
    "\t\tbatching capabilities.\n",
    "\n",
    "\t\tAnalogous to [eval_state(s) for s in states]\n",
    "\n",
    "\t\tReturns: a parallel list of move distributions as in eval_state\n",
    "\t\t\"\"\"\n",
    "\t\tn_states = len(states)\n",
    "\t\tif n_states == 0:\n",
    "\t\t\treturn []\n",
    "\t\tstate_size = states[0].size\n",
    "\t\tif not all([st.size == state_size for st in states]):\n",
    "\t\t\traise ValueError(\"all states must have the same size\")\n",
    "\t\t# concatenate together all one-hot encoded states along the 'batch' dimension\n",
    "\t\tnn_input = np.concatenate([self.preprocessor.state_to_tensor(s) for s in states], axis=0)\n",
    "\t\t# pass all input through the network at once (backend makes use of batches if len(states) is large)\n",
    "\t\tnetwork_output = self.forward(nn_input)\n",
    "\t\t# default move lists to all legal moves\n",
    "\t\tmoves_lists = moves_lists or [st.get_legal_moves() for st in states]\n",
    "\t\tresults = [None] * n_states\n",
    "\t\tfor i in range(n_states):\n",
    "\t\t\tresults[i] = self._select_moves_and_normalize(network_output[i], moves_lists[i], state_size)\n",
    "\t\treturn results\n",
    "\n",
    "\tdef eval_state(self, state, moves=None):\n",
    "\t\t\"\"\"Given a GameState object, returns a list of (action, probability) pairs\n",
    "\t\taccording to the network outputs\n",
    "\n",
    "\t\tIf a list of moves is specified, only those moves are kept in the distribution\n",
    "\t\t\"\"\"\n",
    "\t\ttensor = self.preprocessor.state_to_tensor(state)\n",
    "\t\t# run the tensor through the network\n",
    "\t\tnetwork_output = self.forward(tensor)\n",
    "\t\tmoves = moves or state.get_legal_moves()\n",
    "\t\treturn self._select_moves_and_normalize(network_output[0], moves, state.size)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef create_network(**kwargs):\n",
    "\t\t\"\"\"construct a convolutional neural network.\n",
    "\n",
    "\t\tKeword Arguments:\n",
    "\t\t- input_dim:         \tdepth of features to be processed by first layer (no default)\n",
    "\t\t- board:             \twidth of the go board to be processed (default 19)\n",
    "\t\t- filters_per_layer: \tnumber of filters used on every layer (default 128)\n",
    "\t\t- layers:            \tnumber of convolutional steps (default 12)\n",
    "\t\t- filter_width_K:    \t(where K is between 1 and <layers>) width of filter on\n",
    "\t\t\t\t\t\t\t\tlayer K (default 3 except 1st layer which defaults to 5).\n",
    "\t\t\t\t\t\t\t\tMust be odd.\n",
    "\t\t\"\"\"\n",
    "\t\tdefaults = {\n",
    "\t\t\t\"board\": 19,\n",
    "\t\t\t\"filters_per_layer\": 128,\n",
    "\t\t\t\"layers\": 12,\n",
    "\t\t\t\"filter_width_1\": 5\n",
    "\t\t}\n",
    "\t\t# copy defaults, but override with anything in kwargs\n",
    "\t\tparams = defaults\n",
    "\t\tparams.update(kwargs)\n",
    "\n",
    "\t\t# create the network:\n",
    "\t\t# a series of zero-paddings followed by convolutions\n",
    "\t\t# such that the output dimensions are also board x board\n",
    "\t\tnetwork = Sequential()\n",
    "\n",
    "\t\t# create first layer\n",
    "\t\tnetwork.add(convolutional.Convolution2D(\n",
    "\t\t\tinput_shape=(params[\"input_dim\"], params[\"board\"], params[\"board\"]),\n",
    "\t\t\tnb_filter=params[\"filters_per_layer\"],\n",
    "\t\t\tnb_row=params[\"filter_width_1\"],\n",
    "\t\t\tnb_col=params[\"filter_width_1\"],\n",
    "\t\t\tinit='uniform',\n",
    "\t\t\tactivation='relu',\n",
    "\t\t\tborder_mode='same'))\n",
    "\n",
    "\t\t# create all other layers\n",
    "\t\tfor i in range(2, params[\"layers\"] + 1):\n",
    "\t\t\t# use filter_width_K if it is there, otherwise use 3\n",
    "\t\t\tfilter_key = \"filter_width_%d\" % i\n",
    "\t\t\tfilter_width = params.get(filter_key, 3)\n",
    "\t\t\tnetwork.add(convolutional.Convolution2D(\n",
    "\t\t\t\tnb_filter=params[\"filters_per_layer\"],\n",
    "\t\t\t\tnb_row=filter_width,\n",
    "\t\t\t\tnb_col=filter_width,\n",
    "\t\t\t\tinit='uniform',\n",
    "\t\t\t\tactivation='relu',\n",
    "\t\t\t\tborder_mode='same'))\n",
    "\n",
    "\t\t# the last layer maps each <filters_per_layer> feature to a number\n",
    "\t\tnetwork.add(convolutional.Convolution2D(\n",
    "\t\t\tnb_filter=1,\n",
    "\t\t\tnb_row=1,\n",
    "\t\t\tnb_col=1,\n",
    "\t\t\tinit='uniform',\n",
    "\t\t\tborder_mode='same'))\n",
    "\t\t# reshape output to be board x board\n",
    "\t\tnetwork.add(Flatten())\n",
    "\t\t# add a bias to each board location\n",
    "\t\tnetwork.add(Bias())\n",
    "\t\t# softmax makes it into a probability distribution\n",
    "\t\tnetwork.add(Activation('softmax'))\n",
    "\n",
    "\t\treturn network\n",
    "\n",
    "\n",
    "@neuralnet\n",
    "class ResnetPolicy(CNNPolicy):\n",
    "\t\"\"\"Residual network architecture as per He at al. 2015\n",
    "\t\"\"\"\n",
    "\t@staticmethod\n",
    "\tdef create_network(**kwargs):\n",
    "\t\t\"\"\"construct a convolutional neural network with Resnet-style skip connections.\n",
    "\t\tArguments are the same as with the default CNNPolicy network, except the default\n",
    "\t\tnumber of layers is 20 plus a new n_skip parameter\n",
    "\n",
    "\t\tKeword Arguments:\n",
    "\t\t- input_dim:         \tdepth of features to be processed by first layer (no default)\n",
    "\t\t- board:             \twidth of the go board to be processed (default 19)\n",
    "\t\t- filters_per_layer: \tnumber of filters used on every layer (default 128)\n",
    "\t\t- layers:            \tnumber of convolutional steps (default 20)\n",
    "\t\t- filter_width_K:    \t(where K is between 1 and <layers>) width of filter on\n",
    "\t\t\t\t\t\t\t\tlayer K (default 3 except 1st layer which defaults to 5).\n",
    "\t\t\t\t\t\t\t\tMust be odd.\n",
    "\t\t- n_skip_K:             (where K is as in filter_width_K) number of convolutional\n",
    "\t\t\t\t\t\t\t\tlayers to skip with the linear path starting at K. Only valid\n",
    "\t\t\t\t\t\t\t\tat K >= 1. (Each layer defaults to 1)\n",
    "\n",
    "\t\tNote that n_skip_1=s means that the next valid value of n_skip_* is 3\n",
    "\n",
    "\t\tA diagram may help explain (numbers indicate layer):\n",
    "\n",
    "\t\t\t1             2              3                   4              5              6\n",
    "\t\tI--C -- B -- R -- C -- B -- R -- C -- M -- B -- R -- C -- B -- R -- C -- B -- R -- C -- M  ...  M  -- R -- F -- O\n",
    "\t\t\t\\___________________________/ \\____________________________________________________/ \\ ... /\n",
    "\t\t\t\t\t[n_skip_1 = 2]                             [n_skip_3 = 3]\n",
    "\n",
    "\t\tI - input\n",
    "\t\tB - BatchNormalization\n",
    "\t\tR - ReLU\n",
    "\t\tC - Conv2D\n",
    "\t\tF - Flatten\n",
    "\t\tO - output\n",
    "\t\tM - merge\n",
    "\n",
    "\t\tThe input is always passed through a Conv2D layer, the output of which layer is counted as '1'.\n",
    "\t\tEach subsequent [R -- C] block is counted as one 'layer'. The 'merge' layer isn't counted; hence\n",
    "\t\tif n_skip_1 is 2, the next valid skip parameter is n_skip_3, which will start at the output\n",
    "\t\tof the merge\n",
    "\t\t\"\"\"\n",
    "\t\tdefaults = {\n",
    "\t\t\t\"board\": 19,\n",
    "\t\t\t\"filters_per_layer\": 128,\n",
    "\t\t\t\"layers\": 20,\n",
    "\t\t\t\"filter_width_1\": 5\n",
    "\t\t}\n",
    "\t\t# copy defaults, but override with anything in kwargs\n",
    "\t\tparams = defaults\n",
    "\t\tparams.update(kwargs)\n",
    "\n",
    "\t\t# create the network using Keras' functional API,\n",
    "\t\t# since this isn't 'Sequential'\n",
    "\t\tmodel_input = Input(shape=(params[\"input_dim\"], params[\"board\"], params[\"board\"]))\n",
    "\n",
    "\t\t# create first layer\n",
    "\t\tconvolution_path = convolutional.Convolution2D(\n",
    "\t\t\tinput_shape=(),\n",
    "\t\t\tnb_filter=params[\"filters_per_layer\"],\n",
    "\t\t\tnb_row=params[\"filter_width_1\"],\n",
    "\t\t\tnb_col=params[\"filter_width_1\"],\n",
    "\t\t\tinit='uniform',\n",
    "\t\t\tactivation='linear',  # relu activations done inside resnet modules\n",
    "\t\t\tborder_mode='same')(model_input)\n",
    "\n",
    "\t\tdef add_resnet_unit(path, K, **params):\n",
    "\t\t\t\"\"\"Add a resnet unit to path starting at layer 'K',\n",
    "\t\t\tadding as many (ReLU + Conv2D) modules as specified by n_skip_K\n",
    "\n",
    "\t\t\tReturns new path and next layer index, i.e. K + n_skip_K, in a tuple\n",
    "\t\t\t\"\"\"\n",
    "\t\t\t# loosely based on https://github.com/keunwoochoi/residual_block_keras\n",
    "\t\t\t# (see also keras docs here: http://keras.io/getting-started/functional-api-guide/#all-models-are-callable-just-like-layers)\n",
    "\n",
    "\t\t\tblock_input = path\n",
    "\t\t\t# use n_skip_K if it is there, default to 1\n",
    "\t\t\tskip_key = \"n_skip_%d\" % K\n",
    "\t\t\tn_skip = params.get(skip_key, 1)\n",
    "\t\t\tfor i in range(n_skip):\n",
    "\t\t\t\tlayer = K + i\n",
    "\t\t\t\t# add BatchNorm\n",
    "\t\t\t\tpath = BatchNormalization()(path)\n",
    "\t\t\t\t# add ReLU\n",
    "\t\t\t\tpath = Activation('relu')(path)\n",
    "\t\t\t\t# use filter_width_K if it is there, otherwise use 3\n",
    "\t\t\t\tfilter_key = \"filter_width_%d\" % layer\n",
    "\t\t\t\tfilter_width = params.get(filter_key, 3)\n",
    "\t\t\t\t# add Conv2D\n",
    "\t\t\t\tpath = convolutional.Convolution2D(\n",
    "\t\t\t\t\tnb_filter=params[\"filters_per_layer\"],\n",
    "\t\t\t\t\tnb_row=filter_width,\n",
    "\t\t\t\t\tnb_col=filter_width,\n",
    "\t\t\t\t\tinit='uniform',\n",
    "\t\t\t\t\tactivation='linear',\n",
    "\t\t\t\t\tborder_mode='same')(path)\n",
    "\t\t\t# Merge 'input layer' with the path\n",
    "\t\t\tpath = merge([block_input, path], mode='sum')\n",
    "\t\t\treturn path, K + n_skip\n",
    "\n",
    "\t\t# create all other layers\n",
    "\t\tlayer = 1\n",
    "\t\twhile layer < params['layers']:\n",
    "\t\t\tconvolution_path, layer = add_resnet_unit(convolution_path, layer, **params)\n",
    "\t\tif layer > params['layers']:\n",
    "\t\t\tprint \"Due to skipping, ended with {} layers instead of {}\".format(layer, params['layers'])\n",
    "\n",
    "\t\t# since each layer's activation was linear, need one more ReLu\n",
    "\t\tconvolution_path = Activation('relu')(convolution_path)\n",
    "\n",
    "\t\t# the last layer maps each <filters_per_layer> featuer to a number\n",
    "\t\tconvolution_path = convolutional.Convolution2D(\n",
    "\t\t\tnb_filter=1,\n",
    "\t\t\tnb_row=1,\n",
    "\t\t\tnb_col=1,\n",
    "\t\t\tinit='uniform',\n",
    "\t\t\tborder_mode='same')(convolution_path)\n",
    "\t\t# flatten output\n",
    "\t\tnetwork_output = Flatten()(convolution_path)\n",
    "\t\t# add a bias to each board location\n",
    "\t\tnetwork_output = Bias()(network_output)\n",
    "\t\t# softmax makes it into a probability distribution\n",
    "\t\tnetwork_output = Activation('softmax')(network_output)\n",
    "\n",
    "\t\treturn Model(input=[model_input], output=[network_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#Value network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import convolutional\n",
    "from keras.layers.core import Dense, Flatten\n",
    "# from SGD_exponential_decay import SGD_exponential_decay as SGD\n",
    "\n",
    "### Parameters obtained from paper ###\n",
    "K = 152                        # depth of convolutional layers\n",
    "LEARNING_RATE = .003           # initial learning rate\n",
    "DECAY = 8.664339379294006e-08  # rate of exponential learning_rate decay\n",
    "\n",
    "\n",
    "class value_trainer:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(convolutional.Convolution2D(\n",
    "            input_shape=(49, 19, 19), nb_filter=K, nb_row=5, nb_col=5,\n",
    "            init='uniform', activation='relu', border_mode='same'))\n",
    "        for i in range(2, 13):\n",
    "            self.model.add(convolutional.Convolution2D(\n",
    "                nb_filter=K, nb_row=3, nb_col=3,\n",
    "                init='uniform', activation='relu', border_mode='same'))\n",
    "\n",
    "        self.model.add(convolutional.Convolution2D(\n",
    "            nb_filter=1, nb_row=1, nb_col=1,\n",
    "            init='uniform', activation='linear', border_mode='same'))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(256, init='uniform'))\n",
    "        self.model.add(Dense(1, init='uniform', activation=\"tanh\"))\n",
    "\n",
    "        # sgd = SGD(lr=LEARNING_RATE, decay=DECAY)\n",
    "        # self.model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "    def get_samples(self):\n",
    "        # TODO non-terminating loop that draws training samples uniformly at random\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        # TODO use self.model.fit_generator to train from data source\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer = value_trainer()\n",
    "    # TODO command line instantiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named AlphaGo.preprocessing.preprocessing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c479326f8200>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mAlphaGo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named AlphaGo.preprocessing.preprocessing"
     ]
    }
   ],
   "source": [
    "##NN_util\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.engine.topology import Layer\n",
    "from AlphaGo.preprocessing.preprocessing import Preprocess\n",
    "import json\n",
    "\n",
    "\n",
    "class NeuralNetBase(object):\n",
    "\t\"\"\"Base class for neural network classes handling feature processing, construction\n",
    "\tof a 'forward' function, etc.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# keep track of subclasses to make generic saving/loading cleaner.\n",
    "\t# subclasses can be 'registered' with the @neuralnet decorator\n",
    "\tsubclasses = {}\n",
    "\n",
    "\tdef __init__(self, feature_list, **kwargs):\n",
    "\t\t\"\"\"create a neural net object that preprocesses according to feature_list and uses\n",
    "\t\ta neural network specified by keyword arguments (using subclass' create_network())\n",
    "\n",
    "\t\toptional argument: init_network (boolean). If set to False, skips initializing\n",
    "\t\tself.model and self.forward and the calling function should set them.\n",
    "\t\t\"\"\"\n",
    "\t\tself.preprocessor = Preprocess(feature_list)\n",
    "\t\tkwargs[\"input_dim\"] = self.preprocessor.output_dim\n",
    "\n",
    "\t\tif kwargs.get('init_network', True):\n",
    "\t\t\t# self.__class__ refers to the subclass so that subclasses only\n",
    "\t\t\t# need to override create_network()\n",
    "\t\t\tself.model = self.__class__.create_network(**kwargs)\n",
    "\t\t\t# self.forward is a lambda function wrapping a Keras function\n",
    "\t\t\tself.forward = self._model_forward()\n",
    "\n",
    "\tdef _model_forward(self):\n",
    "\t\t\"\"\"Construct a function using the current keras backend that, when given a batch\n",
    "\t\tof inputs, simply processes them forward and returns the output\n",
    "\n",
    "\t\tThis is as opposed to model.compile(), which takes a loss function\n",
    "\t\tand training method.\n",
    "\n",
    "\t\tc.f. https://github.com/fchollet/keras/issues/1426\n",
    "\t\t\"\"\"\n",
    "\t\t# The uses_learning_phase property is True if the model contains layers that behave\n",
    "\t\t# differently during training and testing, e.g. Dropout or BatchNormalization.\n",
    "\t\t# In these cases, K.learning_phase() is a reference to a backend variable that should\n",
    "\t\t# be set to 0 when using the network in prediction mode and is automatically set to 1\n",
    "\t\t# during training.\n",
    "\t\tif self.model.uses_learning_phase:\n",
    "\t\t\tforward_function = K.function([self.model.input, K.learning_phase()], [self.model.output])\n",
    "\n",
    "\t\t\t# the forward_function returns a list of tensors\n",
    "\t\t\t# the first [0] gets the front tensor.\n",
    "\t\t\treturn lambda inpt: forward_function([inpt, 0])[0]\n",
    "\t\telse:\n",
    "\t\t\t# identical but without a second input argument for the learning phase\n",
    "\t\t\tforward_function = K.function([self.model.input], [self.model.output])\n",
    "\t\t\treturn lambda inpt: forward_function([inpt])[0]\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef load_model(json_file):\n",
    "\t\t\"\"\"create a new neural net object from the architecture specified in json_file\n",
    "\t\t\"\"\"\n",
    "\t\twith open(json_file, 'r') as f:\n",
    "\t\t\tobject_specs = json.load(f)\n",
    "\n",
    "\t\t# Create object; may be a subclass of networks saved in specs['class']\n",
    "\t\tclass_name = object_specs.get('class', 'CNNPolicy')\n",
    "\t\ttry:\n",
    "\t\t\tnetwork_class = NeuralNetBase.subclasses[class_name]\n",
    "\t\texcept KeyError:\n",
    "\t\t\traise ValueError(\"Unknown neural network type in json file: {}\\n(was it registered with the @neuralnet decorator?)\".format(network_class))\n",
    "\n",
    "\t\t# create new object\n",
    "\t\tnew_net = network_class(object_specs['feature_list'], init_network=False)\n",
    "\n",
    "\t\tnew_net.model = model_from_json(object_specs['keras_model'], custom_objects={'Bias': Bias})\n",
    "\t\tif 'weights_file' in object_specs:\n",
    "\t\t\tnew_net.model.load_weights(object_specs['weights_file'])\n",
    "\t\tnew_net.forward = new_net._model_forward()\n",
    "\t\treturn new_net\n",
    "\n",
    "\tdef save_model(self, json_file, weights_file=None):\n",
    "\t\t\"\"\"write the network model and preprocessing features to the specified file\n",
    "\n",
    "\t\tIf a weights_file (.hdf5 extension) is also specified, model weights are also\n",
    "\t\tsaved to that file and will be reloaded automatically in a call to load_model\n",
    "\t\t\"\"\"\n",
    "\t\t# this looks odd because we are serializing a model with json as a string\n",
    "\t\t# then making that the value of an object which is then serialized as\n",
    "\t\t# json again.\n",
    "\t\t# It's not as crazy as it looks. A Network has 2 moving parts - the\n",
    "\t\t# feature preprocessing and the neural net, each of which gets a top-level\n",
    "\t\t# entry in the saved file. Keras just happens to serialize models with JSON\n",
    "\t\t# as well. Note how this format makes load_model fairly clean as well.\n",
    "\t\tobject_specs = {\n",
    "\t\t\t'class': self.__class__.__name__,\n",
    "\t\t\t'keras_model': self.model.to_json(),\n",
    "\t\t\t'feature_list': self.preprocessor.feature_list\n",
    "\t\t}\n",
    "\t\tif weights_file is not None:\n",
    "\t\t\tself.model.save_weights(weights_file)\n",
    "\t\t\tobject_specs['weights_file'] = weights_file\n",
    "\t\t# use the json module to write object_specs to file\n",
    "\t\twith open(json_file, 'w') as f:\n",
    "\t\t\tjson.dump(object_specs, f)\n",
    "\n",
    "\n",
    "def neuralnet(cls):\n",
    "\t\"\"\"Class decorator for registering subclasses of NeuralNetBase\n",
    "\t\"\"\"\n",
    "\tNeuralNetBase.subclasses[cls.__name__] = cls\n",
    "\treturn cls\n",
    "\n",
    "\n",
    "class Bias(Layer):\n",
    "\t\"\"\"Custom keras layer that simply adds a scalar bias to each location in the input\n",
    "\n",
    "\tLargely copied from the keras docs:\n",
    "\thttp://keras.io/layers/writing-your-own-keras-layers/#writing-your-own-keras-layers\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Bias, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tself.W = K.zeros(input_shape[1:])\n",
    "\t\tself.trainable_weights = [self.W]\n",
    "\n",
    "\tdef call(self, x, mask=None):\n",
    "\t\treturn x + self.W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named AlphaGo.preprocessing.preprocessing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8afd3ced4077>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mAlphaGo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named AlphaGo.preprocessing.preprocessing"
     ]
    }
   ],
   "source": [
    "#mcts\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.engine.topology import Layer\n",
    "from AlphaGo.preprocessing.preprocessing import Preprocess\n",
    "import json\n",
    "\n",
    "\n",
    "class NeuralNetBase(object):\n",
    "\t\"\"\"Base class for neural network classes handling feature processing, construction\n",
    "\tof a 'forward' function, etc.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# keep track of subclasses to make generic saving/loading cleaner.\n",
    "\t# subclasses can be 'registered' with the @neuralnet decorator\n",
    "\tsubclasses = {}\n",
    "\n",
    "\tdef __init__(self, feature_list, **kwargs):\n",
    "\t\t\"\"\"create a neural net object that preprocesses according to feature_list and uses\n",
    "\t\ta neural network specified by keyword arguments (using subclass' create_network())\n",
    "\n",
    "\t\toptional argument: init_network (boolean). If set to False, skips initializing\n",
    "\t\tself.model and self.forward and the calling function should set them.\n",
    "\t\t\"\"\"\n",
    "\t\tself.preprocessor = Preprocess(feature_list)\n",
    "\t\tkwargs[\"input_dim\"] = self.preprocessor.output_dim\n",
    "\n",
    "\t\tif kwargs.get('init_network', True):\n",
    "\t\t\t# self.__class__ refers to the subclass so that subclasses only\n",
    "\t\t\t# need to override create_network()\n",
    "\t\t\tself.model = self.__class__.create_network(**kwargs)\n",
    "\t\t\t# self.forward is a lambda function wrapping a Keras function\n",
    "\t\t\tself.forward = self._model_forward()\n",
    "\n",
    "\tdef _model_forward(self):\n",
    "\t\t\"\"\"Construct a function using the current keras backend that, when given a batch\n",
    "\t\tof inputs, simply processes them forward and returns the output\n",
    "\n",
    "\t\tThis is as opposed to model.compile(), which takes a loss function\n",
    "\t\tand training method.\n",
    "\n",
    "\t\tc.f. https://github.com/fchollet/keras/issues/1426\n",
    "\t\t\"\"\"\n",
    "\t\t# The uses_learning_phase property is True if the model contains layers that behave\n",
    "\t\t# differently during training and testing, e.g. Dropout or BatchNormalization.\n",
    "\t\t# In these cases, K.learning_phase() is a reference to a backend variable that should\n",
    "\t\t# be set to 0 when using the network in prediction mode and is automatically set to 1\n",
    "\t\t# during training.\n",
    "\t\tif self.model.uses_learning_phase:\n",
    "\t\t\tforward_function = K.function([self.model.input, K.learning_phase()], [self.model.output])\n",
    "\n",
    "\t\t\t# the forward_function returns a list of tensors\n",
    "\t\t\t# the first [0] gets the front tensor.\n",
    "\t\t\treturn lambda inpt: forward_function([inpt, 0])[0]\n",
    "\t\telse:\n",
    "\t\t\t# identical but without a second input argument for the learning phase\n",
    "\t\t\tforward_function = K.function([self.model.input], [self.model.output])\n",
    "\t\t\treturn lambda inpt: forward_function([inpt])[0]\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef load_model(json_file):\n",
    "\t\t\"\"\"create a new neural net object from the architecture specified in json_file\n",
    "\t\t\"\"\"\n",
    "\t\twith open(json_file, 'r') as f:\n",
    "\t\t\tobject_specs = json.load(f)\n",
    "\n",
    "\t\t# Create object; may be a subclass of networks saved in specs['class']\n",
    "\t\tclass_name = object_specs.get('class', 'CNNPolicy')\n",
    "\t\ttry:\n",
    "\t\t\tnetwork_class = NeuralNetBase.subclasses[class_name]\n",
    "\t\texcept KeyError:\n",
    "\t\t\traise ValueError(\"Unknown neural network type in json file: {}\\n(was it registered with the @neuralnet decorator?)\".format(network_class))\n",
    "\n",
    "\t\t# create new object\n",
    "\t\tnew_net = network_class(object_specs['feature_list'], init_network=False)\n",
    "\n",
    "\t\tnew_net.model = model_from_json(object_specs['keras_model'], custom_objects={'Bias': Bias})\n",
    "\t\tif 'weights_file' in object_specs:\n",
    "\t\t\tnew_net.model.load_weights(object_specs['weights_file'])\n",
    "\t\tnew_net.forward = new_net._model_forward()\n",
    "\t\treturn new_net\n",
    "\n",
    "\tdef save_model(self, json_file, weights_file=None):\n",
    "\t\t\"\"\"write the network model and preprocessing features to the specified file\n",
    "\n",
    "\t\tIf a weights_file (.hdf5 extension) is also specified, model weights are also\n",
    "\t\tsaved to that file and will be reloaded automatically in a call to load_model\n",
    "\t\t\"\"\"\n",
    "\t\t# this looks odd because we are serializing a model with json as a string\n",
    "\t\t# then making that the value of an object which is then serialized as\n",
    "\t\t# json again.\n",
    "\t\t# It's not as crazy as it looks. A Network has 2 moving parts - the\n",
    "\t\t# feature preprocessing and the neural net, each of which gets a top-level\n",
    "\t\t# entry in the saved file. Keras just happens to serialize models with JSON\n",
    "\t\t# as well. Note how this format makes load_model fairly clean as well.\n",
    "\t\tobject_specs = {\n",
    "\t\t\t'class': self.__class__.__name__,\n",
    "\t\t\t'keras_model': self.model.to_json(),\n",
    "\t\t\t'feature_list': self.preprocessor.feature_list\n",
    "\t\t}\n",
    "\t\tif weights_file is not None:\n",
    "\t\t\tself.model.save_weights(weights_file)\n",
    "\t\t\tobject_specs['weights_file'] = weights_file\n",
    "\t\t# use the json module to write object_specs to file\n",
    "\t\twith open(json_file, 'w') as f:\n",
    "\t\t\tjson.dump(object_specs, f)\n",
    "\n",
    "\n",
    "def neuralnet(cls):\n",
    "\t\"\"\"Class decorator for registering subclasses of NeuralNetBase\n",
    "\t\"\"\"\n",
    "\tNeuralNetBase.subclasses[cls.__name__] = cls\n",
    "\treturn cls\n",
    "\n",
    "\n",
    "class Bias(Layer):\n",
    "\t\"\"\"Custom keras layer that simply adds a scalar bias to each location in the input\n",
    "\n",
    "\tLargely copied from the keras docs:\n",
    "\thttp://keras.io/layers/writing-your-own-keras-layers/#writing-your-own-keras-layers\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(Bias, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tself.W = K.zeros(input_shape[1:])\n",
    "\t\tself.trainable_weights = [self.W]\n",
    "\n",
    "\tdef call(self, x, mask=None):\n",
    "\t\treturn x + self.W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-074651e65ed2>, line 130)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-074651e65ed2>\"\u001b[1;36m, line \u001b[1;32m130\u001b[0m\n\u001b[1;33m    print \"creating output directory {}\".format(args.out_directory)\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from keras.optimizers import SGD\n",
    "from AlphaGo.ai import ProbabilisticPolicyPlayer\n",
    "import AlphaGo.go as go\n",
    "from AlphaGo.go import GameState\n",
    "from AlphaGo.models.policy import CNNPolicy\n",
    "from AlphaGo.preprocessing.preprocessing import Preprocess\n",
    "from AlphaGo.util import flatten_idx\n",
    "\n",
    "\n",
    "def make_training_pairs(player, opp, features, mini_batch_size, board_size=19):\n",
    "\t\"\"\"Make training pairs for batch of matches, utilizing player.get_moves (parallel form of\n",
    "\tplayer.get_move), which calls `CNNPolicy.batch_eval_state`.\n",
    "\n",
    "\tArgs:\n",
    "\tplayer -- player that we're always updating\n",
    "\topp -- batch opponent\n",
    "\tfeature_list -- game features to be one-hot encoded\n",
    "\tmini_batch_size -- number of games in mini-batch\n",
    "\n",
    "\tReturn:\n",
    "\tX_list -- list of 1-hot board states associated with moves.\n",
    "\ty_list -- list of 1-hot moves associated with board states.\n",
    "\twinners -- list of winners associated with each game in batch\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef do_move(states, states_prev, moves, X_list, y_list, player_color):\n",
    "\t\tbsize_flat = bsize * bsize\n",
    "\t\tfor st, st_prev, mv, X, y in zip(states, states_prev, moves, X_list, y_list):\n",
    "\t\t\tif not st.is_end_of_game:\n",
    "\t\t\t\t# Only do more moves if not end of game already\n",
    "\t\t\t\tst.do_move(mv)\n",
    "\t\t\t\tif st.current_player != player_color and mv is not go.PASS_MOVE:\n",
    "\t\t\t\t\t# Convert move to one-hot\n",
    "\t\t\t\t\tstate_1hot = preprocessor.state_to_tensor(st_prev)\n",
    "\t\t\t\t\tmove_1hot = np.zeros(bsize_flat)\n",
    "\t\t\t\t\tmove_1hot[flatten_idx(mv, bsize)] = 1\n",
    "\t\t\t\t\tX.append(state_1hot)\n",
    "\t\t\t\t\ty.append(move_1hot)\n",
    "\t\treturn states, X_list, y_list\n",
    "\n",
    "\t# Lists of game training pairs (1-hot)\n",
    "\tX_list = [list() for _ in xrange(mini_batch_size)]\n",
    "\ty_list = [list() for _ in xrange(mini_batch_size)]\n",
    "\tpreprocessor = Preprocess(features)\n",
    "\tbsize = player.policy.model.input_shape[-1]\n",
    "\tstates = [GameState(size=board_size) for i in xrange(mini_batch_size)]\n",
    "\t# Randomly choose who goes first (i.e. color of 'player')\n",
    "\tplayer_color = np.random.choice([go.BLACK, go.WHITE])\n",
    "\tplayer1, player2 = (player, opp) if player_color == go.BLACK else \\\n",
    "\t\t(opp, player)\n",
    "\twhile True:\n",
    "\t\t# Cache states before moves\n",
    "\t\tstates_prev = [st.copy() for st in states]\n",
    "\t\t# Get moves (batch)\n",
    "\t\tmoves_black = player1.get_moves(states)\n",
    "\t\t# Do moves (black)\n",
    "\t\tstates, X_list, y_list = do_move(states, states_prev, moves_black, X_list, y_list, player_color)\n",
    "\t\t# Do moves (white)\n",
    "\t\tmoves_white = player2.get_moves(states)\n",
    "\t\tstates, X_list, y_list = do_move(states, states_prev, moves_white, X_list, y_list, player_color)\n",
    "\t\t# If all games have ended, we're done. Get winners.\n",
    "\t\tdone = [st.is_end_of_game for st in states]\n",
    "\t\tif all(done):\n",
    "\t\t\tbreak\n",
    "\twinners = [st.get_winner() for st in states]\n",
    "\t# Concatenate tensors across turns within each game\n",
    "\tfor i in xrange(mini_batch_size):\n",
    "\t\tX_list[i] = np.concatenate(X_list[i], axis=0)\n",
    "\t\ty_list[i] = np.vstack(y_list[i])\n",
    "\treturn X_list, y_list, winners\n",
    "\n",
    "\n",
    "def train_batch(player, X_list, y_list, winners, lr):\n",
    "\t\"\"\"Given the outcomes of a mini-batch of play against a fixed opponent,\n",
    "\t\tupdate the weights with reinforcement learning.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\tplayer -- player object with policy weights to be updated\n",
    "\t\tX_list -- List of one-hot encoded states.\n",
    "\t\ty_list -- List of one-hot encoded actions (to pair with X_list).\n",
    "\t\twinners -- List of winners corresponding to each item in\n",
    "\t\t\t\t\ttraining_pairs_list\n",
    "\t\tlr -- Keras learning rate\n",
    "\n",
    "\t\tReturn:\n",
    "\t\tplayer -- same player, with updated weights.\n",
    "\t\"\"\"\n",
    "\n",
    "\tfor X, y, winner in zip(X_list, y_list, winners):\n",
    "\t\t# Update weights in + direction if player won, and - direction if player lost.\n",
    "\t\t# Setting learning rate negative is hack for negative weights update.\n",
    "\t\tif winner == -1:\n",
    "\t\t\tplayer.policy.model.optimizer.lr.set_value(-lr)\n",
    "\t\telse:\n",
    "\t\t\tplayer.policy.model.optimizer.lr.set_value(lr)\n",
    "\t\tplayer.policy.model.fit(X, y, nb_epoch=1, batch_size=len(X))\n",
    "\n",
    "\n",
    "def run_training(cmd_line_args=None):\n",
    "\tparser = argparse.ArgumentParser(description='Perform reinforcement learning to improve given policy network. Second phase of pipeline.')\n",
    "\tparser.add_argument(\"model_json\", help=\"Path to policy model JSON.\")\n",
    "\tparser.add_argument(\"initial_weights\", help=\"Path to HDF5 file with inital weights (i.e. result of supervised training).\")\n",
    "\tparser.add_argument(\"out_directory\", help=\"Path to folder where the model params and metadata will be saved after each epoch.\")\n",
    "\tparser.add_argument(\"--learning-rate\", help=\"Keras learning rate (Default: .03)\", type=float, default=.03)\n",
    "\tparser.add_argument(\"--policy-temp\", help=\"Distribution temperature of players using policies (Default: 0.67)\", type=float, default=0.67)\n",
    "\tparser.add_argument(\"--save-every\", help=\"Save policy as a new opponent every n batches (Default: 500)\", type=int, default=500)\n",
    "\tparser.add_argument(\"--game-batch\", help=\"Number of games per mini-batch (Default: 20)\", type=int, default=20)\n",
    "\tparser.add_argument(\"--iterations\", help=\"Number of training batches/iterations (Default: 10000)\", type=int, default=10000)\n",
    "\tparser.add_argument(\"--resume\", help=\"Load latest weights in out_directory and resume\", default=False, action=\"store_true\")\n",
    "\tparser.add_argument(\"--verbose\", \"-v\", help=\"Turn on verbose mode\", default=False, action=\"store_true\")\n",
    "\t# Baseline function (TODO) default lambda state: 0  (receives either file\n",
    "\t# paths to JSON and weights or None, in which case it uses default baseline 0)\n",
    "\tif cmd_line_args is None:\n",
    "\t\targs = parser.parse_args()\n",
    "\telse:\n",
    "\t\targs = parser.parse_args(cmd_line_args)\n",
    "\n",
    "\tZEROTH_FILE = \"weights.00000.hdf5\"\n",
    "\n",
    "\tif args.resume:\n",
    "\t\tif not os.path.exists(os.path.join(args.out_directory, \"metadata.json\")):\n",
    "\t\t\traise ValueError(\"Cannot resume without existing output directory\")\n",
    "\n",
    "\tif not os.path.exists(args.out_directory):\n",
    "\t\tif args.verbose:\n",
    "\t\t\tprint \"creating output directory {}\".format(args.out_directory)\n",
    "\t\tos.makedirs(args.out_directory)\n",
    "\n",
    "\tif not args.resume:\n",
    "\t\t# make a copy of weights file, \"weights.00000.hdf5\" in the output directory\n",
    "\t\tcopyfile(args.initial_weights, os.path.join(args.out_directory, ZEROTH_FILE))\n",
    "\t\tif args.verbose:\n",
    "\t\t\tprint \"copied {} to {}\".format(args.initial_weights, os.path.join(args.out_directory, ZEROTH_FILE))\n",
    "\t\tplayer_weights = ZEROTH_FILE\n",
    "\telse:\n",
    "\t\t# if resuming, we expect initial_weights to be just a \"weights.#####.hdf5\" file, not a full path\n",
    "\t\targs.initial_weights = os.path.join(args.out_directory, os.path.basename(args.initial_weights))\n",
    "\t\tif not os.path.exists(args.initial_weights):\n",
    "\t\t\traise ValueError(\"Cannot resume; weights {} do not exist\".format(args.initial_weights))\n",
    "\t\telif args.verbose:\n",
    "\t\t\tprint \"Resuming with weights {}\".format(args.initial_weights)\n",
    "\t\tplayer_weights = os.path.basename(args.initial_weights)\n",
    "\n",
    "\t# Set initial conditions\n",
    "\tpolicy = CNNPolicy.load_model(args.model_json)\n",
    "\tpolicy.model.load_weights(args.initial_weights)\n",
    "\tplayer = ProbabilisticPolicyPlayer(policy, temperature=args.policy_temp)\n",
    "\tfeatures = policy.preprocessor.feature_list\n",
    "\n",
    "\t# different opponents come from simply changing the weights of\n",
    "\t# opponent.policy.model \"behind the scenes\"\n",
    "\topp_policy = CNNPolicy.load_model(args.model_json)\n",
    "\topponent = ProbabilisticPolicyPlayer(opp_policy, temperature=args.policy_temp)\n",
    "\n",
    "\tif args.verbose:\n",
    "\t\tprint \"created player and opponent with temperature {}\".format(args.policy_temp)\n",
    "\n",
    "\tif not args.resume:\n",
    "\t\tmetadata = {\n",
    "\t\t\t\"model_file\": args.model_json,\n",
    "\t\t\t\"init_weights\": args.initial_weights,\n",
    "\t\t\t\"learning_rate\": args.learning_rate,\n",
    "\t\t\t\"temperature\": args.policy_temp,\n",
    "\t\t\t\"game_batch\": args.game_batch,\n",
    "\t\t\t\"opponents\": [ZEROTH_FILE],  # which weights from which to sample an opponent each batch\n",
    "\t\t\t\"win_ratio\": {}  # map from player to tuple of (opponent, win ratio) Useful for validating in lieu of 'accuracy/loss'\n",
    "\t\t}\n",
    "\telse:\n",
    "\t\twith open(os.path.join(args.out_directory, \"metadata.json\"), \"r\") as f:\n",
    "\t\t\tmetadata = json.load(f)\n",
    "\n",
    "\tdef save_metadata():\n",
    "\t\twith open(os.path.join(args.out_directory, \"metadata.json\"), \"w\") as f:\n",
    "\t\t\tjson.dump(metadata, f)\n",
    "\n",
    "\t# Set SGD and compile\n",
    "\tsgd = SGD(lr=args.learning_rate)\n",
    "\tplayer.policy.model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\tboard_size = player.policy.model.input_shape[-1]\n",
    "\tfor i_iter in xrange(1, args.iterations + 1):\n",
    "\t\t# Train mini-batches by randomly choosing opponent from pool (possibly self)\n",
    "\t\t# and playing game_batch games against them\n",
    "\t\topp_weights = np.random.choice(metadata[\"opponents\"])\n",
    "\t\topp_path = os.path.join(args.out_directory, opp_weights)\n",
    "\t\t# load new weights into opponent, but otherwise its the same\n",
    "\t\topponent.policy.model.load_weights(opp_path)\n",
    "\t\tif args.verbose:\n",
    "\t\t\tprint \"Batch {}\\tsampled opponent is {}\".format(i_iter, opp_weights)\n",
    "\t\t# Make training pairs and do RL\n",
    "\t\tX_list, y_list, winners = make_training_pairs(player, opponent, features, args.game_batch, board_size)\n",
    "\t\twin_ratio = np.sum(np.array(winners) == 1) / float(args.game_batch)\n",
    "\t\tmetadata[\"win_ratio\"][player_weights] = (opp_weights, win_ratio)\n",
    "\t\ttrain_batch(player, X_list, y_list, winners, args.learning_rate)\n",
    "\t\t# Save intermediate models\n",
    "\t\tplayer_weights = \"weights.%05d.hdf5\" % i_iter\n",
    "\t\tplayer.policy.model.save_weights(os.path.join(args.out_directory, player_weights))\n",
    "\t\t# add player to batch of oppenents once in a while\n",
    "\t\tif i_iter % args.save_every == 0:\n",
    "\t\t\tmetadata[\"opponents\"].append(player_weights)\n",
    "\t\tsave_metadata()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\trun_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
