{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Self-Taught Learning and Unsupervised Feature Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done\n",
      "<type 'dict'>\n",
      "(12180, 13)\n",
      "[[ 343.33881535  292.94608369  317.56346931 ...,    0.            0.            2.        ]\n",
      " [ 229.55690517  183.19172676    0.         ...,    0.            0.            2.        ]\n",
      " [ 106.98249046   79.89465389   60.93378272 ...,   50.91543646\n",
      "    44.87156581    1.        ]\n",
      " ..., \n",
      " [ 232.28249497  135.43054     200.91859872 ...,  187.78214091\n",
      "   191.41443704    0.        ]\n",
      " [ 239.19601078  206.37272444  174.6964434  ...,  297.67230497\n",
      "   202.62736538    0.        ]\n",
      " [ 277.38010202  209.01016123  277.00169283 ...,  146.70298371\n",
      "   309.63963308    0.        ]]\n",
      "Example:\n",
      "[ 229.55690517  183.19172676    0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.            0.\n",
      "    2.        ]\n",
      "(12180, 12) (12180,)\n",
      "(12180,)\n",
      "np.unique(Y1) [ 0.  1.]\n",
      "(8526, 12) (8526,) (3654, 12) (3654,)\n",
      "(8526, 12, 1) (3654, 12, 1)\n",
      "(8526, 'train samples')\n",
      "(3654, 'test samples')\n",
      "Sets made\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.optimize as opt\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "from theano import shared\n",
    "import theano\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv\n",
    "\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Layer, Reshape, Merge #,AutoEncoder\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D,Convolution1D,MaxPooling1D,UpSampling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adadelta\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "#import keras.utils.visualize_util as vutil #Pydot issues\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l1, l2, l1l2, activity_l2, activity_l1\n",
    "#import keras.layers.containers as containers  #No module named containers\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.regularizers import ActivityRegularizer\n",
    "from keras import backend as K\n",
    "#import keras.utils.visualize_util as vutil\n",
    "from keras.models import model_from_json\n",
    "#from keras.utils.visualize_util import plot, to_graph\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "np.set_printoptions(suppress=True)  #Supress exponent of the number 2e-2 == 0.02\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"All imports done\")\n",
    "\n",
    "\n",
    "#Ref: http://deeplearning.net/, https://www.kaggle.com/wiki/Tutorials etc..\n",
    "\n",
    "#caso_I.mat, caso_II.mat, caso_III.mat, caso_base.mat, consumer_data.xlsx\n",
    "\n",
    "#print(os.getcwd() + \"\\n\")\n",
    "#print os.listdir(os.getcwd())\n",
    "dataSmartGrid = scpy.loadmat('../demcliMat.mat')\n",
    "print type(dataSmartGrid)\n",
    "\n",
    "data = dataSmartGrid['demcliMat']\n",
    "print data.shape\n",
    "print data\n",
    "print(\"Example:\")\n",
    "print data[1]\n",
    "\n",
    "np.unique(data[:,-1:])\n",
    "\n",
    "X = data[:,0:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print X.shape, Y.shape\n",
    "\n",
    "#print X.shape\n",
    "Y1 = Y\n",
    "Y1[Y1==2] = 1\n",
    "print Y1.shape\n",
    "\n",
    "print \"np.unique(Y1)\",np.unique(Y1)\n",
    "\n",
    "#print data[0:5,:]\n",
    "#print data[0:5,0:-1]\n",
    "#print data[0:5,-1]\n",
    "\n",
    "\n",
    "#print X\n",
    "#print Y\n",
    "#Splitting data into train and testing. 70% Training and 30% Testing..\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y1, test_size=0.3, random_state=4)\n",
    "\n",
    "print X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "#print numpy.unique(Y_test)\n",
    "\n",
    "X_train_1 = X_train.reshape(8526,12,1)\n",
    "X_test_1 = X_test.reshape(3654,12,1)\n",
    "\n",
    "print X_train_1.shape, X_test_1.shape \n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 2)\n",
    "Y_test = np_utils.to_categorical(y_test, 2)\n",
    "\n",
    "Y_train = Y_train.astype('int')\n",
    "Y_test = Y_test.astype('int')\n",
    "\n",
    "print(\"Sets made\")\n",
    "\n",
    "rms = RMSprop()\n",
    "\n",
    "import load_MNIST\n",
    "import numpy as np\n",
    "import sparse_autoencoder\n",
    "import scipy.optimize\n",
    "import display_network\n",
    "import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7926"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_train==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8526, 12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# examples in unlabeled set: 4262\n",
      "\n",
      "# examples in supervised training set: 4262\n",
      "\n",
      "# examples in supervised testing set: 3654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = 12\n",
    "num_labels = 2\n",
    "hidden_size = 196\n",
    "\n",
    "tot_iter = 400\n",
    "\n",
    "sparsity_param = 0.1  # desired average activation of the hidden units.\n",
    "lambda_ = 3e-3  # weight decay parameter\n",
    "beta = 3  # weight of sparsity penalty term\n",
    "\n",
    "#Unlabelled Data\n",
    "\n",
    "X_train_new = np.transpose(X_train)\n",
    "X_test_new  = np.transpose(X_test)\n",
    "\n",
    "unlabeled_data = X_train_new[ : , 4263:8525]\n",
    "\n",
    "\n",
    "#Labelled Data\n",
    "train_data = X_train_new[ : , 0:4262]\n",
    "train_labels = y_train[0:4262]\n",
    "\n",
    "test_data = X_test_new\n",
    "test_labels = y_test\n",
    "\n",
    "print '# examples in unlabeled set: {0:d}\\n'.format(unlabeled_data.shape[1])\n",
    "print '# examples in supervised training set: {0:d}\\n'.format(train_data.shape[1])\n",
    "print '# examples in supervised testing set: {0:d}\\n'.format(test_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4262) (12, 4262) (12, 3654)\n",
      "(4262,) (3654,)\n"
     ]
    }
   ],
   "source": [
    "print unlabeled_data.shape, train_data.shape, test_data.shape\n",
    "print train_labels.shape, test_labels.shape\n",
    "#print X_train_new.shape\n",
    "#print X_train.shape\n",
    "#print unlabeled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "print input_size\n",
    "print hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 366279.89677175845\n",
      " hess_inv: <4912x4912 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.00938838, -0.00843164, -0.01003908, ..., -0.03288747,\n",
      "       -0.03360219, -0.00618806])\n",
      "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 472\n",
      "      nit: 385\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-0.52776593, -0.44027155, -0.41921669, ...,  3.81998351,\n",
      "        3.95004707,  4.51677459])\n"
     ]
    }
   ],
   "source": [
    "## ======================================================================\n",
    "#  STEP 2: Train the sparse autoencoder\n",
    "#  This trains the sparse autoencoder on the unlabeled training\n",
    "#  images.\n",
    "\n",
    "#  Randomly initialize the parameters\n",
    "theta = sparse_autoencoder.initialize(hidden_size, input_size)\n",
    "\n",
    "J = lambda x: sparse_autoencoder.sparse_autoencoder_cost(x, input_size, hidden_size,\n",
    "                                                         lambda_, sparsity_param,\n",
    "                                                         beta, unlabeled_data)\n",
    "\n",
    "options_ = {'maxiter': tot_iter, 'disp': True} #Was 400\n",
    "result = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options_)\n",
    "opt_theta = result.x\n",
    "\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the weights\n",
    "W1 = opt_theta[0:hidden_size * input_size].reshape(hidden_size, input_size).transpose()\n",
    "#display_network.display_network(W1,filename='STL_Weights.png')\n",
    "\n",
    "##======================================================================\n",
    "## STEP 3: Extract Features from the Supervised Dataset\n",
    "#\n",
    "#  Train on labeled set using unlabeled weights.\n",
    "#  \n",
    "\n",
    "train_features = sparse_autoencoder.sparse_autoencoder(opt_theta, hidden_size,\n",
    "                                                       input_size, train_data)\n",
    "\n",
    "test_features = sparse_autoencoder.sparse_autoencoder(opt_theta, hidden_size,\n",
    "                                                      input_size, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 451.55786961797395\n",
      " hess_inv: <392x392 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 0.00151353,  0.0012326 , -0.00012981, -0.00143158,  0.00058462,\n",
      "        0.00040253,  0.00284361,  0.00285728, -0.00000474,  0.00082271,\n",
      "        0.00115339, -0.00266407, -0.00032278,  0.00026697,  0.00090129,\n",
      "       -0.00055717,  0.00265099, -0.00000336, -0.00211373,  0.00013404,\n",
      "        0.00039839, -0.00000469, -0.00195429,  0.00306103, -0.00009902,\n",
      "       -0.00234171,  0.00045572,  0.00277489, -0.00000452, -0.0000053 ,\n",
      "       -0.00000438, -0.00000479, -0.00036804,  0.00032453, -0.00160581,\n",
      "       -0.00008686, -0.00076715, -0.00702163, -0.00279483,  0.00378551,\n",
      "        0.00296232, -0.00000533, -0.00000505,  0.00271155, -0.00000477,\n",
      "        0.0014817 ,  0.00003584, -0.00001704, -0.00000401,  0.00232008,\n",
      "       -0.00000638,  0.00023511, -0.00090129, -0.00036955, -0.00000447,\n",
      "       -0.00009441,  0.00422986, -0.00152975, -0.00000544, -0.00000581,\n",
      "       -0.0000047 ,  0.00096583,  0.00206537, -0.00344308, -0.00000543,\n",
      "       -0.00061266,  0.00036853, -0.00106557, -0.00168471, -0.00116877,\n",
      "       -0.00228424,  0.00110099, -0.0004823 , -0.00102927,  0.00223996,\n",
      "        0.00162536,  0.00042073,  0.00146887, -0.00100764, -0.0012004 ,\n",
      "        0.00122392, -0.00231322,  0.00211078, -0.00034071, -0.00000486,\n",
      "       -0.00000529, -0.00000392, -0.00000478,  0.00227986,  0.00213246,\n",
      "       -0.00000445,  0.00149564,  0.00121194, -0.00000459,  0.00090114,\n",
      "        0.0015154 ,  0.00032135,  0.00089882,  0.00398706, -0.00195906,\n",
      "       -0.00114426,  0.00057122, -0.00107525, -0.00000778,  0.00632415,\n",
      "       -0.0000039 ,  0.00483246,  0.00048045,  0.00182354,  0.00030568,\n",
      "       -0.00035455, -0.00000605, -0.00361749, -0.00125245,  0.00136978,\n",
      "       -0.00000501, -0.00000485,  0.0018568 , -0.00200711, -0.00000436,\n",
      "        0.00044347, -0.00000486, -0.00049111, -0.00166195,  0.00228688,\n",
      "        0.0007236 , -0.00009832,  0.0034215 ,  0.00317697, -0.00025361,\n",
      "        0.00326653, -0.0000041 , -0.00000499, -0.0000046 , -0.00025112,\n",
      "        0.00079141, -0.00515319,  0.00393901, -0.00004857,  0.00369665,\n",
      "        0.00156596, -0.00000492, -0.00013381,  0.00219499,  0.00158249,\n",
      "        0.00178818,  0.0055679 ,  0.00028802,  0.00084863,  0.00161605,\n",
      "        0.00299622, -0.00000614, -0.0000057 ,  0.00113761,  0.00101523,\n",
      "       -0.00058236, -0.00000498, -0.00000463,  0.00075635,  0.00398857,\n",
      "       -0.00069735, -0.00044465,  0.0010241 ,  0.00182646, -0.00000381,\n",
      "        0.00000079,  0.00054169,  0.00007789,  0.00076276,  0.00142219,\n",
      "       -0.00000528, -0.00010056,  0.00265906,  0.00108037, -0.00087394,\n",
      "       -0.00123335, -0.00000411, -0.00192361, -0.00000516, -0.00027935,\n",
      "        0.00320858,  0.00062063, -0.00217849, -0.00110647, -0.00278869,\n",
      "       -0.00626753, -0.00165873,  0.00135294,  0.00027265,  0.00079311,\n",
      "        0.00005683,  0.00204973,  0.00070819,  0.00132458,  0.00300478,\n",
      "       -0.00051229, -0.00151308, -0.00123206,  0.00013099,  0.00143001,\n",
      "       -0.00058515, -0.00040249, -0.00284463, -0.00285833,  0.00000482,\n",
      "       -0.00082339, -0.00115291,  0.00266484,  0.00032324, -0.00026732,\n",
      "       -0.0009014 ,  0.00055695, -0.00265103,  0.00000303,  0.00211306,\n",
      "       -0.00013512, -0.00039895,  0.00000469,  0.00195449, -0.00306047,\n",
      "        0.00009951,  0.00234162, -0.00045608, -0.00277489,  0.00000438,\n",
      "        0.00000573,  0.00000473,  0.00000483,  0.0003675 , -0.00032457,\n",
      "        0.00160528,  0.00008717,  0.00076767,  0.00702179,  0.00279676,\n",
      "       -0.00378515, -0.00296299,  0.00000488,  0.00000465, -0.00271298,\n",
      "        0.00000542, -0.00148166, -0.00003505,  0.00001701,  0.00000552,\n",
      "       -0.0023203 ,  0.0000062 , -0.00023482,  0.00090071,  0.00037023,\n",
      "        0.00000421,  0.00009364, -0.00422996,  0.0015309 ,  0.0000059 ,\n",
      "        0.00000456,  0.00000556, -0.00096604, -0.00206634,  0.00344259,\n",
      "        0.00000498,  0.00061259, -0.00036964,  0.00106463,  0.00168363,\n",
      "        0.00116852,  0.00228433, -0.00110095,  0.00048206,  0.00103048,\n",
      "       -0.00223959, -0.00162637, -0.00042   , -0.001469  ,  0.00100672,\n",
      "        0.00119971, -0.00122425,  0.00231185, -0.00211005,  0.00034221,\n",
      "        0.00000493,  0.00000446,  0.0000048 ,  0.00000507, -0.00228026,\n",
      "       -0.00213228,  0.00000507, -0.00149579, -0.00121168,  0.00000463,\n",
      "       -0.0009017 , -0.00151548, -0.00032143, -0.00089862, -0.00398692,\n",
      "        0.00195946,  0.00114407, -0.00057129,  0.00107493,  0.00000879,\n",
      "       -0.00632509,  0.00000462, -0.00483228, -0.00048117, -0.00182221,\n",
      "       -0.00030537,  0.00035564,  0.00000693,  0.00361667,  0.00125238,\n",
      "       -0.00136814,  0.00000512,  0.000005  , -0.00185714,  0.00200737,\n",
      "        0.00000483, -0.00044316,  0.00000498,  0.00049095,  0.00166246,\n",
      "       -0.00228709, -0.00072335,  0.00009759, -0.0034223 , -0.00317579,\n",
      "        0.00025285, -0.0032671 ,  0.00000526,  0.00000553,  0.00000489,\n",
      "        0.00025041, -0.00079118,  0.0051538 , -0.00393887,  0.00004921,\n",
      "       -0.00369708, -0.00156624,  0.0000053 ,  0.00013312, -0.00219523,\n",
      "       -0.00158268, -0.00178773, -0.00556865, -0.00028756, -0.00084847,\n",
      "       -0.00161612, -0.00299599,  0.00000481,  0.00000504, -0.00113798,\n",
      "       -0.00101539,  0.00058182,  0.00000506,  0.00000504, -0.00075687,\n",
      "       -0.00398961,  0.00069679,  0.00044496, -0.00102344, -0.00182611,\n",
      "        0.00000553, -0.000001  , -0.00054178, -0.00007833, -0.00076306,\n",
      "       -0.00142179,  0.00000423,  0.00009993, -0.00266001, -0.00108089,\n",
      "        0.00087278,  0.00123339,  0.00000499,  0.00192463,  0.00000485,\n",
      "        0.00027908, -0.00320946, -0.00062119,  0.00217765,  0.00110733,\n",
      "        0.00278876,  0.00626621,  0.00165846, -0.00135381, -0.00027151,\n",
      "       -0.00079266, -0.00005745, -0.00204899, -0.00070795, -0.00132349,\n",
      "       -0.00300445,  0.00051202])\n",
      "  message: 'STOP: TOTAL NO. of ITERATIONS EXCEEDS LIMIT'\n",
      "     nfev: 435\n",
      "      nit: 401\n",
      "   status: 1\n",
      "  success: False\n",
      "        x: array([ -0.4058566 ,  -0.28796548,   0.29116731,  -0.177064  ,\n",
      "        -0.2995414 ,   0.75307974,  -0.17377666,  -4.081296  ,\n",
      "        -0.04744448,   0.0594666 ,  -0.10625289,   0.07494823,\n",
      "        -0.07475246,   0.0732777 ,   0.48501398,   0.23665025,\n",
      "        -0.28257987,  -0.05304848,  -0.34042247,  -0.30019584,\n",
      "        -0.19484498,  -0.04689391,   0.26476835,  -0.41798179,\n",
      "        -0.99468515,  -0.30004915,  -0.07021459,   0.34830901,\n",
      "        -0.04516727,  -0.05302981,  -0.04262936,  -0.04792671,\n",
      "         0.06470225,  -0.17186968,  -0.04045397,  -0.86866065,\n",
      "         0.04548476,  -2.04471528,   0.03955626,  -0.13727929,\n",
      "        -0.98274748,  -0.05331135,  -0.05050099,   2.55085532,\n",
      "        -0.04770982,  -0.75798964,  -0.10978223,   0.08482291,\n",
      "        -0.04010407,   0.02433658,  -0.06379083,   0.46468458,\n",
      "        -0.03626224,  -0.25183981,  -0.04470472,   0.04189256,\n",
      "         0.05738564,   0.34968252,  -0.05325757,  -0.05816951,\n",
      "        -0.04742362,  -0.13488117,   0.57115866,  -0.12041017,\n",
      "        -0.05421389,   0.74412936,  -0.42603329,   0.89279726,\n",
      "         0.58672237,  -0.23006568,  -0.02139257,   0.05678192,\n",
      "        -0.10995877,   0.19420486,  -0.20210963,  -0.24475793,\n",
      "        -0.42905823,   0.10793469,  -0.09042159,  -0.16941857,\n",
      "        -0.35834296,   0.40678275,   0.22838668,  -0.26472211,\n",
      "        -0.04856871,  -0.05293627,  -0.04066544,  -0.04784259,\n",
      "         0.19486258,   0.40762175,  -0.04447454,  -0.08684586,\n",
      "        -0.17801781,  -0.0458611 ,  -1.38670389,  -0.28510687,\n",
      "         0.21223469,  -0.18606666,   0.14196746,   0.29258205,\n",
      "       -20.09313232,   0.32931744,  -0.18375469,  -0.04001943,\n",
      "        -0.39151771,  -0.03904836,   0.59752232,   0.02808484,\n",
      "        -0.19960682,   0.07032101,   0.24531732,  -0.06054413,\n",
      "        -0.54440034,   0.26398962,  -0.28201224,  -0.05007804,\n",
      "        -0.04853036,   0.13249121,   0.06839459,  -0.04363469,\n",
      "        -0.42216546,  -0.04842154,  -0.3795266 ,   0.18156739,\n",
      "        -0.33966096,   0.52875125,  -0.98329253,  -0.53143234,\n",
      "         0.31181468,  -0.24651395,  -0.24869364,  -0.04102141,\n",
      "        -0.04988129,  -0.0459847 ,  -0.25180556,  -1.21014543,\n",
      "        -0.14125471,   0.01474863,  -0.2009315 ,  -0.28087415,\n",
      "        -0.04653046,  -0.0491565 ,  -0.0933311 ,   0.12153571,\n",
      "         0.16119303,   0.0092188 ,  -0.25058038,  -0.13604185,\n",
      "         0.47189985,  -0.54134616,   0.4344008 ,  -0.06143326,\n",
      "        -0.05703553,  -0.33349593,  -0.14478635,   0.19271954,\n",
      "        -0.04981003,  -0.0461583 ,  -0.22537688,  -0.00102396,\n",
      "        -0.16584727,   0.35948583,   0.1373286 ,   0.10972395,\n",
      "        -0.03806928,  -0.0611317 ,   0.38013811,  -0.52681115,\n",
      "        -0.20833474,   0.16818352,  -0.05278931,   0.05878097,\n",
      "        -0.04275971,  -0.01573796,  -0.40296545,  -0.45632024,\n",
      "        -0.04113582,  -0.0888247 ,  -0.05157496,   0.30594793,\n",
      "         0.27230603,  -0.06132613,  -0.17365149,  -0.24018689,\n",
      "        -0.04293977,  -0.19205336,  -0.46521213,   0.15405638,\n",
      "        -0.39504236,   0.57603502,  -0.15068061,   0.14122958,\n",
      "        -0.2427697 ,  -0.00724021,  -0.19038497,   0.03693715,\n",
      "         0.41027752,   0.29343573,  -0.27937109,   0.16133083,\n",
      "         0.29415052,  -0.75269643,   0.16360194,   4.07078942,\n",
      "         0.04819038,  -0.06624684,   0.11104036,  -0.06729304,\n",
      "         0.07940384,  -0.07681912,  -0.48607337,  -0.2388381 ,\n",
      "         0.28218936,   0.04976057,   0.33367399,   0.28941795,\n",
      "         0.1891846 ,   0.04691208,  -0.26278104,   0.42358057,\n",
      "         0.99965209,   0.29909741,   0.06662232,  -0.34831977,\n",
      "         0.04375143,   0.05725793,   0.04619237,   0.04827883,\n",
      "        -0.07007888,   0.17148347,   0.03512934,   0.87168501,\n",
      "        -0.04031405,   2.04631687,  -0.02032023,   0.14093642,\n",
      "         0.97606572,   0.04879988,   0.04645737,  -2.56524091,\n",
      "         0.0542159 ,   0.75831366,   0.11763431,  -0.0850557 ,\n",
      "         0.05521288,  -0.02656443,   0.06204217,  -0.4617273 ,\n",
      "         0.03043993,   0.25865011,   0.04206551,  -0.04953162,\n",
      "        -0.05832841,  -0.33813167,   0.05794315,   0.04560303,\n",
      "         0.05601038,   0.1327505 ,  -0.58087524,   0.11544432,\n",
      "         0.0497775 ,  -0.74486734,   0.4149429 ,  -0.9021611 ,\n",
      "        -0.59746767,   0.22753835,   0.02226742,  -0.05636637,\n",
      "         0.10755611,  -0.1821166 ,   0.20580779,   0.23474309,\n",
      "         0.43635457,  -0.10919955,   0.08126579,   0.16254308,\n",
      "         0.3550723 ,  -0.42051451,  -0.2210241 ,   0.27971377,\n",
      "         0.04926681,   0.04461404,   0.04947443,   0.0507034 ,\n",
      "        -0.19882982,  -0.40588336,   0.05071768,   0.08538216,\n",
      "         0.18057997,   0.0462693 ,   1.3811375 ,   0.28428272,\n",
      "        -0.21305883,   0.18803562,  -0.14055539,  -0.28854755,\n",
      "        20.09124922,  -0.329975  ,   0.18059484,   0.05012706,\n",
      "         0.38208882,   0.04617405,  -0.59564175,  -0.0353145 ,\n",
      "         0.21288478,  -0.06718041,  -0.23444048,   0.0692874 ,\n",
      "         0.53618288,  -0.26463087,   0.29838878,   0.05123829,\n",
      "         0.04998254,  -0.13588624,  -0.06577725,   0.048308  ,\n",
      "         0.42532213,   0.04963029,   0.37796683,  -0.17648517,\n",
      "         0.33757884,  -0.52618272,   0.97593805,   0.52347884,\n",
      "        -0.30004942,   0.23893973,   0.24297418,   0.05257444,\n",
      "         0.05528977,   0.04887805,   0.244764  ,   1.21243761,\n",
      "         0.14738111,  -0.01330383,   0.20739401,   0.2765952 ,\n",
      "         0.04367892,   0.05301902,   0.08638995,  -0.12393722,\n",
      "        -0.16305789,  -0.00470929,   0.2430512 ,   0.14067715,\n",
      "        -0.47026878,   0.5406567 ,  -0.43211158,   0.04808059,\n",
      "         0.05037681,   0.32973657,   0.14326332,  -0.19810369,\n",
      "         0.05058315,   0.05024281,   0.22014033,  -0.00937322,\n",
      "         0.16017869,  -0.35632111,  -0.13072357,  -0.10627271,\n",
      "         0.05530709,   0.05904735,  -0.38096446,   0.52250395,\n",
      "         0.20528376,  -0.16416372,   0.04229355,  -0.06501529,\n",
      "         0.03323472,   0.01058172,   0.39142063,   0.45679551,\n",
      "         0.04993741,   0.09903125,   0.04854241,  -0.30860978,\n",
      "        -0.28115748,   0.05564753,   0.16527227,   0.2488439 ,\n",
      "         0.04356022,   0.17887216,   0.46254955,  -0.16271084,\n",
      "         0.40640964,  -0.57160409,   0.14444123,  -0.13381538,\n",
      "         0.24512643,   0.01809212,   0.19362753,  -0.03964086])\n"
     ]
    }
   ],
   "source": [
    "##======================================================================\n",
    "## STEP 4: Train the softmax classifier\n",
    "\n",
    "lambda_ = 1e-4\n",
    "options_ = {'maxiter': tot_iter, 'disp': True}\n",
    "\n",
    "opt_theta, input_size, num_classes = softmax.softmax_train(hidden_size, num_labels,\n",
    "                                                           lambda_, train_features,\n",
    "                                                           train_labels, options_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.66%\n"
     ]
    }
   ],
   "source": [
    "##======================================================================\n",
    "## STEP 5: Testing\n",
    "\n",
    "predictions = softmax.softmax_predict((opt_theta, input_size, num_classes), test_features)\n",
    "print \"Accuracy: {0:.2f}%\".format(100 * np.sum(predictions == test_labels, dtype=np.float64) / test_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DO NOT RUN\n",
    "\n",
    "import load_MNIST\n",
    "import numpy as np\n",
    "import sparse_autoencoder\n",
    "import scipy.optimize\n",
    "import display_network\n",
    "import softmax\n",
    "\n",
    "## ======================================================================\n",
    "#  STEP 0: Here we provide the relevant parameters values that will\n",
    "#  allow your sparse autoencoder to get good filters;\n",
    "\n",
    "input_size = 28 * 28\n",
    "num_labels = 5\n",
    "hidden_size = 196\n",
    "\n",
    "sparsity_param = 0.1  # desired average activation of the hidden units.\n",
    "lambda_ = 3e-3  # weight decay parameter\n",
    "beta = 3  # weight of sparsity penalty term\n",
    "\n",
    "## ======================================================================\n",
    "#  STEP 1: Load data from the MNIST database\n",
    "#\n",
    "#  This loads our training and test data from the MNIST database files.\n",
    "#  \n",
    "\n",
    "images = load_MNIST.load_MNIST_images('train-images-idx3-ubyte')\n",
    "labels = load_MNIST.load_MNIST_labels('train-labels-idx1-ubyte')\n",
    "\n",
    "unlabeled_index = np.argwhere(labels >= 5).flatten()  #5,6,7,8,9,10 unlabeled\n",
    "labeled_index = np.argwhere(labels < 5).flatten()     #0,1,2,3,4 labeled\n",
    "\n",
    "num_train = round(labeled_index.shape[0] / 2)\n",
    "train_index = labeled_index[0:num_train]\n",
    "test_index = labeled_index[num_train:]\n",
    "\n",
    "unlabeled_data = images[:, unlabeled_index]\n",
    "\n",
    "train_data = images[:, train_index]\n",
    "train_labels = labels[train_index]\n",
    "\n",
    "test_data = images[:, test_index]\n",
    "test_labels = labels[test_index]\n",
    "\n",
    "print '# examples in unlabeled set: {0:d}\\n'.format(unlabeled_data.shape[1])\n",
    "print '# examples in supervised training set: {0:d}\\n'.format(train_data.shape[1])\n",
    "print '# examples in supervised testing set: {0:d}\\n'.format(test_data.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
