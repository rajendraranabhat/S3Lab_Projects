{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 650 Ti (CNMeM is disabled, cuDNN 4007)\n",
      "/home/rahul/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done\n",
      "<type 'dict'>\n",
      "(12180, 13)\n",
      "[[ 343.33881535  292.94608369  317.56346931 ...,    0.            0.            2.        ]\n",
      " [ 229.55690517  183.19172676    0.         ...,    0.            0.            2.        ]\n",
      " [ 106.98249046   79.89465389   60.93378272 ...,   50.91543646\n",
      "    44.87156581    1.        ]\n",
      " ..., \n",
      " [ 232.28249497  135.43054     200.91859872 ...,  187.78214091\n",
      "   191.41443704    0.        ]\n",
      " [ 239.19601078  206.37272444  174.6964434  ...,  297.67230497\n",
      "   202.62736538    0.        ]\n",
      " [ 277.38010202  209.01016123  277.00169283 ...,  146.70298371\n",
      "   309.63963308    0.        ]]\n",
      "Example:\n",
      "[ 229.55690517  183.19172676    0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.            0.\n",
      "    2.        ]\n",
      "(12180, 12) (12180,)\n",
      "(12180,)\n",
      "(8526, 12) (8526,) (3654, 12) (3654,)\n",
      "(8526, 12, 1) (3654, 12, 1)\n",
      "(8526, 'train samples')\n",
      "(3654, 'test samples')\n",
      "Sets made\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/anaconda2/lib/python2.7/site-packages/keras/utils/np_utils.py:16: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y[i, y[i]] = 1.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.optimize as opt\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "from theano import shared\n",
    "import theano\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv\n",
    "\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Layer, Reshape, Merge #,AutoEncoder\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D,Convolution1D,MaxPooling1D,UpSampling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adadelta\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "#import keras.utils.visualize_util as vutil #Pydot issues\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l1, l2, l1l2, activity_l2, activity_l1\n",
    "#import keras.layers.containers as containers  #No module named containers\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.regularizers import ActivityRegularizer\n",
    "from keras import backend as K\n",
    "#import keras.utils.visualize_util as vutil\n",
    "from keras.models import model_from_json\n",
    "#from keras.utils.visualize_util import plot, to_graph\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "np.set_printoptions(suppress=True)  #Supress exponent of the number 2e-2 == 0.02\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"All imports done\")\n",
    "\n",
    "\n",
    "#Ref: http://deeplearning.net/, https://www.kaggle.com/wiki/Tutorials etc..\n",
    "\n",
    "#caso_I.mat, caso_II.mat, caso_III.mat, caso_base.mat, consumer_data.xlsx\n",
    "\n",
    "#print(os.getcwd() + \"\\n\")\n",
    "#print os.listdir(os.getcwd())\n",
    "dataSmartGrid = scpy.loadmat('demcliMat.mat')\n",
    "print type(dataSmartGrid)\n",
    "\n",
    "data = dataSmartGrid['demcliMat']\n",
    "print data.shape\n",
    "print data\n",
    "print(\"Example:\")\n",
    "print data[1]\n",
    "\n",
    "np.unique(data[:,-1:])\n",
    "\n",
    "X = data[:,0:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print X.shape, Y.shape\n",
    "\n",
    "#print X.shape\n",
    "Y1 = Y\n",
    "Y1[Y1==2] = 1\n",
    "print Y1.shape\n",
    "\n",
    "np.unique(Y1)\n",
    "\n",
    "#print data[0:5,:]\n",
    "#print data[0:5,0:-1]\n",
    "#print data[0:5,-1]\n",
    "\n",
    "\n",
    "#print X\n",
    "#print Y\n",
    "#Splitting data into train and testing. 70% Training and 30% Testing..\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y1, test_size=0.3, random_state=4)\n",
    "\n",
    "print X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "#print numpy.unique(Y_test)\n",
    "\n",
    "X_train_1 = X_train.reshape(8526,12,1)\n",
    "X_test_1 = X_test.reshape(3654,12,1)\n",
    "\n",
    "print X_train_1.shape, X_test_1.shape \n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 2)\n",
    "Y_test = np_utils.to_categorical(y_test, 2)\n",
    "\n",
    "Y_train = Y_train.astype('int')\n",
    "Y_test = Y_test.astype('int')\n",
    "\n",
    "print(\"Sets made\")\n",
    "\n",
    "rms = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8526 samples, validate on 3654 samples\n",
      "Epoch 1/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1750 - acc: 0.9516 - val_loss: 0.1385 - val_acc: 0.9663\n",
      "Epoch 2/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1523 - acc: 0.9646 - val_loss: 0.1374 - val_acc: 0.9699\n",
      "Epoch 3/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1485 - acc: 0.9656 - val_loss: 0.1335 - val_acc: 0.9699\n",
      "Epoch 4/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1459 - acc: 0.9655 - val_loss: 0.1431 - val_acc: 0.9699\n",
      "Epoch 5/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1453 - acc: 0.9654 - val_loss: 0.1294 - val_acc: 0.9699\n",
      "Epoch 6/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1441 - acc: 0.9656 - val_loss: 0.1276 - val_acc: 0.9699\n",
      "Epoch 7/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1418 - acc: 0.9661 - val_loss: 0.1242 - val_acc: 0.9702\n",
      "Epoch 8/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1385 - acc: 0.9661 - val_loss: 0.1240 - val_acc: 0.9710\n",
      "Epoch 9/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1390 - acc: 0.9659 - val_loss: 0.1289 - val_acc: 0.9699\n",
      "Epoch 10/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1386 - acc: 0.9666 - val_loss: 0.1291 - val_acc: 0.9699\n",
      "Epoch 11/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1389 - acc: 0.9660 - val_loss: 0.1264 - val_acc: 0.9702\n",
      "Epoch 12/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1381 - acc: 0.9661 - val_loss: 0.1279 - val_acc: 0.9699\n",
      "Epoch 13/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1364 - acc: 0.9667 - val_loss: 0.1239 - val_acc: 0.9702\n",
      "Epoch 14/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1359 - acc: 0.9661 - val_loss: 0.1208 - val_acc: 0.9702\n",
      "Epoch 15/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1371 - acc: 0.9662 - val_loss: 0.1218 - val_acc: 0.9699\n",
      "Epoch 16/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1360 - acc: 0.9672 - val_loss: 0.1309 - val_acc: 0.9699\n",
      "Epoch 17/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1365 - acc: 0.9661 - val_loss: 0.1224 - val_acc: 0.9680\n",
      "Epoch 18/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1353 - acc: 0.9667 - val_loss: 0.1211 - val_acc: 0.9696\n",
      "Epoch 19/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1352 - acc: 0.9666 - val_loss: 0.1208 - val_acc: 0.9702\n",
      "Epoch 20/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1345 - acc: 0.9667 - val_loss: 0.1211 - val_acc: 0.9702\n",
      "Epoch 21/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1332 - acc: 0.9670 - val_loss: 0.1192 - val_acc: 0.9707\n",
      "Epoch 22/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1327 - acc: 0.9667 - val_loss: 0.1173 - val_acc: 0.9707\n",
      "Epoch 23/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1335 - acc: 0.9663 - val_loss: 0.1247 - val_acc: 0.9699\n",
      "Epoch 24/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1323 - acc: 0.9665 - val_loss: 0.1207 - val_acc: 0.9699\n",
      "Epoch 25/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1311 - acc: 0.9666 - val_loss: 0.1194 - val_acc: 0.9707\n",
      "Epoch 26/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1343 - acc: 0.9662 - val_loss: 0.1218 - val_acc: 0.9702\n",
      "Epoch 27/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1318 - acc: 0.9673 - val_loss: 0.1192 - val_acc: 0.9715\n",
      "Epoch 28/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1325 - acc: 0.9668 - val_loss: 0.1237 - val_acc: 0.9704\n",
      "Epoch 29/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1321 - acc: 0.9673 - val_loss: 0.1252 - val_acc: 0.9699\n",
      "Epoch 30/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1310 - acc: 0.9663 - val_loss: 0.1201 - val_acc: 0.9707\n",
      "Epoch 31/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1320 - acc: 0.9674 - val_loss: 0.1197 - val_acc: 0.9710\n",
      "Epoch 32/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1315 - acc: 0.9662 - val_loss: 0.1181 - val_acc: 0.9715\n",
      "Epoch 33/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1343 - acc: 0.9668 - val_loss: 0.1199 - val_acc: 0.9699\n",
      "Epoch 34/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1298 - acc: 0.9667 - val_loss: 0.1280 - val_acc: 0.9685\n",
      "Epoch 35/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1293 - acc: 0.9675 - val_loss: 0.1358 - val_acc: 0.9699\n",
      "Epoch 36/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1309 - acc: 0.9674 - val_loss: 0.1200 - val_acc: 0.9704\n",
      "Epoch 37/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1312 - acc: 0.9665 - val_loss: 0.1176 - val_acc: 0.9713\n",
      "Epoch 38/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1309 - acc: 0.9677 - val_loss: 0.1202 - val_acc: 0.9688\n",
      "Epoch 39/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1312 - acc: 0.9673 - val_loss: 0.1218 - val_acc: 0.9691\n",
      "Epoch 40/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1295 - acc: 0.9670 - val_loss: 0.1161 - val_acc: 0.9707\n",
      "Epoch 41/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1301 - acc: 0.9668 - val_loss: 0.1212 - val_acc: 0.9702\n",
      "Epoch 42/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1303 - acc: 0.9672 - val_loss: 0.1177 - val_acc: 0.9710\n",
      "Epoch 43/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1294 - acc: 0.9676 - val_loss: 0.1184 - val_acc: 0.9713\n",
      "Epoch 44/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1293 - acc: 0.9677 - val_loss: 0.1163 - val_acc: 0.9713\n",
      "Epoch 45/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1297 - acc: 0.9684 - val_loss: 0.1221 - val_acc: 0.9702\n",
      "Epoch 46/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1293 - acc: 0.9666 - val_loss: 0.1241 - val_acc: 0.9713\n",
      "Epoch 47/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1296 - acc: 0.9679 - val_loss: 0.1156 - val_acc: 0.9718\n",
      "Epoch 48/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1301 - acc: 0.9669 - val_loss: 0.1215 - val_acc: 0.9699\n",
      "Epoch 49/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1296 - acc: 0.9672 - val_loss: 0.1213 - val_acc: 0.9702\n",
      "Epoch 50/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1295 - acc: 0.9681 - val_loss: 0.1196 - val_acc: 0.9707\n",
      "Epoch 51/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1290 - acc: 0.9676 - val_loss: 0.1205 - val_acc: 0.9710\n",
      "Epoch 52/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1287 - acc: 0.9680 - val_loss: 0.1181 - val_acc: 0.9710\n",
      "Epoch 53/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1293 - acc: 0.9677 - val_loss: 0.1163 - val_acc: 0.9718\n",
      "Epoch 54/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1290 - acc: 0.9677 - val_loss: 0.1169 - val_acc: 0.9710\n",
      "Epoch 55/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1259 - acc: 0.9689 - val_loss: 0.1217 - val_acc: 0.9710\n",
      "Epoch 56/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1264 - acc: 0.9682 - val_loss: 0.1186 - val_acc: 0.9715\n",
      "Epoch 57/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1270 - acc: 0.9687 - val_loss: 0.1161 - val_acc: 0.9724\n",
      "Epoch 58/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1293 - acc: 0.9674 - val_loss: 0.1203 - val_acc: 0.9704\n",
      "Epoch 59/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1278 - acc: 0.9682 - val_loss: 0.1187 - val_acc: 0.9715\n",
      "Epoch 60/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1287 - acc: 0.9682 - val_loss: 0.1156 - val_acc: 0.9718\n",
      "Epoch 61/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1255 - acc: 0.9689 - val_loss: 0.1183 - val_acc: 0.9704\n",
      "Epoch 62/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1278 - acc: 0.9684 - val_loss: 0.1175 - val_acc: 0.9713\n",
      "Epoch 63/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1257 - acc: 0.9688 - val_loss: 0.1187 - val_acc: 0.9704\n",
      "Epoch 64/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1280 - acc: 0.9674 - val_loss: 0.1189 - val_acc: 0.9718\n",
      "Epoch 65/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1265 - acc: 0.9683 - val_loss: 0.1159 - val_acc: 0.9713\n",
      "Epoch 66/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1275 - acc: 0.9683 - val_loss: 0.1166 - val_acc: 0.9702\n",
      "Epoch 67/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1279 - acc: 0.9681 - val_loss: 0.1214 - val_acc: 0.9710\n",
      "Epoch 68/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1264 - acc: 0.9684 - val_loss: 0.1151 - val_acc: 0.9702\n",
      "Epoch 69/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1259 - acc: 0.9680 - val_loss: 0.1749 - val_acc: 0.9417\n",
      "Epoch 70/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1271 - acc: 0.9669 - val_loss: 0.1161 - val_acc: 0.9696\n",
      "Epoch 71/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1269 - acc: 0.9682 - val_loss: 0.1214 - val_acc: 0.9704\n",
      "Epoch 72/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1265 - acc: 0.9676 - val_loss: 0.1182 - val_acc: 0.9715\n",
      "Epoch 73/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1259 - acc: 0.9673 - val_loss: 0.1225 - val_acc: 0.9699\n",
      "Epoch 74/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1263 - acc: 0.9677 - val_loss: 0.1176 - val_acc: 0.9710\n",
      "Epoch 75/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1249 - acc: 0.9688 - val_loss: 0.1150 - val_acc: 0.9707\n",
      "Epoch 76/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1268 - acc: 0.9687 - val_loss: 0.1160 - val_acc: 0.9707\n",
      "Epoch 77/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1257 - acc: 0.9673 - val_loss: 0.1210 - val_acc: 0.9696\n",
      "Epoch 78/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1261 - acc: 0.9687 - val_loss: 0.1131 - val_acc: 0.9726\n",
      "Epoch 79/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1273 - acc: 0.9683 - val_loss: 0.1211 - val_acc: 0.9713\n",
      "Epoch 80/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1252 - acc: 0.9683 - val_loss: 0.1171 - val_acc: 0.9715\n",
      "Epoch 81/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1247 - acc: 0.9682 - val_loss: 0.1152 - val_acc: 0.9718\n",
      "Epoch 82/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1245 - acc: 0.9684 - val_loss: 0.1209 - val_acc: 0.9707\n",
      "Epoch 83/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1261 - acc: 0.9684 - val_loss: 0.1226 - val_acc: 0.9718\n",
      "Epoch 84/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1248 - acc: 0.9688 - val_loss: 0.1193 - val_acc: 0.9710\n",
      "Epoch 85/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1244 - acc: 0.9690 - val_loss: 0.1146 - val_acc: 0.9721\n",
      "Epoch 86/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1240 - acc: 0.9695 - val_loss: 0.1157 - val_acc: 0.9713\n",
      "Epoch 87/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1244 - acc: 0.9689 - val_loss: 0.1164 - val_acc: 0.9713\n",
      "Epoch 88/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1242 - acc: 0.9688 - val_loss: 0.1162 - val_acc: 0.9718\n",
      "Epoch 89/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1254 - acc: 0.9683 - val_loss: 0.1148 - val_acc: 0.9724\n",
      "Epoch 90/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1227 - acc: 0.9690 - val_loss: 0.1265 - val_acc: 0.9704\n",
      "Epoch 91/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1238 - acc: 0.9682 - val_loss: 0.1199 - val_acc: 0.9696\n",
      "Epoch 92/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1231 - acc: 0.9689 - val_loss: 0.1198 - val_acc: 0.9713\n",
      "Epoch 93/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1232 - acc: 0.9689 - val_loss: 0.1194 - val_acc: 0.9715\n",
      "Epoch 94/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1247 - acc: 0.9692 - val_loss: 0.1175 - val_acc: 0.9713\n",
      "Epoch 95/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1230 - acc: 0.9687 - val_loss: 0.1169 - val_acc: 0.9713\n",
      "Epoch 96/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1250 - acc: 0.9686 - val_loss: 0.1152 - val_acc: 0.9710\n",
      "Epoch 97/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1261 - acc: 0.9688 - val_loss: 0.1162 - val_acc: 0.9724\n",
      "Epoch 98/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1232 - acc: 0.9692 - val_loss: 0.1142 - val_acc: 0.9718\n",
      "Epoch 99/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1227 - acc: 0.9689 - val_loss: 0.1347 - val_acc: 0.9699\n",
      "Epoch 100/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1215 - acc: 0.9692 - val_loss: 0.1180 - val_acc: 0.9707\n",
      "Epoch 101/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1255 - acc: 0.9686 - val_loss: 0.1178 - val_acc: 0.9710\n",
      "Epoch 102/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1229 - acc: 0.9688 - val_loss: 0.1194 - val_acc: 0.9707\n",
      "Epoch 103/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1218 - acc: 0.9692 - val_loss: 0.1175 - val_acc: 0.9710\n",
      "Epoch 104/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1230 - acc: 0.9695 - val_loss: 0.1174 - val_acc: 0.9721\n",
      "Epoch 105/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1219 - acc: 0.9690 - val_loss: 0.1179 - val_acc: 0.9713\n",
      "Epoch 106/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1229 - acc: 0.9699 - val_loss: 0.1247 - val_acc: 0.9688\n",
      "Epoch 107/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1223 - acc: 0.9700 - val_loss: 0.1231 - val_acc: 0.9710\n",
      "Epoch 108/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1230 - acc: 0.9692 - val_loss: 0.1169 - val_acc: 0.9715\n",
      "Epoch 109/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1202 - acc: 0.9702 - val_loss: 0.1197 - val_acc: 0.9713\n",
      "Epoch 110/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1206 - acc: 0.9694 - val_loss: 0.1285 - val_acc: 0.9704\n",
      "Epoch 111/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1217 - acc: 0.9694 - val_loss: 0.1254 - val_acc: 0.9669\n",
      "Epoch 112/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1215 - acc: 0.9695 - val_loss: 0.1222 - val_acc: 0.9718\n",
      "Epoch 113/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1247 - acc: 0.9689 - val_loss: 0.1142 - val_acc: 0.9724\n",
      "Epoch 114/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1218 - acc: 0.9689 - val_loss: 0.1223 - val_acc: 0.9707\n",
      "Epoch 115/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1242 - acc: 0.9677 - val_loss: 0.1121 - val_acc: 0.9718\n",
      "Epoch 116/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1236 - acc: 0.9699 - val_loss: 0.1144 - val_acc: 0.9721\n",
      "Epoch 117/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1249 - acc: 0.9682 - val_loss: 0.1178 - val_acc: 0.9704\n",
      "Epoch 118/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1238 - acc: 0.9687 - val_loss: 0.1151 - val_acc: 0.9721\n",
      "Epoch 119/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1222 - acc: 0.9687 - val_loss: 0.1174 - val_acc: 0.9715\n",
      "Epoch 120/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1221 - acc: 0.9689 - val_loss: 0.1147 - val_acc: 0.9713\n",
      "Epoch 121/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1216 - acc: 0.9690 - val_loss: 0.1180 - val_acc: 0.9713\n",
      "Epoch 122/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1234 - acc: 0.9688 - val_loss: 0.1241 - val_acc: 0.9710\n",
      "Epoch 123/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1225 - acc: 0.9697 - val_loss: 0.1198 - val_acc: 0.9713\n",
      "Epoch 124/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1226 - acc: 0.9693 - val_loss: 0.1158 - val_acc: 0.9715\n",
      "Epoch 125/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1210 - acc: 0.9682 - val_loss: 0.1203 - val_acc: 0.9707\n",
      "Epoch 126/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1217 - acc: 0.9697 - val_loss: 0.1174 - val_acc: 0.9721\n",
      "Epoch 127/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1213 - acc: 0.9703 - val_loss: 0.1223 - val_acc: 0.9691\n",
      "Epoch 128/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1215 - acc: 0.9690 - val_loss: 0.1240 - val_acc: 0.9688\n",
      "Epoch 129/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1194 - acc: 0.9694 - val_loss: 0.1212 - val_acc: 0.9696\n",
      "Epoch 130/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1204 - acc: 0.9696 - val_loss: 0.1229 - val_acc: 0.9699\n",
      "Epoch 131/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1199 - acc: 0.9694 - val_loss: 0.1210 - val_acc: 0.9715\n",
      "Epoch 132/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1212 - acc: 0.9694 - val_loss: 0.1198 - val_acc: 0.9718\n",
      "Epoch 133/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1201 - acc: 0.9693 - val_loss: 0.1156 - val_acc: 0.9715\n",
      "Epoch 134/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1200 - acc: 0.9696 - val_loss: 0.1191 - val_acc: 0.9713\n",
      "Epoch 135/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1216 - acc: 0.9692 - val_loss: 0.1162 - val_acc: 0.9710\n",
      "Epoch 136/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1216 - acc: 0.9687 - val_loss: 0.1186 - val_acc: 0.9713\n",
      "Epoch 137/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1211 - acc: 0.9695 - val_loss: 0.1158 - val_acc: 0.9713\n",
      "Epoch 138/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1204 - acc: 0.9693 - val_loss: 0.1140 - val_acc: 0.9724\n",
      "Epoch 139/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1203 - acc: 0.9700 - val_loss: 0.1223 - val_acc: 0.9707\n",
      "Epoch 140/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1214 - acc: 0.9690 - val_loss: 0.1173 - val_acc: 0.9721\n",
      "Epoch 141/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1187 - acc: 0.9701 - val_loss: 0.1194 - val_acc: 0.9715\n",
      "Epoch 142/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1189 - acc: 0.9695 - val_loss: 0.1166 - val_acc: 0.9718\n",
      "Epoch 143/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1215 - acc: 0.9700 - val_loss: 0.1188 - val_acc: 0.9710\n",
      "Epoch 144/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1183 - acc: 0.9700 - val_loss: 0.1244 - val_acc: 0.9713\n",
      "Epoch 145/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1212 - acc: 0.9696 - val_loss: 0.1180 - val_acc: 0.9715\n",
      "Epoch 146/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1199 - acc: 0.9697 - val_loss: 0.1183 - val_acc: 0.9724\n",
      "Epoch 147/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1208 - acc: 0.9695 - val_loss: 0.1224 - val_acc: 0.9704\n",
      "Epoch 148/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1219 - acc: 0.9696 - val_loss: 0.1191 - val_acc: 0.9710\n",
      "Epoch 149/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1205 - acc: 0.9689 - val_loss: 0.1162 - val_acc: 0.9718\n",
      "Epoch 150/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1187 - acc: 0.9697 - val_loss: 0.1173 - val_acc: 0.9718\n",
      "Epoch 151/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1195 - acc: 0.9700 - val_loss: 0.1174 - val_acc: 0.9715\n",
      "Epoch 152/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1178 - acc: 0.9697 - val_loss: 0.1189 - val_acc: 0.9718\n",
      "Epoch 153/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1209 - acc: 0.9690 - val_loss: 0.1175 - val_acc: 0.9713\n",
      "Epoch 154/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1184 - acc: 0.9702 - val_loss: 0.1192 - val_acc: 0.9715\n",
      "Epoch 155/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1199 - acc: 0.9695 - val_loss: 0.1163 - val_acc: 0.9713\n",
      "Epoch 156/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1190 - acc: 0.9701 - val_loss: 0.1213 - val_acc: 0.9704\n",
      "Epoch 157/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1175 - acc: 0.9703 - val_loss: 0.1209 - val_acc: 0.9721\n",
      "Epoch 158/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1186 - acc: 0.9697 - val_loss: 0.1171 - val_acc: 0.9702\n",
      "Epoch 159/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1167 - acc: 0.9701 - val_loss: 0.1161 - val_acc: 0.9715\n",
      "Epoch 160/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1175 - acc: 0.9706 - val_loss: 0.1177 - val_acc: 0.9713\n",
      "Epoch 161/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1192 - acc: 0.9706 - val_loss: 0.1179 - val_acc: 0.9699\n",
      "Epoch 162/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1191 - acc: 0.9702 - val_loss: 0.1214 - val_acc: 0.9715\n",
      "Epoch 163/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1196 - acc: 0.9702 - val_loss: 0.1199 - val_acc: 0.9710\n",
      "Epoch 164/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1182 - acc: 0.9694 - val_loss: 0.1172 - val_acc: 0.9732\n",
      "Epoch 165/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1172 - acc: 0.9699 - val_loss: 0.1187 - val_acc: 0.9718\n",
      "Epoch 166/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1153 - acc: 0.9709 - val_loss: 0.1188 - val_acc: 0.9713\n",
      "Epoch 167/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1171 - acc: 0.9697 - val_loss: 0.1249 - val_acc: 0.9704\n",
      "Epoch 168/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1158 - acc: 0.9707 - val_loss: 0.1251 - val_acc: 0.9710\n",
      "Epoch 169/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1188 - acc: 0.9704 - val_loss: 0.1193 - val_acc: 0.9721\n",
      "Epoch 170/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1156 - acc: 0.9710 - val_loss: 0.1258 - val_acc: 0.9704\n",
      "Epoch 171/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1162 - acc: 0.9709 - val_loss: 0.1220 - val_acc: 0.9715\n",
      "Epoch 172/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1202 - acc: 0.9699 - val_loss: 0.1181 - val_acc: 0.9710\n",
      "Epoch 173/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1157 - acc: 0.9714 - val_loss: 0.1192 - val_acc: 0.9718\n",
      "Epoch 174/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1181 - acc: 0.9707 - val_loss: 0.1194 - val_acc: 0.9696\n",
      "Epoch 175/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1167 - acc: 0.9704 - val_loss: 0.1198 - val_acc: 0.9710\n",
      "Epoch 176/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1138 - acc: 0.9701 - val_loss: 0.1195 - val_acc: 0.9715\n",
      "Epoch 177/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1158 - acc: 0.9710 - val_loss: 0.1214 - val_acc: 0.9713\n",
      "Epoch 178/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1163 - acc: 0.9699 - val_loss: 0.1178 - val_acc: 0.9710\n",
      "Epoch 179/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1171 - acc: 0.9700 - val_loss: 0.1205 - val_acc: 0.9702\n",
      "Epoch 180/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1135 - acc: 0.9707 - val_loss: 0.1201 - val_acc: 0.9704\n",
      "Epoch 181/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1162 - acc: 0.9701 - val_loss: 0.1175 - val_acc: 0.9721\n",
      "Epoch 182/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1170 - acc: 0.9706 - val_loss: 0.1233 - val_acc: 0.9693\n",
      "Epoch 183/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1138 - acc: 0.9713 - val_loss: 0.1221 - val_acc: 0.9713\n",
      "Epoch 184/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1162 - acc: 0.9708 - val_loss: 0.1245 - val_acc: 0.9702\n",
      "Epoch 185/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1150 - acc: 0.9702 - val_loss: 0.1225 - val_acc: 0.9704\n",
      "Epoch 186/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1152 - acc: 0.9704 - val_loss: 0.1201 - val_acc: 0.9704\n",
      "Epoch 187/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1189 - acc: 0.9697 - val_loss: 0.1176 - val_acc: 0.9718\n",
      "Epoch 188/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1160 - acc: 0.9699 - val_loss: 0.1204 - val_acc: 0.9715\n",
      "Epoch 189/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1159 - acc: 0.9704 - val_loss: 0.1195 - val_acc: 0.9718\n",
      "Epoch 190/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1154 - acc: 0.9715 - val_loss: 0.1192 - val_acc: 0.9718\n",
      "Epoch 191/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1156 - acc: 0.9703 - val_loss: 0.1191 - val_acc: 0.9718\n",
      "Epoch 192/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1140 - acc: 0.9709 - val_loss: 0.1159 - val_acc: 0.9718\n",
      "Epoch 193/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1139 - acc: 0.9714 - val_loss: 0.1236 - val_acc: 0.9721\n",
      "Epoch 194/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1195 - acc: 0.9697 - val_loss: 0.1215 - val_acc: 0.9710\n",
      "Epoch 195/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1126 - acc: 0.9720 - val_loss: 0.1184 - val_acc: 0.9715\n",
      "Epoch 196/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1144 - acc: 0.9697 - val_loss: 0.1201 - val_acc: 0.9704\n",
      "Epoch 197/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1129 - acc: 0.9722 - val_loss: 0.1275 - val_acc: 0.9669\n",
      "Epoch 198/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1147 - acc: 0.9710 - val_loss: 0.1200 - val_acc: 0.9715\n",
      "Epoch 199/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1156 - acc: 0.9706 - val_loss: 0.1249 - val_acc: 0.9693\n",
      "Epoch 200/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1148 - acc: 0.9709 - val_loss: 0.1213 - val_acc: 0.9715\n",
      "Epoch 201/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1131 - acc: 0.9713 - val_loss: 0.1240 - val_acc: 0.9715\n",
      "Epoch 202/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1142 - acc: 0.9707 - val_loss: 0.1217 - val_acc: 0.9715\n",
      "Epoch 203/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1151 - acc: 0.9713 - val_loss: 0.1227 - val_acc: 0.9696\n",
      "Epoch 204/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1148 - acc: 0.9704 - val_loss: 0.1295 - val_acc: 0.9710\n",
      "Epoch 205/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1144 - acc: 0.9701 - val_loss: 0.1226 - val_acc: 0.9702\n",
      "Epoch 206/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1134 - acc: 0.9709 - val_loss: 0.1193 - val_acc: 0.9713\n",
      "Epoch 207/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1135 - acc: 0.9709 - val_loss: 0.1233 - val_acc: 0.9710\n",
      "Epoch 208/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1158 - acc: 0.9709 - val_loss: 0.1326 - val_acc: 0.9693\n",
      "Epoch 209/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1159 - acc: 0.9709 - val_loss: 0.1228 - val_acc: 0.9674\n",
      "Epoch 210/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1129 - acc: 0.9719 - val_loss: 0.1272 - val_acc: 0.9713\n",
      "Epoch 211/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1137 - acc: 0.9706 - val_loss: 0.1339 - val_acc: 0.9710\n",
      "Epoch 212/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1115 - acc: 0.9720 - val_loss: 0.1318 - val_acc: 0.9674\n",
      "Epoch 213/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1130 - acc: 0.9708 - val_loss: 0.1196 - val_acc: 0.9707\n",
      "Epoch 214/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1133 - acc: 0.9711 - val_loss: 0.1227 - val_acc: 0.9715\n",
      "Epoch 215/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1130 - acc: 0.9693 - val_loss: 0.1216 - val_acc: 0.9699\n",
      "Epoch 216/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1138 - acc: 0.9713 - val_loss: 0.1206 - val_acc: 0.9704\n",
      "Epoch 217/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1100 - acc: 0.9716 - val_loss: 0.1180 - val_acc: 0.9718\n",
      "Epoch 218/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1115 - acc: 0.9709 - val_loss: 0.1236 - val_acc: 0.9718\n",
      "Epoch 219/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1130 - acc: 0.9709 - val_loss: 0.1216 - val_acc: 0.9702\n",
      "Epoch 220/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1087 - acc: 0.9716 - val_loss: 0.1320 - val_acc: 0.9661\n",
      "Epoch 221/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1121 - acc: 0.9710 - val_loss: 0.1325 - val_acc: 0.9710\n",
      "Epoch 222/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1120 - acc: 0.9709 - val_loss: 0.1244 - val_acc: 0.9718\n",
      "Epoch 223/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1108 - acc: 0.9713 - val_loss: 0.1305 - val_acc: 0.9677\n",
      "Epoch 224/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1120 - acc: 0.9710 - val_loss: 0.1215 - val_acc: 0.9718\n",
      "Epoch 225/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1128 - acc: 0.9697 - val_loss: 0.1212 - val_acc: 0.9704\n",
      "Epoch 226/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1088 - acc: 0.9715 - val_loss: 0.1265 - val_acc: 0.9704\n",
      "Epoch 227/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1125 - acc: 0.9709 - val_loss: 0.1179 - val_acc: 0.9713\n",
      "Epoch 228/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1112 - acc: 0.9711 - val_loss: 0.1210 - val_acc: 0.9710\n",
      "Epoch 229/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1116 - acc: 0.9709 - val_loss: 0.1397 - val_acc: 0.9587\n",
      "Epoch 230/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1107 - acc: 0.9713 - val_loss: 0.1284 - val_acc: 0.9696\n",
      "Epoch 231/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1111 - acc: 0.9702 - val_loss: 0.1281 - val_acc: 0.9707\n",
      "Epoch 232/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1089 - acc: 0.9719 - val_loss: 0.1270 - val_acc: 0.9707\n",
      "Epoch 233/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1082 - acc: 0.9714 - val_loss: 0.1290 - val_acc: 0.9704\n",
      "Epoch 234/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1088 - acc: 0.9707 - val_loss: 0.1246 - val_acc: 0.9724\n",
      "Epoch 235/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1077 - acc: 0.9733 - val_loss: 0.1216 - val_acc: 0.9704\n",
      "Epoch 236/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1096 - acc: 0.9724 - val_loss: 0.1295 - val_acc: 0.9715\n",
      "Epoch 237/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1096 - acc: 0.9706 - val_loss: 0.1315 - val_acc: 0.9713\n",
      "Epoch 238/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1096 - acc: 0.9719 - val_loss: 0.1243 - val_acc: 0.9685\n",
      "Epoch 239/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1089 - acc: 0.9720 - val_loss: 0.1232 - val_acc: 0.9715\n",
      "Epoch 240/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1060 - acc: 0.9721 - val_loss: 0.1279 - val_acc: 0.9715\n",
      "Epoch 241/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1081 - acc: 0.9714 - val_loss: 0.1248 - val_acc: 0.9710\n",
      "Epoch 242/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1091 - acc: 0.9719 - val_loss: 0.1236 - val_acc: 0.9683\n",
      "Epoch 243/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1069 - acc: 0.9727 - val_loss: 0.1215 - val_acc: 0.9713\n",
      "Epoch 244/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1096 - acc: 0.9715 - val_loss: 0.1223 - val_acc: 0.9710\n",
      "Epoch 245/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1060 - acc: 0.9721 - val_loss: 0.1209 - val_acc: 0.9718\n",
      "Epoch 246/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1066 - acc: 0.9723 - val_loss: 0.1271 - val_acc: 0.9710\n",
      "Epoch 247/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1097 - acc: 0.9713 - val_loss: 0.1284 - val_acc: 0.9707\n",
      "Epoch 248/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1056 - acc: 0.9723 - val_loss: 0.1298 - val_acc: 0.9707\n",
      "Epoch 249/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1042 - acc: 0.9723 - val_loss: 0.1262 - val_acc: 0.9702\n",
      "Epoch 250/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1029 - acc: 0.9722 - val_loss: 0.1288 - val_acc: 0.9699\n",
      "Epoch 251/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1055 - acc: 0.9726 - val_loss: 0.1333 - val_acc: 0.9693\n",
      "Epoch 252/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1054 - acc: 0.9719 - val_loss: 0.1266 - val_acc: 0.9713\n",
      "Epoch 253/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1061 - acc: 0.9720 - val_loss: 0.1272 - val_acc: 0.9704\n",
      "Epoch 254/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0997 - acc: 0.9729 - val_loss: 0.1217 - val_acc: 0.9707\n",
      "Epoch 255/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1039 - acc: 0.9727 - val_loss: 0.1349 - val_acc: 0.9704\n",
      "Epoch 256/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1025 - acc: 0.9738 - val_loss: 0.1330 - val_acc: 0.9693\n",
      "Epoch 257/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1051 - acc: 0.9722 - val_loss: 0.1245 - val_acc: 0.9704\n",
      "Epoch 258/500\n",
      "8526/8526 [==============================] - 9s - loss: 0.1028 - acc: 0.9727 - val_loss: 0.1301 - val_acc: 0.9710\n",
      "Epoch 259/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1000 - acc: 0.9737 - val_loss: 0.1245 - val_acc: 0.9704\n",
      "Epoch 260/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1037 - acc: 0.9720 - val_loss: 0.1298 - val_acc: 0.9658\n",
      "Epoch 261/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1056 - acc: 0.9727 - val_loss: 0.1268 - val_acc: 0.9704\n",
      "Epoch 262/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1008 - acc: 0.9737 - val_loss: 0.1246 - val_acc: 0.9710\n",
      "Epoch 263/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1004 - acc: 0.9731 - val_loss: 0.1283 - val_acc: 0.9704\n",
      "Epoch 264/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1018 - acc: 0.9733 - val_loss: 0.1295 - val_acc: 0.9713\n",
      "Epoch 265/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1050 - acc: 0.9727 - val_loss: 0.1262 - val_acc: 0.9702\n",
      "Epoch 266/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1007 - acc: 0.9738 - val_loss: 0.1287 - val_acc: 0.9680\n",
      "Epoch 267/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1011 - acc: 0.9729 - val_loss: 0.1300 - val_acc: 0.9710\n",
      "Epoch 268/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0997 - acc: 0.9738 - val_loss: 0.1300 - val_acc: 0.9699\n",
      "Epoch 269/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1026 - acc: 0.9726 - val_loss: 0.1265 - val_acc: 0.9691\n",
      "Epoch 270/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.1039 - acc: 0.9715 - val_loss: 0.1261 - val_acc: 0.9715\n",
      "Epoch 271/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1003 - acc: 0.9727 - val_loss: 0.1324 - val_acc: 0.9707\n",
      "Epoch 272/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0991 - acc: 0.9729 - val_loss: 0.1324 - val_acc: 0.9693\n",
      "Epoch 273/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0990 - acc: 0.9740 - val_loss: 0.1287 - val_acc: 0.9688\n",
      "Epoch 274/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0994 - acc: 0.9723 - val_loss: 0.1317 - val_acc: 0.9715\n",
      "Epoch 275/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0990 - acc: 0.9740 - val_loss: 0.1325 - val_acc: 0.9672\n",
      "Epoch 276/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0971 - acc: 0.9744 - val_loss: 0.1397 - val_acc: 0.9710\n",
      "Epoch 277/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1006 - acc: 0.9724 - val_loss: 0.1410 - val_acc: 0.9663\n",
      "Epoch 278/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0998 - acc: 0.9738 - val_loss: 0.1333 - val_acc: 0.9710\n",
      "Epoch 279/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1000 - acc: 0.9740 - val_loss: 0.1285 - val_acc: 0.9699\n",
      "Epoch 280/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.1017 - acc: 0.9722 - val_loss: 0.1309 - val_acc: 0.9718\n",
      "Epoch 281/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0985 - acc: 0.9736 - val_loss: 0.1303 - val_acc: 0.9710\n",
      "Epoch 282/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0985 - acc: 0.9728 - val_loss: 0.1322 - val_acc: 0.9674\n",
      "Epoch 283/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0976 - acc: 0.9737 - val_loss: 0.1281 - val_acc: 0.9710\n",
      "Epoch 284/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0980 - acc: 0.9735 - val_loss: 0.1356 - val_acc: 0.9718\n",
      "Epoch 285/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0990 - acc: 0.9727 - val_loss: 0.1360 - val_acc: 0.9680\n",
      "Epoch 286/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0969 - acc: 0.9747 - val_loss: 0.1274 - val_acc: 0.9713\n",
      "Epoch 287/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0973 - acc: 0.9743 - val_loss: 0.1282 - val_acc: 0.9683\n",
      "Epoch 288/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0978 - acc: 0.9736 - val_loss: 0.1331 - val_acc: 0.9691\n",
      "Epoch 289/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0949 - acc: 0.9754 - val_loss: 0.1398 - val_acc: 0.9633\n",
      "Epoch 290/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0971 - acc: 0.9735 - val_loss: 0.1333 - val_acc: 0.9707\n",
      "Epoch 291/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0938 - acc: 0.9738 - val_loss: 0.1341 - val_acc: 0.9661\n",
      "Epoch 292/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0934 - acc: 0.9747 - val_loss: 0.1326 - val_acc: 0.9688\n",
      "Epoch 293/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0957 - acc: 0.9748 - val_loss: 0.1323 - val_acc: 0.9693\n",
      "Epoch 294/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0906 - acc: 0.9755 - val_loss: 0.1396 - val_acc: 0.9693\n",
      "Epoch 295/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0936 - acc: 0.9741 - val_loss: 0.1376 - val_acc: 0.9666\n",
      "Epoch 296/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0925 - acc: 0.9745 - val_loss: 0.1264 - val_acc: 0.9704\n",
      "Epoch 297/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0933 - acc: 0.9753 - val_loss: 0.1329 - val_acc: 0.9713\n",
      "Epoch 298/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0936 - acc: 0.9753 - val_loss: 0.1382 - val_acc: 0.9710\n",
      "Epoch 299/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0928 - acc: 0.9748 - val_loss: 0.1428 - val_acc: 0.9718\n",
      "Epoch 300/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0916 - acc: 0.9749 - val_loss: 0.1344 - val_acc: 0.9696\n",
      "Epoch 301/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0894 - acc: 0.9751 - val_loss: 0.1368 - val_acc: 0.9683\n",
      "Epoch 302/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0909 - acc: 0.9745 - val_loss: 0.1398 - val_acc: 0.9693\n",
      "Epoch 303/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0935 - acc: 0.9737 - val_loss: 0.1356 - val_acc: 0.9691\n",
      "Epoch 304/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0903 - acc: 0.9744 - val_loss: 0.1384 - val_acc: 0.9658\n",
      "Epoch 305/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0884 - acc: 0.9768 - val_loss: 0.1422 - val_acc: 0.9683\n",
      "Epoch 306/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0881 - acc: 0.9763 - val_loss: 0.1358 - val_acc: 0.9696\n",
      "Epoch 307/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0910 - acc: 0.9740 - val_loss: 0.1491 - val_acc: 0.9658\n",
      "Epoch 308/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0926 - acc: 0.9728 - val_loss: 0.1393 - val_acc: 0.9688\n",
      "Epoch 309/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0948 - acc: 0.9747 - val_loss: 0.1357 - val_acc: 0.9688\n",
      "Epoch 310/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0906 - acc: 0.9755 - val_loss: 0.1437 - val_acc: 0.9696\n",
      "Epoch 311/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0899 - acc: 0.9750 - val_loss: 0.1413 - val_acc: 0.9652\n",
      "Epoch 312/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0928 - acc: 0.9742 - val_loss: 0.1428 - val_acc: 0.9693\n",
      "Epoch 313/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0915 - acc: 0.9740 - val_loss: 0.1394 - val_acc: 0.9699\n",
      "Epoch 314/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0889 - acc: 0.9764 - val_loss: 0.1353 - val_acc: 0.9693\n",
      "Epoch 315/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0939 - acc: 0.9745 - val_loss: 0.1374 - val_acc: 0.9702\n",
      "Epoch 316/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0886 - acc: 0.9764 - val_loss: 0.1343 - val_acc: 0.9680\n",
      "Epoch 317/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0864 - acc: 0.9757 - val_loss: 0.1388 - val_acc: 0.9688\n",
      "Epoch 318/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0869 - acc: 0.9761 - val_loss: 0.1362 - val_acc: 0.9702\n",
      "Epoch 319/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0895 - acc: 0.9753 - val_loss: 0.1324 - val_acc: 0.9699\n",
      "Epoch 320/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0854 - acc: 0.9761 - val_loss: 0.1465 - val_acc: 0.9611\n",
      "Epoch 321/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0828 - acc: 0.9775 - val_loss: 0.1349 - val_acc: 0.9674\n",
      "Epoch 322/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0869 - acc: 0.9757 - val_loss: 0.1427 - val_acc: 0.9699\n",
      "Epoch 323/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0880 - acc: 0.9756 - val_loss: 0.1407 - val_acc: 0.9693\n",
      "Epoch 324/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0845 - acc: 0.9764 - val_loss: 0.1337 - val_acc: 0.9685\n",
      "Epoch 325/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0891 - acc: 0.9750 - val_loss: 0.1387 - val_acc: 0.9691\n",
      "Epoch 326/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0842 - acc: 0.9764 - val_loss: 0.1415 - val_acc: 0.9666\n",
      "Epoch 327/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0887 - acc: 0.9767 - val_loss: 0.1407 - val_acc: 0.9691\n",
      "Epoch 328/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0848 - acc: 0.9764 - val_loss: 0.1411 - val_acc: 0.9669\n",
      "Epoch 329/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0826 - acc: 0.9777 - val_loss: 0.1467 - val_acc: 0.9636\n",
      "Epoch 330/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0829 - acc: 0.9751 - val_loss: 0.1425 - val_acc: 0.9639\n",
      "Epoch 331/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0812 - acc: 0.9772 - val_loss: 0.1457 - val_acc: 0.9674\n",
      "Epoch 332/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0839 - acc: 0.9772 - val_loss: 0.1470 - val_acc: 0.9674\n",
      "Epoch 333/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0827 - acc: 0.9774 - val_loss: 0.1525 - val_acc: 0.9655\n",
      "Epoch 334/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0844 - acc: 0.9768 - val_loss: 0.1509 - val_acc: 0.9652\n",
      "Epoch 335/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0825 - acc: 0.9757 - val_loss: 0.1530 - val_acc: 0.9620\n",
      "Epoch 336/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0845 - acc: 0.9751 - val_loss: 0.1428 - val_acc: 0.9666\n",
      "Epoch 337/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0823 - acc: 0.9763 - val_loss: 0.1470 - val_acc: 0.9707\n",
      "Epoch 338/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0826 - acc: 0.9753 - val_loss: 0.1443 - val_acc: 0.9688\n",
      "Epoch 339/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0844 - acc: 0.9776 - val_loss: 0.1427 - val_acc: 0.9699\n",
      "Epoch 340/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0811 - acc: 0.9779 - val_loss: 0.1399 - val_acc: 0.9685\n",
      "Epoch 341/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0787 - acc: 0.9788 - val_loss: 0.1507 - val_acc: 0.9688\n",
      "Epoch 342/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0784 - acc: 0.9783 - val_loss: 0.1472 - val_acc: 0.9691\n",
      "Epoch 343/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0752 - acc: 0.9771 - val_loss: 0.1600 - val_acc: 0.9589\n",
      "Epoch 344/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0809 - acc: 0.9776 - val_loss: 0.1523 - val_acc: 0.9685\n",
      "Epoch 345/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0822 - acc: 0.9769 - val_loss: 0.1494 - val_acc: 0.9691\n",
      "Epoch 346/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0755 - acc: 0.9798 - val_loss: 0.1484 - val_acc: 0.9650\n",
      "Epoch 347/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0773 - acc: 0.9785 - val_loss: 0.1575 - val_acc: 0.9693\n",
      "Epoch 348/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0771 - acc: 0.9783 - val_loss: 0.1470 - val_acc: 0.9666\n",
      "Epoch 349/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0803 - acc: 0.9776 - val_loss: 0.1498 - val_acc: 0.9677\n",
      "Epoch 350/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0748 - acc: 0.9795 - val_loss: 0.1459 - val_acc: 0.9683\n",
      "Epoch 351/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0823 - acc: 0.9751 - val_loss: 0.1553 - val_acc: 0.9652\n",
      "Epoch 352/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0856 - acc: 0.9757 - val_loss: 0.1530 - val_acc: 0.9663\n",
      "Epoch 353/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0841 - acc: 0.9756 - val_loss: 0.1519 - val_acc: 0.9639\n",
      "Epoch 354/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0833 - acc: 0.9763 - val_loss: 0.1517 - val_acc: 0.9658\n",
      "Epoch 355/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0792 - acc: 0.9783 - val_loss: 0.1507 - val_acc: 0.9655\n",
      "Epoch 356/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0800 - acc: 0.9751 - val_loss: 0.1485 - val_acc: 0.9685\n",
      "Epoch 357/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0811 - acc: 0.9774 - val_loss: 0.1564 - val_acc: 0.9696\n",
      "Epoch 358/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0825 - acc: 0.9760 - val_loss: 0.1551 - val_acc: 0.9680\n",
      "Epoch 359/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0744 - acc: 0.9785 - val_loss: 0.1451 - val_acc: 0.9685\n",
      "Epoch 360/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0766 - acc: 0.9794 - val_loss: 0.1504 - val_acc: 0.9699\n",
      "Epoch 361/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0740 - acc: 0.9785 - val_loss: 0.1531 - val_acc: 0.9696\n",
      "Epoch 362/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0752 - acc: 0.9798 - val_loss: 0.1495 - val_acc: 0.9661\n",
      "Epoch 363/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0736 - acc: 0.9801 - val_loss: 0.1515 - val_acc: 0.9707\n",
      "Epoch 364/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0797 - acc: 0.9769 - val_loss: 0.1516 - val_acc: 0.9677\n",
      "Epoch 365/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0733 - acc: 0.9794 - val_loss: 0.1673 - val_acc: 0.9669\n",
      "Epoch 366/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0733 - acc: 0.9794 - val_loss: 0.1597 - val_acc: 0.9677\n",
      "Epoch 367/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0722 - acc: 0.9788 - val_loss: 0.1636 - val_acc: 0.9641\n",
      "Epoch 368/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0767 - acc: 0.9784 - val_loss: 0.1530 - val_acc: 0.9699\n",
      "Epoch 369/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0785 - acc: 0.9772 - val_loss: 0.1577 - val_acc: 0.9691\n",
      "Epoch 370/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0729 - acc: 0.9791 - val_loss: 0.1554 - val_acc: 0.9650\n",
      "Epoch 371/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0705 - acc: 0.9804 - val_loss: 0.1546 - val_acc: 0.9669\n",
      "Epoch 372/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0784 - acc: 0.9772 - val_loss: 0.1476 - val_acc: 0.9672\n",
      "Epoch 373/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0730 - acc: 0.9797 - val_loss: 0.1595 - val_acc: 0.9644\n",
      "Epoch 374/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0711 - acc: 0.9789 - val_loss: 0.1568 - val_acc: 0.9661\n",
      "Epoch 375/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0707 - acc: 0.9792 - val_loss: 0.1546 - val_acc: 0.9669\n",
      "Epoch 376/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0799 - acc: 0.9778 - val_loss: 0.1450 - val_acc: 0.9693\n",
      "Epoch 377/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0752 - acc: 0.9788 - val_loss: 0.1579 - val_acc: 0.9685\n",
      "Epoch 378/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0689 - acc: 0.9791 - val_loss: 0.1645 - val_acc: 0.9693\n",
      "Epoch 379/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0708 - acc: 0.9797 - val_loss: 0.1581 - val_acc: 0.9691\n",
      "Epoch 380/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0721 - acc: 0.9792 - val_loss: 0.1550 - val_acc: 0.9696\n",
      "Epoch 381/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0745 - acc: 0.9792 - val_loss: 0.1539 - val_acc: 0.9655\n",
      "Epoch 382/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0717 - acc: 0.9799 - val_loss: 0.1562 - val_acc: 0.9669\n",
      "Epoch 383/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0745 - acc: 0.9790 - val_loss: 0.1561 - val_acc: 0.9669\n",
      "Epoch 384/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0702 - acc: 0.9806 - val_loss: 0.1571 - val_acc: 0.9683\n",
      "Epoch 385/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0661 - acc: 0.9809 - val_loss: 0.1602 - val_acc: 0.9666\n",
      "Epoch 386/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0714 - acc: 0.9798 - val_loss: 0.1599 - val_acc: 0.9663\n",
      "Epoch 387/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0701 - acc: 0.9806 - val_loss: 0.1621 - val_acc: 0.9661\n",
      "Epoch 388/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0734 - acc: 0.9797 - val_loss: 0.1702 - val_acc: 0.9672\n",
      "Epoch 389/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0713 - acc: 0.9799 - val_loss: 0.1577 - val_acc: 0.9650\n",
      "Epoch 390/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0693 - acc: 0.9798 - val_loss: 0.1575 - val_acc: 0.9674\n",
      "Epoch 391/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0717 - acc: 0.9797 - val_loss: 0.1557 - val_acc: 0.9693\n",
      "Epoch 392/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0681 - acc: 0.9810 - val_loss: 0.1692 - val_acc: 0.9696\n",
      "Epoch 393/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0648 - acc: 0.9810 - val_loss: 0.1600 - val_acc: 0.9680\n",
      "Epoch 394/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0637 - acc: 0.9816 - val_loss: 0.1639 - val_acc: 0.9680\n",
      "Epoch 395/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0678 - acc: 0.9803 - val_loss: 0.1620 - val_acc: 0.9669\n",
      "Epoch 396/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0679 - acc: 0.9802 - val_loss: 0.1607 - val_acc: 0.9704\n",
      "Epoch 397/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0670 - acc: 0.9801 - val_loss: 0.1590 - val_acc: 0.9652\n",
      "Epoch 398/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0692 - acc: 0.9794 - val_loss: 0.1640 - val_acc: 0.9680\n",
      "Epoch 399/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0693 - acc: 0.9804 - val_loss: 0.1647 - val_acc: 0.9685\n",
      "Epoch 400/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0703 - acc: 0.9796 - val_loss: 0.1719 - val_acc: 0.9655\n",
      "Epoch 401/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0647 - acc: 0.9817 - val_loss: 0.1636 - val_acc: 0.9666\n",
      "Epoch 402/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0653 - acc: 0.9824 - val_loss: 0.1614 - val_acc: 0.9696\n",
      "Epoch 403/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0654 - acc: 0.9803 - val_loss: 0.1642 - val_acc: 0.9633\n",
      "Epoch 404/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0682 - acc: 0.9797 - val_loss: 0.1734 - val_acc: 0.9598\n",
      "Epoch 405/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0628 - acc: 0.9812 - val_loss: 0.1619 - val_acc: 0.9636\n",
      "Epoch 406/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0647 - acc: 0.9806 - val_loss: 0.1715 - val_acc: 0.9683\n",
      "Epoch 407/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0646 - acc: 0.9818 - val_loss: 0.1582 - val_acc: 0.9661\n",
      "Epoch 408/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0649 - acc: 0.9825 - val_loss: 0.1657 - val_acc: 0.9636\n",
      "Epoch 409/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0670 - acc: 0.9801 - val_loss: 0.1664 - val_acc: 0.9688\n",
      "Epoch 410/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0646 - acc: 0.9817 - val_loss: 0.1606 - val_acc: 0.9685\n",
      "Epoch 411/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0633 - acc: 0.9822 - val_loss: 0.1724 - val_acc: 0.9677\n",
      "Epoch 412/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0662 - acc: 0.9815 - val_loss: 0.1611 - val_acc: 0.9652\n",
      "Epoch 413/500\n",
      "8526/8526 [==============================] - 8s - loss: 0.0634 - acc: 0.9819 - val_loss: 0.1712 - val_acc: 0.9672\n",
      "Epoch 414/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0669 - acc: 0.9817 - val_loss: 0.1578 - val_acc: 0.9666\n",
      "Epoch 415/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0627 - acc: 0.9824 - val_loss: 0.1640 - val_acc: 0.9672\n",
      "Epoch 416/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0722 - acc: 0.9804 - val_loss: 0.1586 - val_acc: 0.9677\n",
      "Epoch 417/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0632 - acc: 0.9825 - val_loss: 0.1617 - val_acc: 0.9666\n",
      "Epoch 418/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0595 - acc: 0.9837 - val_loss: 0.1656 - val_acc: 0.9663\n",
      "Epoch 419/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0647 - acc: 0.9809 - val_loss: 0.1719 - val_acc: 0.9672\n",
      "Epoch 420/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0602 - acc: 0.9819 - val_loss: 0.1673 - val_acc: 0.9661\n",
      "Epoch 421/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0600 - acc: 0.9817 - val_loss: 0.1833 - val_acc: 0.9702\n",
      "Epoch 422/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0648 - acc: 0.9819 - val_loss: 0.1653 - val_acc: 0.9658\n",
      "Epoch 423/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0630 - acc: 0.9821 - val_loss: 0.1668 - val_acc: 0.9688\n",
      "Epoch 424/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0662 - acc: 0.9816 - val_loss: 0.1625 - val_acc: 0.9644\n",
      "Epoch 425/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0670 - acc: 0.9811 - val_loss: 0.1630 - val_acc: 0.9672\n",
      "Epoch 426/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0642 - acc: 0.9814 - val_loss: 0.1719 - val_acc: 0.9669\n",
      "Epoch 427/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0601 - acc: 0.9837 - val_loss: 0.1587 - val_acc: 0.9652\n",
      "Epoch 428/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0624 - acc: 0.9828 - val_loss: 0.1629 - val_acc: 0.9674\n",
      "Epoch 429/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0664 - acc: 0.9811 - val_loss: 0.1654 - val_acc: 0.9644\n",
      "Epoch 430/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0597 - acc: 0.9828 - val_loss: 0.1657 - val_acc: 0.9663\n",
      "Epoch 431/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0614 - acc: 0.9823 - val_loss: 0.1704 - val_acc: 0.9663\n",
      "Epoch 432/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0612 - acc: 0.9819 - val_loss: 0.1667 - val_acc: 0.9672\n",
      "Epoch 433/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0658 - acc: 0.9805 - val_loss: 0.1612 - val_acc: 0.9658\n",
      "Epoch 434/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0631 - acc: 0.9822 - val_loss: 0.1788 - val_acc: 0.9672\n",
      "Epoch 435/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0623 - acc: 0.9826 - val_loss: 0.1694 - val_acc: 0.9663\n",
      "Epoch 436/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0592 - acc: 0.9835 - val_loss: 0.1710 - val_acc: 0.9666\n",
      "Epoch 437/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0667 - acc: 0.9809 - val_loss: 0.1738 - val_acc: 0.9611\n",
      "Epoch 438/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0588 - acc: 0.9835 - val_loss: 0.1719 - val_acc: 0.9647\n",
      "Epoch 439/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0563 - acc: 0.9833 - val_loss: 0.1782 - val_acc: 0.9674\n",
      "Epoch 440/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0558 - acc: 0.9839 - val_loss: 0.1813 - val_acc: 0.9647\n",
      "Epoch 441/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0588 - acc: 0.9824 - val_loss: 0.1865 - val_acc: 0.9672\n",
      "Epoch 442/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0565 - acc: 0.9838 - val_loss: 0.1747 - val_acc: 0.9661\n",
      "Epoch 443/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0539 - acc: 0.9842 - val_loss: 0.1822 - val_acc: 0.9691\n",
      "Epoch 444/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0581 - acc: 0.9824 - val_loss: 0.1835 - val_acc: 0.9650\n",
      "Epoch 445/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0627 - acc: 0.9803 - val_loss: 0.1692 - val_acc: 0.9674\n",
      "Epoch 446/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0599 - acc: 0.9825 - val_loss: 0.1701 - val_acc: 0.9680\n",
      "Epoch 447/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0584 - acc: 0.9826 - val_loss: 0.1753 - val_acc: 0.9683\n",
      "Epoch 448/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0555 - acc: 0.9849 - val_loss: 0.1743 - val_acc: 0.9685\n",
      "Epoch 449/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0602 - acc: 0.9832 - val_loss: 0.1762 - val_acc: 0.9683\n",
      "Epoch 450/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0613 - acc: 0.9819 - val_loss: 0.1714 - val_acc: 0.9655\n",
      "Epoch 451/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0572 - acc: 0.9829 - val_loss: 0.1674 - val_acc: 0.9677\n",
      "Epoch 452/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0564 - acc: 0.9839 - val_loss: 0.1777 - val_acc: 0.9666\n",
      "Epoch 453/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0550 - acc: 0.9826 - val_loss: 0.1788 - val_acc: 0.9633\n",
      "Epoch 454/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0596 - acc: 0.9822 - val_loss: 0.1854 - val_acc: 0.9535\n",
      "Epoch 455/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0574 - acc: 0.9829 - val_loss: 0.1720 - val_acc: 0.9644\n",
      "Epoch 456/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0548 - acc: 0.9853 - val_loss: 0.1716 - val_acc: 0.9628\n",
      "Epoch 457/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0548 - acc: 0.9845 - val_loss: 0.1833 - val_acc: 0.9652\n",
      "Epoch 458/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0604 - acc: 0.9826 - val_loss: 0.1788 - val_acc: 0.9669\n",
      "Epoch 459/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0570 - acc: 0.9843 - val_loss: 0.1806 - val_acc: 0.9680\n",
      "Epoch 460/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0593 - acc: 0.9826 - val_loss: 0.1786 - val_acc: 0.9669\n",
      "Epoch 461/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0582 - acc: 0.9840 - val_loss: 0.1831 - val_acc: 0.9641\n",
      "Epoch 462/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0557 - acc: 0.9836 - val_loss: 0.1742 - val_acc: 0.9677\n",
      "Epoch 463/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0564 - acc: 0.9837 - val_loss: 0.1911 - val_acc: 0.9598\n",
      "Epoch 464/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0551 - acc: 0.9839 - val_loss: 0.1820 - val_acc: 0.9669\n",
      "Epoch 465/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0516 - acc: 0.9860 - val_loss: 0.1738 - val_acc: 0.9669\n",
      "Epoch 466/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0535 - acc: 0.9852 - val_loss: 0.1859 - val_acc: 0.9688\n",
      "Epoch 467/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0560 - acc: 0.9848 - val_loss: 0.1864 - val_acc: 0.9652\n",
      "Epoch 468/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0549 - acc: 0.9840 - val_loss: 0.1841 - val_acc: 0.9672\n",
      "Epoch 469/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0611 - acc: 0.9824 - val_loss: 0.1735 - val_acc: 0.9661\n",
      "Epoch 470/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0537 - acc: 0.9839 - val_loss: 0.1917 - val_acc: 0.9647\n",
      "Epoch 471/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0564 - acc: 0.9837 - val_loss: 0.1883 - val_acc: 0.9606\n",
      "Epoch 472/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0532 - acc: 0.9839 - val_loss: 0.1879 - val_acc: 0.9655\n",
      "Epoch 473/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0609 - acc: 0.9818 - val_loss: 0.1748 - val_acc: 0.9663\n",
      "Epoch 474/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0528 - acc: 0.9843 - val_loss: 0.1794 - val_acc: 0.9663\n",
      "Epoch 475/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0504 - acc: 0.9862 - val_loss: 0.1733 - val_acc: 0.9655\n",
      "Epoch 476/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0589 - acc: 0.9838 - val_loss: 0.1795 - val_acc: 0.9658\n",
      "Epoch 477/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0557 - acc: 0.9833 - val_loss: 0.1799 - val_acc: 0.9650\n",
      "Epoch 478/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0533 - acc: 0.9853 - val_loss: 0.1802 - val_acc: 0.9661\n",
      "Epoch 479/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0564 - acc: 0.9826 - val_loss: 0.1823 - val_acc: 0.9641\n",
      "Epoch 480/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0536 - acc: 0.9843 - val_loss: 0.1779 - val_acc: 0.9677\n",
      "Epoch 481/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0566 - acc: 0.9839 - val_loss: 0.1830 - val_acc: 0.9636\n",
      "Epoch 482/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0525 - acc: 0.9844 - val_loss: 0.1764 - val_acc: 0.9584\n",
      "Epoch 483/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0552 - acc: 0.9843 - val_loss: 0.1780 - val_acc: 0.9641\n",
      "Epoch 484/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0529 - acc: 0.9848 - val_loss: 0.1811 - val_acc: 0.9677\n",
      "Epoch 485/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0510 - acc: 0.9853 - val_loss: 0.1782 - val_acc: 0.9658\n",
      "Epoch 486/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0528 - acc: 0.9844 - val_loss: 0.1819 - val_acc: 0.9614\n",
      "Epoch 487/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0589 - acc: 0.9838 - val_loss: 0.1747 - val_acc: 0.9650\n",
      "Epoch 488/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0547 - acc: 0.9855 - val_loss: 0.1752 - val_acc: 0.9633\n",
      "Epoch 489/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0577 - acc: 0.9837 - val_loss: 0.1817 - val_acc: 0.9633\n",
      "Epoch 490/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0563 - acc: 0.9829 - val_loss: 0.1821 - val_acc: 0.9644\n",
      "Epoch 491/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0557 - acc: 0.9837 - val_loss: 0.1863 - val_acc: 0.9669\n",
      "Epoch 492/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0513 - acc: 0.9851 - val_loss: 0.1877 - val_acc: 0.9677\n",
      "Epoch 493/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0519 - acc: 0.9845 - val_loss: 0.1839 - val_acc: 0.9672\n",
      "Epoch 494/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0579 - acc: 0.9830 - val_loss: 0.1864 - val_acc: 0.9611\n",
      "Epoch 495/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0507 - acc: 0.9853 - val_loss: 0.1837 - val_acc: 0.9693\n",
      "Epoch 496/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0607 - acc: 0.9832 - val_loss: 0.1804 - val_acc: 0.9617\n",
      "Epoch 497/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0630 - acc: 0.9823 - val_loss: 0.1780 - val_acc: 0.9655\n",
      "Epoch 498/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0588 - acc: 0.9832 - val_loss: 0.1766 - val_acc: 0.9644\n",
      "Epoch 499/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0595 - acc: 0.9830 - val_loss: 0.1717 - val_acc: 0.9680\n",
      "Epoch 500/500\n",
      "8526/8526 [==============================] - 7s - loss: 0.0507 - acc: 0.9853 - val_loss: 0.1814 - val_acc: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe401814ad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms = RMSprop()\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,\n",
    "               input_shape = (12 , 1) )  ) # returns a sequence of vectors of dimension 32\n",
    "model.add(Dropout(.25))\n",
    "model.add(LSTM(64, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(Dropout(.25))\n",
    "model.add(LSTM(64))  # return a single vector of dimension 32\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=rms, metrics=[\"accuracy\"] )\n",
    "\n",
    "model.fit(X_train_1, Y_train,\n",
    "          batch_size=64, nb_epoch=500,\n",
    "          validation_data=(X_test_1, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Y_preds_lstm = model.predict_classes(X_test_1, verbose=0)\n",
    "print(Y_preds_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3386   16]\n",
      " [  98  154]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix of logistic regression\n",
    "conf = metrics.confusion_matrix(y_test,Y_preds_lstm)\n",
    "#plt.imshow(conf)\n",
    "print conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.97      1.00      0.98      3402\n",
      "    class 1       0.91      0.61      0.73       252\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, Y_preds_lstm, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99267769  0.00732235]\n",
      " [ 0.99916822  0.00083174]\n",
      " [ 0.9988935   0.00110644]\n",
      " [ 0.99984407  0.00015592]\n",
      " [ 0.99982941  0.00017053]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEeCAYAAACZlyICAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3ffcfb150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_preds_prob_lstm = model.predict_proba(X_test_1, verbose=0)\n",
    "print(Y_preds_prob_lstm[:5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ROC curve\n",
    "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
    "fpr1, tpr1, thresholds = metrics.roc_curve(y_test, Y_preds_prob_lstm[:,1])\n",
    "plt.plot(fpr1, tpr1, lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve',fontsize=14)\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)',fontsize=14) #fontweight='bold'\n",
    "plt.ylabel('True Positive Rate (Sensitivity)',fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('smartgrid_lstm_roc.png')\n",
    "\n",
    "#Y_logPred_prob = logReg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837554706382\n"
     ]
    }
   ],
   "source": [
    "print metrics.roc_auc_score(y_test, Y_preds_prob_lstm[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
