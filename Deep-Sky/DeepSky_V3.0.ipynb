{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle, gzip, os, sys, time \n",
    "import timeit\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "\"\"\" This file contains different utility functions that are not connected\n",
    "in anyway to the networks presented in the tutorials, but rather help in\n",
    "processing the outputs into a more understandable way.\n",
    "\n",
    "For example ``tile_raster_images`` helps in generating a easy to grasp\n",
    "image from a set of samples or weights.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy\n",
    "\n",
    "\n",
    "def scale_to_unit_interval(ndar, eps=1e-8):\n",
    "    \"\"\" Scales all values in the ndarray ndar to be between 0 and 1 \"\"\"\n",
    "    ndar = ndar.copy()\n",
    "    ndar -= ndar.min()\n",
    "    ndar *= 1.0 / (ndar.max() + eps)\n",
    "    return ndar\n",
    "\n",
    "\n",
    "def tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),\n",
    "                       scale_rows_to_unit_interval=True,\n",
    "                       output_pixel_vals=True):\n",
    "    \"\"\"\n",
    "    Transform an array with one flattened image per row, into an array in\n",
    "    which images are reshaped and layed out like tiles on a floor.\n",
    "\n",
    "    This function is useful for visualizing datasets whose rows are images,\n",
    "    and also columns of matrices for transforming those rows\n",
    "    (such as the first layer of a neural net).\n",
    "\n",
    "    :type X: a 2-D ndarray or a tuple of 4 channels, elements of which can\n",
    "    be 2-D ndarrays or None;\n",
    "    :param X: a 2-D array in which every row is a flattened image.\n",
    "\n",
    "    :type img_shape: tuple; (height, width)\n",
    "    :param img_shape: the original shape of each image\n",
    "\n",
    "    :type tile_shape: tuple; (rows, cols)\n",
    "    :param tile_shape: the number of images to tile (rows, cols)\n",
    "\n",
    "    :param output_pixel_vals: if output should be pixel values (i.e. int8\n",
    "    values) or floats\n",
    "\n",
    "    :param scale_rows_to_unit_interval: if the values need to be scaled before\n",
    "    being plotted to [0,1] or not\n",
    "\n",
    "\n",
    "    :returns: array suitable for viewing as an image.\n",
    "    (See:`Image.fromarray`.)\n",
    "    :rtype: a 2-d array with same dtype as X.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(img_shape) == 2\n",
    "    assert len(tile_shape) == 2\n",
    "    assert len(tile_spacing) == 2\n",
    "\n",
    "    # The expression below can be re-written in a more C style as\n",
    "    # follows :\n",
    "    #\n",
    "    # out_shape    = [0,0]\n",
    "    # out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -\n",
    "    #                tile_spacing[0]\n",
    "    # out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -\n",
    "    #                tile_spacing[1]\n",
    "    out_shape = [\n",
    "        (ishp + tsp) * tshp - tsp\n",
    "        for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)\n",
    "    ]\n",
    "\n",
    "    if isinstance(X, tuple):\n",
    "        assert len(X) == 4\n",
    "        # Create an output numpy ndarray to store the image\n",
    "        if output_pixel_vals:\n",
    "            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n",
    "                                    dtype='uint8')\n",
    "        else:\n",
    "            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n",
    "                                    dtype=X.dtype)\n",
    "\n",
    "        #colors default to 0, alpha defaults to 1 (opaque)\n",
    "        if output_pixel_vals:\n",
    "            channel_defaults = [0, 0, 0, 255]\n",
    "        else:\n",
    "            channel_defaults = [0., 0., 0., 1.]\n",
    "\n",
    "        for i in xrange(4):\n",
    "            if X[i] is None:\n",
    "                # if channel is None, fill it with zeros of the correct\n",
    "                # dtype\n",
    "                dt = out_array.dtype\n",
    "                if output_pixel_vals:\n",
    "                    dt = 'uint8'\n",
    "                out_array[:, :, i] = numpy.zeros(\n",
    "                    out_shape,\n",
    "                    dtype=dt\n",
    "                ) + channel_defaults[i]\n",
    "            else:\n",
    "                # use a recurrent call to compute the channel and store it\n",
    "                # in the output\n",
    "                out_array[:, :, i] = tile_raster_images(\n",
    "                    X[i], img_shape, tile_shape, tile_spacing,\n",
    "                    scale_rows_to_unit_interval, output_pixel_vals)\n",
    "        return out_array\n",
    "\n",
    "    else:\n",
    "        # if we are dealing with only one channel\n",
    "        H, W = img_shape\n",
    "        Hs, Ws = tile_spacing\n",
    "\n",
    "        # generate a matrix to store the output\n",
    "        dt = X.dtype\n",
    "        if output_pixel_vals:\n",
    "            dt = 'uint8'\n",
    "        out_array = numpy.zeros(out_shape, dtype=dt)\n",
    "\n",
    "        for tile_row in xrange(tile_shape[0]):\n",
    "            for tile_col in xrange(tile_shape[1]):\n",
    "                if tile_row * tile_shape[1] + tile_col < X.shape[0]:\n",
    "                    this_x = X[tile_row * tile_shape[1] + tile_col]\n",
    "                    if scale_rows_to_unit_interval:\n",
    "                        # if we should scale values to be between 0 and 1\n",
    "                        # do this by calling the `scale_to_unit_interval`\n",
    "                        # function\n",
    "                        this_img = scale_to_unit_interval(\n",
    "                            this_x.reshape(img_shape))\n",
    "                    else:\n",
    "                        this_img = this_x.reshape(img_shape)\n",
    "                    # add the slice to the corresponding position in the\n",
    "                    # output array\n",
    "                    c = 1\n",
    "                    if output_pixel_vals:\n",
    "                        c = 255\n",
    "                    out_array[\n",
    "                        tile_row * (H + Hs): tile_row * (H + Hs) + H,\n",
    "                        tile_col * (W + Ws): tile_col * (W + Ws) + W\n",
    "                    ] = this_img * c\n",
    "        return out_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the data sets\n",
    "f = gzip.open('mnist.pkl.gz','rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[5 0 4 ..., 8 4 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train_set[0].shape, train_set[1].shape\n",
    "print valid_set[0].shape, valid_set[1].shape\n",
    "print test_set[0].shape, test_set[1].shape\n",
    "\n",
    "print train_set[0]\n",
    "print train_set[1]\n",
    "\n",
    "#print 'train= ',train_set[1]\n",
    "np.sqrt(784) #28\n",
    "#print len(train_set)\n",
    "#print train_set\n",
    "#print train_set[0]\n",
    "#print train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shared_dataset(data_xy):\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = theano.shared(np.asarray(data_x, dtype=theano.config.floatX))\n",
    "    shared_y = theano.shared(np.asarray(data_y, dtype=theano.config.floatX))\n",
    "    return shared_x, T.cast(shared_y,'int32')\n",
    "\n",
    "test_set_x, test_set_y = shared_dataset(test_set)\n",
    "valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "data  = train_set_x[2*500:3*500]\n",
    "label = train_set_y[2*500:3*500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "EnvironmentError",
     "evalue": "('The following error happened while compiling the node', GpuFromHost(Elemwise{exp,no_inplace}.0), '\\n', \"You forced the use of gpu device 'gpu', but nvcc was not found. Set it in your PATH environment variable or set the Theano flags 'cuda.root' to its directory\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEnvironmentError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-7e3e1d5c6fcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mrng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msandbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_from_host\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoposort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    221\u001b[0m                 \u001b[0mallow_input_downcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 profile=profile)\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    510\u001b[0m     return orig_function(inputs, cloned_outputs, mode,\n\u001b[0;32m    511\u001b[0m             \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m             on_unused_input=on_unused_input)\n\u001b[0m\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input)\u001b[0m\n\u001b[0;32m   1310\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m                        defaults)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, input_storage, trustme)\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;31m# Get a function instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mstart_linker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1181\u001b[1;33m         \u001b[0m_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_o\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_thunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_storage_lists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1182\u001b[0m         \u001b[0mend_linker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[1;34m(self, profiler, input_storage, output_storage)\u001b[0m\n\u001b[0;32m    432\u001b[0m         return self.make_all(profiler=profiler,\n\u001b[0;32m    433\u001b[0m                              \u001b[0minput_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_storage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                              output_storage=output_storage)[:3]\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/vm.pyc\u001b[0m in \u001b[0;36mmake_all\u001b[1;34m(self, profiler, input_storage, output_storage)\u001b[0m\n\u001b[0;32m    845\u001b[0m                                                  \u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                                                  no_recycling))\n\u001b[0m\u001b[0;32m    848\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m                 e.args = (\"The following error happened while\"\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[1;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[0;32m    233\u001b[0m                                     \u001b[0mdefault_to_move_computation_to_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                                     \u001b[0mmove_shared_float32_to_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                                     enable_cuda=False)\n\u001b[0m\u001b[0;32m    236\u001b[0m         return super(GpuOp, self).make_thunk(node, storage_map,\n\u001b[0;32m    237\u001b[0m                                              compute_map, no_recycling)\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.pyc\u001b[0m in \u001b[0;36muse\u001b[1;34m(device, force, default_to_move_computation_to_gpu, move_shared_float32_to_gpu, enable_cuda, test_driver)\u001b[0m\n\u001b[0;32m    310\u001b[0m                                    \u001b[1;34m\"environment variable or set the Theano \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                                    \u001b[1;34m\"flags 'cuda.root' to its directory\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                                    \"\" % device)\n\u001b[0m\u001b[0;32m    313\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             raise EnvironmentError(\"You forced the use of gpu device %s, \"\n",
      "\u001b[1;31mEnvironmentError\u001b[0m: ('The following error happened while compiling the node', GpuFromHost(Elemwise{exp,no_inplace}.0), '\\n', \"You forced the use of gpu device 'gpu', but nvcc was not found. Set it in your PATH environment variable or set the Theano flags 'cuda.root' to its directory\")"
     ]
    }
   ],
   "source": [
    "#Making sure if it runs in GPU\n",
    "from theano import function, config, shared, sandbox\n",
    "import theano.sandbox.cuda.basic_ops\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), 'float32'))\n",
    "f = function([], sandbox.cuda.basic_ops.gpu_from_host(T.exp(x)))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in xrange(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "print(\"Numpy result is %s\" % (numpy.asarray(r),))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "        \n",
    "    #Load the datasets\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "        \n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "            \n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x, dtype=theano.config.floatX), borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y, dtype=theano.config.floatX), borrow=borrow)\n",
    "        return shared_x, T.cast(shared_y,'int32')\n",
    "        \n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "        \n",
    "    rval = [(train_set_x, train_set_y ), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n",
    "        \n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \n",
    "        self.W = theano.shared(value=np.zeros((n_in, n_out), dtype = theano.config.floatX), name='W', borrow=True)\n",
    "        self.b = theano.shared(value=np.zeros((n_out,), dtype=theano.config.floatX), name='b', borrow=True)\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W)+self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis =1)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def negative_log_likelihood(self, y):\n",
    "        \n",
    "        return -T.mean( T.log(self.p_y_given_x)[T.arange(y.shape[0]),y] )\n",
    "    \n",
    "    def errors(self, y):\n",
    "        \n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError('y should have the same shape as self.y_pred', ('y', y.type, 'y_pred', self.y_pred_.type) )\n",
    "            \n",
    "        if y.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()            \n",
    "        \n",
    "    \n",
    "    def sgd_optimization_mnist(self, learning_rate = 0.13, n_epochs = 100, dataset='mnist.pkl.gz', batch_size=500):\n",
    "        \n",
    "        datasets = load_data(dataset)\n",
    "        \n",
    "        train_set_x , train_set_y = datasets[0]\n",
    "        valid_set_x, valid_set_y = datasets[1]\n",
    "        test_set_x, test_set_y = datasets[2]\n",
    "        \n",
    "        n_train_batches = train_set_x.get_value(borrow=True).shape[0]/ batch_size\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]/ batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]/batch_size\n",
    "        \n",
    "        print '...building the model'\n",
    "        \n",
    "        index = T.lscalar()\n",
    "        print \"index= \", index\n",
    "        print \"n_train_batches= \",n_train_batches\n",
    "        \n",
    "        x = T.matrix('x')\n",
    "        y = T.ivector('y')\n",
    "        \n",
    "        classifier = LogisticRegression(input =x, n_in = 28*28, n_out=10)\n",
    "        \n",
    "        cost = classifier.negative_log_likelihood(y)\n",
    "        \n",
    "        test_model = theano.function( inputs = [index],\n",
    "                                      outputs = classifier.errors(y),\n",
    "                                      givens={\n",
    "                                            x: test_set_x[index*batch_size:(index+1)*batch_size],\n",
    "                                            y: test_set_y[index*batch_size:(index+1)*batch_size]\n",
    "                                        }\n",
    "                                    )\n",
    "        \n",
    "        validate_model = theano.function( inputs = [index],\n",
    "                                      outputs = classifier.errors(y),\n",
    "                                      givens={\n",
    "                                            x: valid_set_x[index*batch_size:(index+1)*batch_size],\n",
    "                                            y: valid_set_y[index*batch_size:(index+1)*batch_size]\n",
    "                                        }\n",
    "                                    )\n",
    "        \n",
    "        g_W = T.grad(cost=cost, wrt = classifier.W)\n",
    "        g_b = T.grad(cost=cost, wrt = classifier.b)\n",
    "        \n",
    "        updates = [ (classifier.W, classifier.W - learning_rate*g_W),\n",
    "                    (classifier.b, classifier.b - learning_rate*g_b)]\n",
    "        \n",
    "        train_model = theano.function(\n",
    "                        inputs = [index],\n",
    "                        outputs = cost,\n",
    "                        updates = updates,\n",
    "                        givens = {\n",
    "                                x: train_set_x[index*batch_size:(index+1)*batch_size],\n",
    "                                y: train_set_y[index*batch_size:(index+1)*batch_size]\n",
    "                            }\n",
    "                        )\n",
    "    \n",
    "        \n",
    "        print '.....training the model'\n",
    "        \n",
    "        #early stopping parameters\n",
    "        patience = 5000\n",
    "        patience_increase = 2\n",
    "        improvement_threshold = 0.995\n",
    "        validation_frequency = min(n_train_batches, patience/2)\n",
    "        \n",
    "        print 'validation_frequency= ',validation_frequency\n",
    "        \n",
    "        best_validation_loss = np.inf\n",
    "        test_score = 0.\n",
    "        start_time = time.clock\n",
    "        \n",
    "        done_looping = False\n",
    "        epoch = 0\n",
    "        while(epoch < n_epochs) and (not done_looping): #n_epochs=100\n",
    "            epoch = epoch +1\n",
    "            for minibatch_index in xrange(n_train_batches): #n_train_batches=100\n",
    "                \n",
    "                #print \"minibatch_index=\",minibatch_index\n",
    "                \n",
    "                minibatch_avg_cost = train_model(minibatch_index)\n",
    "                iter = (epoch -1)*n_train_batches + minibatch_index\n",
    "                #print \"iter=\",iter\n",
    "                \n",
    "                if (iter+1) % validation_frequency == 0 :\n",
    "                    validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
    "                    this_validation_loss = np.mean(validation_losses)\n",
    "                    print ( 'epoch %i, minibatch avg. %i/%i, validation error %f %%' %\n",
    "                              (epoch, minibatch_index+1, n_train_batches, this_validation_loss*100)\n",
    "                          )\n",
    "                    \n",
    "                    if(False and epoch % 10 == 0):\n",
    "                        image = Image.fromarray(\n",
    "                        tile_raster_images(\n",
    "                            X=classifier.W.get_value(borrow=True).T,\n",
    "                            img_shape=(28, 28),\n",
    "                            tile_shape=(10, 10),\n",
    "                            tile_spacing=(1, 1) )\n",
    "                        )\n",
    "                        #imshow(np.asarray(image))\n",
    "                        image.save('filters_at_epoch_%i.png' % epoch)\n",
    "        \n",
    "                    \n",
    "                    if (this_validation_loss < best_validation_loss*improvement_threshold):\n",
    "                        patience = max(patience, iter*patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "\n",
    "                    test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "                    test_score = np.mean(test_losses)\n",
    "\n",
    "                    print ( 'epoch %i, minibatch avg. %i/%i, test error of best model %f %%' %\n",
    "                           (epoch, minibatch_index+1, n_train_batches, test_score*100.)\n",
    "                          )\n",
    "                \n",
    "                if (patience<=iter):\n",
    "                    done_looping = True\n",
    "                    break\n",
    "                \n",
    "        end_time = time.clock()\n",
    "        print('Optimization complete with best validation score of %f with test performance %f %%' %\n",
    "                (best_validation_loss*100., test_score*100.) )\n",
    "        print 'The code run for %d epochs, with %f seconds ' %(epoch, end_time)\n",
    "        print >> sys.stderr #, ('The code for file '+ os.path.split(__file__)[1] + 'ran for %.1fs'%((end_time-start_time)))\n",
    "\n",
    "#if __name__ == '__main__':    \n",
    "#    sgd_optimization_mnist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...building the model\n",
      "index=  <TensorType(int64, scalar)>\n",
      "n_train_batches=  100\n",
      ".....training the model\n",
      "validation_frequency=  100\n",
      "epoch 1, minibatch avg. 100/100, validation error 11.890000 %\n",
      "epoch 1, minibatch avg. 100/100, test error of best model 12.170000 %\n",
      "epoch 2, minibatch avg. 100/100, validation error 10.520000 %\n",
      "epoch 2, minibatch avg. 100/100, test error of best model 10.940000 %\n",
      "epoch 3, minibatch avg. 100/100, validation error 9.940000 %\n",
      "epoch 3, minibatch avg. 100/100, test error of best model 10.210000 %\n",
      "epoch 4, minibatch avg. 100/100, validation error 9.480000 %\n",
      "epoch 4, minibatch avg. 100/100, test error of best model 9.820000 %\n",
      "epoch 5, minibatch avg. 100/100, validation error 9.230000 %\n",
      "epoch 5, minibatch avg. 100/100, test error of best model 9.510000 %\n",
      "epoch 6, minibatch avg. 100/100, validation error 9.100000 %\n",
      "epoch 6, minibatch avg. 100/100, test error of best model 9.280000 %\n",
      "epoch 7, minibatch avg. 100/100, validation error 8.820000 %\n",
      "epoch 7, minibatch avg. 100/100, test error of best model 9.100000 %\n",
      "epoch 8, minibatch avg. 100/100, validation error 8.750000 %\n",
      "epoch 8, minibatch avg. 100/100, test error of best model 8.890000 %\n",
      "epoch 9, minibatch avg. 100/100, validation error 8.630000 %\n",
      "epoch 9, minibatch avg. 100/100, test error of best model 8.780000 %\n",
      "epoch 10, minibatch avg. 100/100, validation error 8.490000 %\n",
      "epoch 10, minibatch avg. 100/100, test error of best model 8.620000 %\n",
      "epoch 11, minibatch avg. 100/100, validation error 8.440000 %\n",
      "epoch 11, minibatch avg. 100/100, test error of best model 8.570000 %\n",
      "epoch 12, minibatch avg. 100/100, validation error 8.380000 %\n",
      "epoch 12, minibatch avg. 100/100, test error of best model 8.530000 %\n",
      "epoch 13, minibatch avg. 100/100, validation error 8.310000 %\n",
      "epoch 13, minibatch avg. 100/100, test error of best model 8.480000 %\n",
      "epoch 14, minibatch avg. 100/100, validation error 8.250000 %\n",
      "epoch 14, minibatch avg. 100/100, test error of best model 8.400000 %\n",
      "epoch 15, minibatch avg. 100/100, validation error 8.270000 %\n",
      "epoch 15, minibatch avg. 100/100, test error of best model 8.330000 %\n",
      "epoch 16, minibatch avg. 100/100, validation error 8.220000 %\n",
      "epoch 16, minibatch avg. 100/100, test error of best model 8.300000 %\n",
      "epoch 17, minibatch avg. 100/100, validation error 8.160000 %\n",
      "epoch 17, minibatch avg. 100/100, test error of best model 8.210000 %\n",
      "epoch 18, minibatch avg. 100/100, validation error 8.100000 %\n",
      "epoch 18, minibatch avg. 100/100, test error of best model 8.200000 %\n",
      "epoch 19, minibatch avg. 100/100, validation error 8.070000 %\n",
      "epoch 19, minibatch avg. 100/100, test error of best model 8.190000 %\n",
      "epoch 20, minibatch avg. 100/100, validation error 8.080000 %\n",
      "epoch 20, minibatch avg. 100/100, test error of best model 8.180000 %\n",
      "epoch 21, minibatch avg. 100/100, validation error 8.030000 %\n",
      "epoch 21, minibatch avg. 100/100, test error of best model 8.160000 %\n",
      "epoch 22, minibatch avg. 100/100, validation error 8.020000 %\n",
      "epoch 22, minibatch avg. 100/100, test error of best model 8.180000 %\n",
      "epoch 23, minibatch avg. 100/100, validation error 7.980000 %\n",
      "epoch 23, minibatch avg. 100/100, test error of best model 8.110000 %\n",
      "epoch 24, minibatch avg. 100/100, validation error 7.970000 %\n",
      "epoch 24, minibatch avg. 100/100, test error of best model 8.130000 %\n",
      "epoch 25, minibatch avg. 100/100, validation error 7.950000 %\n",
      "epoch 25, minibatch avg. 100/100, test error of best model 8.110000 %\n",
      "epoch 26, minibatch avg. 100/100, validation error 7.940000 %\n",
      "epoch 26, minibatch avg. 100/100, test error of best model 8.090000 %\n",
      "epoch 27, minibatch avg. 100/100, validation error 7.890000 %\n",
      "epoch 27, minibatch avg. 100/100, test error of best model 8.060000 %\n",
      "epoch 28, minibatch avg. 100/100, validation error 7.880000 %\n",
      "epoch 28, minibatch avg. 100/100, test error of best model 8.030000 %\n",
      "epoch 29, minibatch avg. 100/100, validation error 7.830000 %\n",
      "epoch 29, minibatch avg. 100/100, test error of best model 7.970000 %\n",
      "epoch 30, minibatch avg. 100/100, validation error 7.800000 %\n",
      "epoch 30, minibatch avg. 100/100, test error of best model 7.930000 %\n",
      "epoch 31, minibatch avg. 100/100, validation error 7.780000 %\n",
      "epoch 31, minibatch avg. 100/100, test error of best model 7.900000 %\n",
      "epoch 32, minibatch avg. 100/100, validation error 7.760000 %\n",
      "epoch 32, minibatch avg. 100/100, test error of best model 7.850000 %\n",
      "epoch 33, minibatch avg. 100/100, validation error 7.750000 %\n",
      "epoch 33, minibatch avg. 100/100, test error of best model 7.840000 %\n",
      "epoch 34, minibatch avg. 100/100, validation error 7.760000 %\n",
      "epoch 34, minibatch avg. 100/100, test error of best model 7.820000 %\n",
      "epoch 35, minibatch avg. 100/100, validation error 7.780000 %\n",
      "epoch 35, minibatch avg. 100/100, test error of best model 7.790000 %\n",
      "epoch 36, minibatch avg. 100/100, validation error 7.750000 %\n",
      "epoch 36, minibatch avg. 100/100, test error of best model 7.800000 %\n",
      "epoch 37, minibatch avg. 100/100, validation error 7.730000 %\n",
      "epoch 37, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 38, minibatch avg. 100/100, validation error 7.720000 %\n",
      "epoch 38, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 39, minibatch avg. 100/100, validation error 7.700000 %\n",
      "epoch 39, minibatch avg. 100/100, test error of best model 7.780000 %\n",
      "epoch 40, minibatch avg. 100/100, validation error 7.690000 %\n",
      "epoch 40, minibatch avg. 100/100, test error of best model 7.800000 %\n",
      "epoch 41, minibatch avg. 100/100, validation error 7.670000 %\n",
      "epoch 41, minibatch avg. 100/100, test error of best model 7.790000 %\n",
      "epoch 42, minibatch avg. 100/100, validation error 7.690000 %\n",
      "epoch 42, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 43, minibatch avg. 100/100, validation error 7.690000 %\n",
      "epoch 43, minibatch avg. 100/100, test error of best model 7.820000 %\n",
      "epoch 44, minibatch avg. 100/100, validation error 7.690000 %\n",
      "epoch 44, minibatch avg. 100/100, test error of best model 7.800000 %\n",
      "epoch 45, minibatch avg. 100/100, validation error 7.680000 %\n",
      "epoch 45, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 46, minibatch avg. 100/100, validation error 7.660000 %\n",
      "epoch 46, minibatch avg. 100/100, test error of best model 7.820000 %\n",
      "epoch 47, minibatch avg. 100/100, validation error 7.640000 %\n",
      "epoch 47, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 48, minibatch avg. 100/100, validation error 7.600000 %\n",
      "epoch 48, minibatch avg. 100/100, test error of best model 7.790000 %\n",
      "epoch 49, minibatch avg. 100/100, validation error 7.600000 %\n",
      "epoch 49, minibatch avg. 100/100, test error of best model 7.800000 %\n",
      "epoch 50, minibatch avg. 100/100, validation error 7.590000 %\n",
      "epoch 50, minibatch avg. 100/100, test error of best model 7.800000 %\n",
      "epoch 51, minibatch avg. 100/100, validation error 7.600000 %\n",
      "epoch 51, minibatch avg. 100/100, test error of best model 7.800000 %\n",
      "epoch 52, minibatch avg. 100/100, validation error 7.580000 %\n",
      "epoch 52, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 53, minibatch avg. 100/100, validation error 7.570000 %\n",
      "epoch 53, minibatch avg. 100/100, test error of best model 7.820000 %\n",
      "epoch 54, minibatch avg. 100/100, validation error 7.560000 %\n",
      "epoch 54, minibatch avg. 100/100, test error of best model 7.820000 %\n",
      "epoch 55, minibatch avg. 100/100, validation error 7.550000 %\n",
      "epoch 55, minibatch avg. 100/100, test error of best model 7.820000 %\n",
      "epoch 56, minibatch avg. 100/100, validation error 7.520000 %\n",
      "epoch 56, minibatch avg. 100/100, test error of best model 7.830000 %\n",
      "epoch 57, minibatch avg. 100/100, validation error 7.490000 %\n",
      "epoch 57, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 58, minibatch avg. 100/100, validation error 7.480000 %\n",
      "epoch 58, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 59, minibatch avg. 100/100, validation error 7.450000 %\n",
      "epoch 59, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 60, minibatch avg. 100/100, validation error 7.450000 %\n",
      "epoch 60, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 61, minibatch avg. 100/100, validation error 7.430000 %\n",
      "epoch 61, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 62, minibatch avg. 100/100, validation error 7.430000 %\n",
      "epoch 62, minibatch avg. 100/100, test error of best model 7.830000 %\n",
      "epoch 63, minibatch avg. 100/100, validation error 7.410000 %\n",
      "epoch 63, minibatch avg. 100/100, test error of best model 7.810000 %\n",
      "epoch 64, minibatch avg. 100/100, validation error 7.400000 %\n",
      "epoch 64, minibatch avg. 100/100, test error of best model 7.790000 %\n",
      "epoch 65, minibatch avg. 100/100, validation error 7.410000 %\n",
      "epoch 65, minibatch avg. 100/100, test error of best model 7.790000 %\n",
      "epoch 66, minibatch avg. 100/100, validation error 7.410000 %\n",
      "epoch 66, minibatch avg. 100/100, test error of best model 7.790000 %\n",
      "epoch 67, minibatch avg. 100/100, validation error 7.390000 %\n",
      "epoch 67, minibatch avg. 100/100, test error of best model 7.770000 %\n",
      "epoch 68, minibatch avg. 100/100, validation error 7.380000 %\n",
      "epoch 68, minibatch avg. 100/100, test error of best model 7.770000 %\n",
      "epoch 69, minibatch avg. 100/100, validation error 7.390000 %\n",
      "epoch 69, minibatch avg. 100/100, test error of best model 7.760000 %\n",
      "epoch 70, minibatch avg. 100/100, validation error 7.370000 %\n",
      "epoch 70, minibatch avg. 100/100, test error of best model 7.750000 %\n",
      "epoch 71, minibatch avg. 100/100, validation error 7.370000 %\n",
      "epoch 71, minibatch avg. 100/100, test error of best model 7.750000 %\n",
      "epoch 72, minibatch avg. 100/100, validation error 7.380000 %\n",
      "epoch 72, minibatch avg. 100/100, test error of best model 7.770000 %\n",
      "epoch 73, minibatch avg. 100/100, validation error 7.380000 %\n",
      "epoch 73, minibatch avg. 100/100, test error of best model 7.770000 %\n",
      "epoch 74, minibatch avg. 100/100, validation error 7.380000 %\n",
      "epoch 74, minibatch avg. 100/100, test error of best model 7.790000 %\n",
      "epoch 75, minibatch avg. 100/100, validation error 7.350000 %\n",
      "epoch 75, minibatch avg. 100/100, test error of best model 7.780000 %\n",
      "epoch 76, minibatch avg. 100/100, validation error 7.330000 %\n",
      "epoch 76, minibatch avg. 100/100, test error of best model 7.800000 %\n",
      "epoch 77, minibatch avg. 100/100, validation error 7.320000 %\n",
      "epoch 77, minibatch avg. 100/100, test error of best model 7.780000 %\n",
      "epoch 78, minibatch avg. 100/100, validation error 7.310000 %\n",
      "epoch 78, minibatch avg. 100/100, test error of best model 7.780000 %\n",
      "epoch 79, minibatch avg. 100/100, validation error 7.310000 %\n",
      "epoch 79, minibatch avg. 100/100, test error of best model 7.780000 %\n",
      "epoch 80, minibatch avg. 100/100, validation error 7.330000 %\n",
      "epoch 80, minibatch avg. 100/100, test error of best model 7.780000 %\n",
      "epoch 81, minibatch avg. 100/100, validation error 7.300000 %\n",
      "epoch 81, minibatch avg. 100/100, test error of best model 7.770000 %\n",
      "epoch 82, minibatch avg. 100/100, validation error 7.290000 %\n",
      "epoch 82, minibatch avg. 100/100, test error of best model 7.750000 %\n",
      "epoch 83, minibatch avg. 100/100, validation error 7.290000 %\n",
      "epoch 83, minibatch avg. 100/100, test error of best model 7.740000 %\n",
      "epoch 84, minibatch avg. 100/100, validation error 7.290000 %\n",
      "epoch 84, minibatch avg. 100/100, test error of best model 7.750000 %\n",
      "epoch 85, minibatch avg. 100/100, validation error 7.300000 %\n",
      "epoch 85, minibatch avg. 100/100, test error of best model 7.740000 %\n",
      "epoch 86, minibatch avg. 100/100, validation error 7.280000 %\n",
      "epoch 86, minibatch avg. 100/100, test error of best model 7.710000 %\n",
      "epoch 87, minibatch avg. 100/100, validation error 7.280000 %\n",
      "epoch 87, minibatch avg. 100/100, test error of best model 7.700000 %\n",
      "epoch 88, minibatch avg. 100/100, validation error 7.260000 %\n",
      "epoch 88, minibatch avg. 100/100, test error of best model 7.690000 %\n",
      "epoch 89, minibatch avg. 100/100, validation error 7.260000 %\n",
      "epoch 89, minibatch avg. 100/100, test error of best model 7.690000 %\n",
      "epoch 90, minibatch avg. 100/100, validation error 7.250000 %\n",
      "epoch 90, minibatch avg. 100/100, test error of best model 7.700000 %\n",
      "epoch 91, minibatch avg. 100/100, validation error 7.250000 %\n",
      "epoch 91, minibatch avg. 100/100, test error of best model 7.690000 %\n",
      "epoch 92, minibatch avg. 100/100, validation error 7.230000 %\n",
      "epoch 92, minibatch avg. 100/100, test error of best model 7.680000 %\n",
      "epoch 93, minibatch avg. 100/100, validation error 7.230000 %\n",
      "epoch 93, minibatch avg. 100/100, test error of best model 7.670000 %\n",
      "epoch 94, minibatch avg. 100/100, validation error 7.230000 %\n",
      "epoch 94, minibatch avg. 100/100, test error of best model 7.680000 %\n",
      "epoch 95, minibatch avg. 100/100, validation error 7.220000 %\n",
      "epoch 95, minibatch avg. 100/100, test error of best model 7.690000 %\n",
      "Optimization complete with best validation score of 7.220000 with test performance 7.690000 %\n",
      "The code run for 96 epochs, with 6116.430464 seconds \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = T.matrix('x')\n",
    "logReg = LogisticRegression(x,n_in = 28*28, n_out=10)\n",
    "logReg.sgd_optimization_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None, activation=T.tanh):\n",
    "        self.input = input\n",
    "        \n",
    "        if W is None:\n",
    "            W_values = np.asarray(rng.uniform(\n",
    "                                              low  = -np.sqrt(6./(n_in + n_out)),\n",
    "                                              high =  np.sqrt(6./(n_in+n_out)),\n",
    "                                              size = (n_in, n_out)\n",
    "                                             ), dtype = theano.config.floatX)\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_value *= 4\n",
    "                \n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "            \n",
    "        if b is None:\n",
    "            b_value = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_value, name='b', borrow=True)\n",
    "            \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (lin_output if activation is None else activation(lin_output) )\n",
    "        self.params = [self.W, self.b]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        self.hiddenLayer = HiddenLayer(rng = rng, input = input, n_in=n_in, n_out=n_hidden, activation=T.tanh)\n",
    "        self.logRegressionLayer = LogisticRegression(input=self.hiddenLayer.output, n_in=n_hidden, n_out= n_out)\n",
    "        self.L1 = ( abs(self.hiddenLayer.W).sum() + abs(self.logRegressionLayer.W).sum() )\n",
    "        self.L2_sqr = ( (self.hiddenLayer.W**2).sum() + (self.logRegressionLayer.W**2).sum() )\n",
    "        self.negative_log_likelihood = ( self.logRegressionLayer.negative_log_likelihood )\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "        \n",
    "          \n",
    "    def test_mlp(self, learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=100,\n",
    "             dataset='mnist.pkl.gz', batch_size=20, n_hidden=500):\n",
    "\n",
    "        datasets = load_data(dataset)\n",
    "\n",
    "        train_set_x, train_set_y = datasets[0]\n",
    "        valid_set_x, valid_set_y = datasets[1]\n",
    "        test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "        ######################\n",
    "        # BUILD ACTUAL MODEL #\n",
    "        ######################\n",
    "        print '... building the model'\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "        index = T.lscalar()  # index to a [mini]batch\n",
    "        x = T.matrix('x')  # the data is presented as rasterized images\n",
    "        y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                            # [int] labels\n",
    "\n",
    "        rng = np.random.RandomState(1234)\n",
    "\n",
    "        # construct the MLP class\n",
    "        classifier = MLP(\n",
    "            rng=rng,\n",
    "            input=x,\n",
    "            n_in=28 * 28,\n",
    "            n_hidden=n_hidden,\n",
    "            n_out=10\n",
    "        )\n",
    "\n",
    "        # the cost we minimize during training is the negative log likelihood of\n",
    "        # the model plus the regularization terms (L1 and L2); cost is expressed\n",
    "        # here symbolically\n",
    "        cost = (\n",
    "            classifier.negative_log_likelihood(y)\n",
    "            + L1_reg * classifier.L1\n",
    "            + L2_reg * classifier.L2_sqr\n",
    "        )\n",
    "       \n",
    "        # compiling a Theano function that computes the mistakes that are made\n",
    "        # by the model on a minibatch\n",
    "        test_model = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=classifier.errors(y),\n",
    "            givens={\n",
    "                x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "                y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        validate_model = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=classifier.errors(y),\n",
    "            givens={\n",
    "                x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "                y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # compute the gradient of cost with respect to theta (sotred in params)\n",
    "        # the resulting gradients will be stored in a list gparams\n",
    "        gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "\n",
    "        # specify how to update the parameters of the model as a list of\n",
    "        # (variable, update expression) pairs\n",
    "\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(classifier.params, gparams)\n",
    "        ]\n",
    "\n",
    "        # compiling a Theano function `train_model` that returns the cost, but\n",
    "        # in the same time updates the parameter of the model based on the rules\n",
    "        # defined in `updates`\n",
    "        train_model = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "            }\n",
    "        )\n",
    "     \n",
    "        ###############\n",
    "        # TRAIN MODEL #\n",
    "        ###############\n",
    "        print '... training'\n",
    "\n",
    "        # early-stopping parameters\n",
    "        patience = 10000  # look as this many examples regardless\n",
    "        patience_increase = 2  # wait this much longer when a new best is\n",
    "                               # found\n",
    "        improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                       # considered significant\n",
    "        validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                      # go through this many\n",
    "                                      # minibatche before checking the network\n",
    "                                      # on the validation set; in this case we\n",
    "                                      # check every epoch\n",
    "\n",
    "        best_validation_loss = np.inf\n",
    "        best_iter = 0\n",
    "        test_score = 0.\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        epoch = 0\n",
    "        done_looping = False\n",
    "\n",
    "        while (epoch < n_epochs) and (not done_looping):\n",
    "            epoch = epoch + 1\n",
    "            for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "                minibatch_avg_cost = train_model(minibatch_index)\n",
    "                # iteration number\n",
    "                iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "                if (iter + 1) % validation_frequency == 0:\n",
    "                    # compute zero-one loss on validation set\n",
    "                    validation_losses = [validate_model(i) for i\n",
    "                                         in xrange(n_valid_batches)]\n",
    "                    this_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "                    print(\n",
    "                        'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            this_validation_loss * 100.\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    if(True and epoch % 10 == 0):\n",
    "                        image = Image.fromarray(\n",
    "                        tile_raster_images(\n",
    "                            X=classifier.hiddenLayer.W.get_value(borrow=True).T,\n",
    "                            img_shape=(28, 28),\n",
    "                            tile_shape=(10, 10),\n",
    "                            tile_spacing=(1, 1) )\n",
    "                        )\n",
    "                        #imshow(np.asarray(image))\n",
    "                        image.save('filters_at_epoch_%i.png' % epoch)\n",
    "\n",
    "                    # if we got the best validation score until now\n",
    "                    if this_validation_loss < best_validation_loss:\n",
    "                        #improve patience if loss improvement is good enough\n",
    "                        if (this_validation_loss < best_validation_loss *improvement_threshold):\n",
    "                            patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                        best_validation_loss = this_validation_loss\n",
    "                        best_iter = iter\n",
    "\n",
    "                        # test it on the test set\n",
    "                        test_losses = [test_model(i) for i\n",
    "                                       in xrange(n_test_batches)]\n",
    "                        test_score = np.mean(test_losses)\n",
    "\n",
    "                        print(('epoch %i, minibatch %i/%i, test error of '\n",
    "                               'best model %f %%') %\n",
    "                              (epoch, minibatch_index + 1, n_train_batches,\n",
    "                               test_score * 100.))\n",
    "\n",
    "                if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    print \"patience = \", patience, \" iter = \", iter\n",
    "                    break\n",
    "\n",
    "        end_time = timeit.default_timer()\n",
    "        print(('Optimization complete. Best validation score of %f %% '\n",
    "               'obtained at iteration %i, with test performance %f %%') %\n",
    "              (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "        print >> sys.stderr #, ('The code for file ' + os.path.split(__file__)[1] +\n",
    "                            #' ran for %.2fm' % ((end_time - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... training\n",
      "epoch 1, minibatch 2500/2500, validation error 9.620000 %\n",
      "epoch 1, minibatch 2500/2500, test error of best model 10.090000 %\n",
      "epoch 2, minibatch 2500/2500, validation error 8.610000 %\n",
      "epoch 2, minibatch 2500/2500, test error of best model 8.740000 %"
     ]
    }
   ],
   "source": [
    "x = T.matrix('x')\n",
    "rng = np.random.RandomState(1234)\n",
    "mlp = MLP(rng, input=x, n_in=28 * 28, n_hidden=100, n_out=10)\n",
    "mlp.test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LeNetConvPoolLayer(object):\n",
    "    \"\"\"Pool Layer of a convolutional network \"\"\"\n",
    "\n",
    "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2)):\n",
    "\n",
    "        assert image_shape[1] == filter_shape[1]\n",
    "        self.input = input\n",
    "\n",
    "        # there are \"num input feature maps * filter height * filter width\"\n",
    "        # inputs to each hidden unit\n",
    "        fan_in = numpy.prod(filter_shape[1:])\n",
    "        # each unit in the lower layer receives a gradient from:\n",
    "        # \"num output feature maps * filter height * filter width\" /\n",
    "        #   pooling size\n",
    "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) /\n",
    "                   numpy.prod(poolsize))\n",
    "        # initialize weights with random weights\n",
    "        W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.asarray(\n",
    "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # the bias is a 1D tensor -- one bias per output feature map\n",
    "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
    "        self.b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        # convolve input feature maps with filters\n",
    "        conv_out = conv.conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "        # downsample each feature map individually, using maxpooling\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        # add the bias term. Since the bias is a vector (1D array), we first\n",
    "        # reshape it to a tensor of shape (1, n_filters, 1, 1). Each bias will\n",
    "        # thus be broadcasted across mini-batches and feature map\n",
    "        # width & height\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "\n",
    "        # store parameters of this layer\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nkerns=[20, 50]   #Number of kernel filter in layer0 and layer1\n",
    "learning_rate=0.01\n",
    "#L1_reg=0.00\n",
    "#L2_reg=0.0001\n",
    "n_epochs=100\n",
    "#dataset='mnist.pkl.gz' \n",
    "batch_size=50\n",
    "#n_hidden=500\n",
    "dataset='mnist.pkl.gz'\n",
    "    \n",
    "x = T.matrix('x')   # the data is presented as rasterized images\n",
    "y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                        # [int] labels\n",
    "    \n",
    "datasets = load_data(dataset)\n",
    "\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x,  test_set_y  = datasets[2]\n",
    "\n",
    "# compute number of minibatches for training, validation and testing\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_test_batches  = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "######################\n",
    "# BUILD ACTUAL MODEL #\n",
    "######################\n",
    "print '... building the model'\n",
    "\n",
    "# Reshape matrix of rasterized images of shape (batch_size, 28 * 28)\n",
    "# to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
    "# (28, 28) is the size of MNIST images.\n",
    "layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "\n",
    "# Construct the first convolutional pooling layer:\n",
    "# filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)\n",
    "# maxpooling reduces this further to (24/2, 24/2) = (12, 12)\n",
    "# 4D output tensor is thus of shape (batch_size, nkerns[0], 12, 12)\n",
    "layer0 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "# Construct the second convolutional pooling layer\n",
    "# filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)\n",
    "# maxpooling reduces this further to (8/2, 8/2) = (4, 4)\n",
    "# 4D output tensor is thus of shape (batch_size, nkerns[1], 4, 4)\n",
    "layer1 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "# the HiddenLayer being fully-connected, it operates on 2D matrices of\n",
    "# shape (batch_size, num_pixels) (i.e matrix of rasterized images).\n",
    "# This will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),\n",
    "# or (500, 50 * 4 * 4) = (500, 800) with the default values.\n",
    "layer2_input = layer1.output.flatten(2)\n",
    "\n",
    "# construct a fully-connected sigmoidal layer\n",
    "layer2 = HiddenLayer(\n",
    "        rng,\n",
    "        input=layer2_input,\n",
    "        n_in=nkerns[1] * 4 * 4,\n",
    "        n_out=500,\n",
    "        activation=T.tanh\n",
    "    )\n",
    "\n",
    "# classify the values of the fully-connected sigmoidal layer\n",
    "layer3 = LogisticRegression(input=layer2.output, n_in=500, n_out=10)\n",
    "\n",
    "# the cost we minimize during training is the NLL of the model\n",
    "cost = layer3.negative_log_likelihood(y)\n",
    "    \n",
    "index = T.lscalar()  # index to a [mini]batch\n",
    "    \n",
    "# create a function to compute the mistakes that are made by the model\n",
    "test_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "validate_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# create a list of all model parameters to be fit by gradient descent\n",
    "params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "\n",
    "# create a list of gradients for all model parameters\n",
    "grads = T.grad(cost, params)\n",
    "\n",
    "# train_model is a function that updates the model parameters by\n",
    "# SGD Since this model has many parameters, it would be tedious to\n",
    "# manually create an update rule for each model parameter. We thus\n",
    "# create the updates list by automatically looping over all\n",
    "# (params[i], grads[i]) pairs.\n",
    "updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ]\n",
    "\n",
    "train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "###############\n",
    "# TRAIN MODEL #\n",
    "###############\n",
    "print '... training'\n",
    "\n",
    "# early-stopping parameters\n",
    "patience = 10000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                               # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                       # considered significant\n",
    "validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                      # go through this many\n",
    "                                      # minibatche before checking the network\n",
    "                                      # on the validation set; in this case we\n",
    "                                      # check every epoch\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "best_iter = 0\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "epoch = 0\n",
    "done_looping = False\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                         in xrange(n_valid_batches)]\n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                        'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            this_validation_loss * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (this_validation_loss < best_validation_loss *improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [test_model(i) for i\n",
    "                                       in xrange(n_test_batches)]\n",
    "                    test_score = np.mean(test_losses)\n",
    "\n",
    "                    print(('epoch %i, minibatch %i/%i, test error of '\n",
    "                               'best model %f %%') %\n",
    "                              (epoch, minibatch_index + 1, n_train_batches,\n",
    "                               test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                print \"patience = \", patience, \" iter = \", iter\n",
    "                break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(('Optimization complete. Best validation score of %f %% '\n",
    "               'obtained at iteration %i, with test performance %f %%') %\n",
    "              (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "print >> sys.stderr #, ('The code for file ' + os.path.split(__file__)[1] +\n",
    "                            #' ran for %.2fm' % ((end_time - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
